# -*- coding: utf-8 -*-
"""test_metric.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_K5gkoPFltJ11B5iCWTt-RLCgNbrWFuL
"""

! pip install ijson

!unzip 'trainA_original_700.zip' # initial image tensors without resize

import os
import copy
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
from PIL import Image
from sklearn.model_selection import train_test_split
import random

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Set random seed for reproducibility
random.seed(42)
torch.manual_seed(42)

# -----------------------
# Global settings & helpers
# -----------------------

# Input image size (height, width)
ISIZE = (720, 1280)

# ImageNet statistics (for VGG16)
# imagenet_mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
# imagenet_std  = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)

def normalize_tensor(img):
    """Normalize a tensor image (C, H, W) with values in [0,255]."""
    img = img / 255.0
    return img

def unnormalize_tensor(img):
    """Convert a normalized tensor back to a displayable numpy image."""
    img = img * 255.0
    return img.clamp(0, 255).byte().cpu().numpy()

# Global anchor parameters
ratios = [0.5, 1, 2]
anchor_scales = [8, 16, 32]

import ijson


def extract_first_n_labels(json_file_path, n):
    labels = []
    with open(json_file_path, 'rb') as f:
        parser = ijson.items(f, 'item')
        for i, item in enumerate(parser):
            if i >= n:
                break
            filtered_labels = [
                {"category": li.get("category"), "box2d": li.get("box2d")}
                for li in item.get("labels", []) if "box2d" in li
            ]
            labels.append({
                "name": item.get("name"),
                "timestamp": item.get("timestamp"),
                "labels": filtered_labels
            })
    return labels

def standardize_filename(path_or_name):
    base = os.path.basename(path_or_name)
    base, _ = os.path.splitext(base)
    return base

class CustomDataset(torch.utils.data.Dataset):
    def __init__(self, image_dir, labels, pt_dir='pt_files'):
        self.image_dir = image_dir
        self.pt_dir = pt_dir
        os.makedirs(self.pt_dir, exist_ok=True)
        self.image_files = sorted([
            os.path.join(image_dir, f)
            for f in os.listdir(image_dir)
            if f.lower().endswith(('.jpg', '.png', '.jpeg'))
        ])
        self.label_dict = {}
        for item in labels:
            key = standardize_filename(item["name"])
            self.label_dict[key] = item

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        image_path = self.image_files[idx]
        pt_path = os.path.join(
            self.pt_dir,
            os.path.basename(image_path)
            .replace('.jpg', '.pt')
            .replace('.png', '.pt')
            .replace('.jpeg', '.pt')
        )
        if os.path.exists(pt_path):
            image_tensor = torch.load(pt_path, weights_only=True)
        else:
            image = Image.open(image_path).convert('RGB')
            if image.size != (ISIZE[1], ISIZE[0]):  # PIL: (width, height)
                image = image.resize((ISIZE[1], ISIZE[0]))
            image_tensor = transforms.PILToTensor()(image).float()
            torch.save(image_tensor, pt_path)
        image_tensor = normalize_tensor(image_tensor)

        base_key = standardize_filename(image_path)
        matched = self.label_dict.get(base_key, None)
        if matched is None or "labels" not in matched:
            target = {"boxes": torch.zeros((0, 4), dtype=torch.float32),
                      "labels": torch.zeros((0,), dtype=torch.int64),
                      "names": [],
                      "index": idx}
        else:
            boxes = []
            cats = []
            for obj in matched["labels"]:
                if "box2d" in obj:
                    b2d = obj["box2d"]
                    boxes.append([float(b2d["y1"]), float(b2d["x1"]),
                                  float(b2d["y2"]), float(b2d["x2"])])
                    cats.append(obj["category"])
            boxes_tensor = torch.tensor(boxes, dtype=torch.float32) if boxes else torch.zeros((0,4), dtype=torch.float32)
            labels_tensor = torch.tensor([1] * len(cats), dtype=torch.int64)
            target = {"boxes": boxes_tensor, "labels": labels_tensor, "names": cats, "index": idx}
        return {"image": image_tensor, "boxes": target["boxes"], "labels": target["labels"],
                "names": target["names"], "index": target["index"]}

# Custom collate function (your version)
def custom_collate_fn(batch):
    images = [item["image"] for item in batch]
    boxes = [item["boxes"] for item in batch]
    labels = [item["labels"] for item in batch]
    names = [item["names"] for item in batch]
    indices = [item["index"] for item in batch]
    return {"images": torch.stack(images, 0),
            "boxes": boxes,
            "labels": labels,
            "names": names,
            "indices": indices}

"""## RPN Helper Functions

"""

def create_corner_rect(bb, color='red'):
    bb = np.array(bb, dtype=np.float32)
    return plt.Rectangle((bb[0], bb[1]), bb[2]-bb[0], bb[3]-bb[1], color=color,
                         fill=False, lw=3)

def show_corner_bbs(im, bbs):
    # im expected to be (C, H, W) tensor; convert to numpy image for plotting
    im_np = unnormalize_tensor(im)
    plt.imshow(np.transpose(im_np, (1, 2, 0)))
    for bb in bbs:
        plt.gca().add_patch(create_corner_rect(bb))
    plt.show()

def create_ground_truth_rect(bb, color='blue'):
    # Ground truth boxes are already in the format: [x1, y1, x2, y2]
    bb = np.array(bb, dtype=np.float32)
    return plt.Rectangle((bb[0], bb[1]), bb[2]-bb[0], bb[3]-bb[1],
                         color=color, fill=False, lw=3)

def show_ground_truth_bbs(im, bbs):
    # im is expected to be a (C, H, W) tensor; convert it to a NumPy image for plotting
    im_np = unnormalize_tensor(im)
    plt.imshow(np.transpose(im_np, (1, 2, 0)))
    for bb in bbs:
        plt.gca().add_patch(create_ground_truth_rect(bb))
    plt.show()

# -----------------------
# Vectorized IoU Computation
# -----------------------

def compute_iou_vectorized(anchors, gt_boxes):
    """
    Compute IoU between anchors (N,4) and gt_boxes (M,4).
    Boxes in [y1,x1,y2,x2] format.
    Returns IoU matrix of shape (N, M).
    """
    anchors = anchors.astype(np.float32)
    gt_boxes = gt_boxes.astype(np.float32)
    inter_y1 = np.maximum(anchors[:, None, 0], gt_boxes[None, :, 0])
    inter_x1 = np.maximum(anchors[:, None, 1], gt_boxes[None, :, 1])
    inter_y2 = np.minimum(anchors[:, None, 2], gt_boxes[None, :, 2])
    inter_x2 = np.minimum(anchors[:, None, 3], gt_boxes[None, :, 3])
    inter_h = np.maximum(inter_y2 - inter_y1, 0)
    inter_w = np.maximum(inter_x2 - inter_x1, 0)
    inter_area = inter_h * inter_w
    anchor_area = (anchors[:,2]-anchors[:,0])*(anchors[:,3]-anchors[:,1])
    gt_area = (gt_boxes[:,2]-gt_boxes[:,0])*(gt_boxes[:,3]-gt_boxes[:,1])
    union = anchor_area[:,None] + gt_area[None,:] - inter_area
    iou = inter_area / union
    return iou

# -----------------------
# Revised bbox_generation Function (Vectorized and Padded)
# -----------------------

def bbox_generation(images, targets, X_FM, Y_FM):
    """
    Compute regression targets and classification labels for all anchors.
    All anchors (generated over the full feature map) receive a target;
    anchors outside the image are ignored (label = -1).
    Returns:
       anchor_locations_all_merge: (B, total_anchors, 4)
       anchor_labels_all_merge: (B, total_anchors)
       anchors: (total_anchors, 4)
    """
    num_batch = len(images)
    C, H_IMG, W_IMG = images[0].shape

    # Generate full grid anchors over feature map
    total_positions = X_FM * Y_FM
    num_anchor_per_pos = len(ratios) * len(anchor_scales)
    total_anchors = total_positions * num_anchor_per_pos

    # Generate grid centers (using strides)
    sub_sampling_x = float(W_IMG) / X_FM
    sub_sampling_y = float(H_IMG) / Y_FM

    shift_x = np.arange(sub_sampling_x, (X_FM+1)*sub_sampling_x, sub_sampling_x)
    shift_y = np.arange(sub_sampling_y, (Y_FM+1)*sub_sampling_y, sub_sampling_y)
    shift_x, shift_y = np.meshgrid(shift_x, shift_y)  # shape (Y_FM, X_FM)
    centers = np.stack([shift_y.ravel() - sub_sampling_y/2, shift_x.ravel() - sub_sampling_x/2], axis=1)  # (total_positions, 2)

    # For each center, generate anchors
    anchors = []
    for center in centers:
        cy, cx = center
        for ratio in ratios:
            for scale in anchor_scales:
                h = sub_sampling_y * scale * np.sqrt(ratio)
                w = sub_sampling_x * scale * np.sqrt(1.0/ratio)
                y1 = cy - h/2.
                x1 = cx - w/2.
                y2 = cy + h/2.
                x2 = cx + w/2.
                anchors.append([y1, x1, y2, x2])
    anchors = np.array(anchors, dtype=np.float32)  # shape (total_anchors, 4)

    # Create ground-truth arrays for all anchors (padded to total_anchors)
    # Initialize labels to -1 (ignore) and loc targets to zeros.
    anchor_labels_all = []
    anchor_locs_all = []
    pos_iou_threshold = 0.7
    neg_iou_threshold = 0.3
    n_sample = 256
    pos_ratio = 0.5

    # Compute valid indices: those anchors that are completely inside the image.
    valid_idx = np.where((anchors[:,0] >= 0) & (anchors[:,1] >= 0) &
                         (anchors[:,2] <= H_IMG) & (anchors[:,3] <= W_IMG))[0]

    for i in range(num_batch):
        # Create full target arrays for this image.
        labels = -1 * np.ones((anchors.shape[0],), dtype=np.int32)
        locs = np.zeros((anchors.shape[0], 4), dtype=np.float32)
        gt_boxes_tensor = targets[i]["boxes"]
        if gt_boxes_tensor.numel() > 0:
            gt_boxes = gt_boxes_tensor.cpu().numpy()  # shape (M,4)
            # Compute IoU for valid anchors only.
            valid_anchors = anchors[valid_idx]
            ious = compute_iou_vectorized(valid_anchors, gt_boxes)  # (N_valid, M)
            max_ious = np.max(ious, axis=1)
            argmax_ious = np.argmax(ious, axis=1)
            # Set targets for valid anchors.
            valid_labels = -1 * np.ones((valid_anchors.shape[0],), dtype=np.int32)
            valid_labels[max_ious >= pos_iou_threshold] = 1
            valid_labels[max_ious < neg_iou_threshold] = 0
            # Ensure every GT box gets at least one positive anchor.
            gt_max_ious = np.max(ious, axis=0)  # (M,)
            for j in range(gt_boxes.shape[0]):
                inds = np.where(ious[:, j] == gt_max_ious[j])[0]
                valid_labels[inds] = 1

            # Subsample positives and negatives in valid region.
            pos_inds = np.where(valid_labels == 1)[0]
            neg_inds = np.where(valid_labels == 0)[0]
            if len(pos_inds) > int(pos_ratio * n_sample):
                disable = np.random.choice(pos_inds, size=(len(pos_inds) - int(pos_ratio * n_sample)), replace=False)
                valid_labels[disable] = -1
            remaining = n_sample - np.sum(valid_labels == 1)
            if len(neg_inds) > remaining:
                disable = np.random.choice(neg_inds, size=(len(neg_inds) - remaining), replace=False)
                valid_labels[disable] = -1

            # Compute regression targets for positive valid anchors.
            valid_locs = np.zeros((valid_anchors.shape[0], 4), dtype=np.float32)
            pos_valid_inds = np.where(valid_labels == 1)[0]
            if len(pos_valid_inds) > 0:
                pos_anchors = valid_anchors[pos_valid_inds]
                anchor_heights = pos_anchors[:,2] - pos_anchors[:,0]
                anchor_widths = pos_anchors[:,3] - pos_anchors[:,1]
                anchor_ctr_y = pos_anchors[:,0] + 0.5 * anchor_heights
                anchor_ctr_x = pos_anchors[:,1] + 0.5 * anchor_widths

                target_gt = gt_boxes[argmax_ious[pos_valid_inds]]
                gt_heights = target_gt[:,2] - target_gt[:,0]
                gt_widths = target_gt[:,3] - target_gt[:,1]
                gt_ctr_y = target_gt[:,0] + 0.5 * gt_heights
                gt_ctr_x = target_gt[:,1] + 0.5 * gt_widths

                dy = (gt_ctr_y - anchor_ctr_y) / anchor_heights
                dx = (gt_ctr_x - anchor_ctr_x) / anchor_widths
                dh = np.log(gt_heights / anchor_heights)
                dw = np.log(gt_widths / anchor_widths)
                valid_locs[pos_valid_inds, :] = np.stack([dy, dx, dh, dw], axis=1)
            # Assign computed valid targets into the full arrays.
            labels[valid_idx] = valid_labels
            locs[valid_idx, :] = valid_locs
        # Append for current image.
        anchor_labels_all.append(labels)
        anchor_locs_all.append(locs)
    anchor_labels_all_merge = np.stack(anchor_labels_all, axis=0)  # (B, total_anchors)
    anchor_locs_all_merge = np.stack(anchor_locs_all, axis=0)        # (B, total_anchors, 4)
    return anchor_locs_all_merge, anchor_labels_all_merge, anchors


def pred_bbox_to_xywh(bbox, anchors):
    anchors = anchors.astype(np.float32)
    anc_height = anchors[:,2] - anchors[:,0]
    anc_width  = anchors[:,3] - anchors[:,1]
    anc_ctr_y = anchors[:,0] + 0.5 * anc_height
    anc_ctr_x = anchors[:,1] + 0.5 * anc_width
    bbox_np = bbox.detach().cpu().numpy()
    dy = bbox_np[:,0]
    dx = bbox_np[:,1]
    dh = bbox_np[:,2]
    dw = bbox_np[:,3]
    ctr_y = dy * anc_height + anc_ctr_y
    ctr_x = dx * anc_width + anc_ctr_x
    h = np.exp(dh) * anc_height
    w = np.exp(dw) * anc_width
    roi = np.zeros(bbox_np.shape, dtype=np.float32)
    roi[:,0] = ctr_x - 0.5 * w
    roi[:,1] = ctr_y - 0.5 * h
    roi[:,2] = ctr_x + 0.5 * w
    roi[:,3] = ctr_y + 0.5 * h
    return roi



"""## RPN Network"""

vgg_model = torchvision.models.vgg16(pretrained=True).to(device)
vgg_model.eval()
for param in vgg_model.features.parameters():
    param.requires_grad = False
req_features = [layer for layer in list(vgg_model.features)[:30]]

class RPN(nn.Module):
    def __init__(self, in_channels=512, mid_channels=512, n_anchor=9):
        super(RPN, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=3, stride=1, padding=1)
        self.reg_layer = nn.Conv2d(mid_channels, n_anchor*4, kernel_size=1, stride=1)
        self.cls_layer = nn.Conv2d(mid_channels, n_anchor*2, kernel_size=1, stride=1)
        nn.init.normal_(self.conv1.weight, std=0.01)
        nn.init.constant_(self.conv1.bias, 0)
        nn.init.normal_(self.reg_layer.weight, std=0.01)
        nn.init.constant_(self.reg_layer.bias, 0)
        nn.init.normal_(self.cls_layer.weight, std=0.01)
        nn.init.constant_(self.cls_layer.bias, 0)

    def forward(self, x):
        batch_size = x.shape[0]
        x = self.conv1(x)
        pred_anchor_locs = self.reg_layer(x)
        pred_cls_scores = self.cls_layer(x)
        pred_anchor_locs = pred_anchor_locs.permute(0,2,3,1).contiguous().view(batch_size, -1, 4)
        pred_cls_scores = pred_cls_scores.permute(0,2,3,1).contiguous().view(batch_size, -1, 2)
        objectness_score = pred_cls_scores[:,:,1]
        return pred_anchor_locs, pred_cls_scores, objectness_score

rpn_model = RPN().to(device)

"""## Training"""

def compute_iou_vectorized_yxyx(boxes1, boxes2):
    """
    Compute IoU between boxes1 (N,4) and boxes2 (M,4) in y1,x1,y2,x2 format.
    Returns IoU matrix of shape (N, M).
    """
    # Convert to numpy if they're tensors
    if isinstance(boxes1, torch.Tensor):
        boxes1 = boxes1.cpu().numpy()
    if isinstance(boxes2, torch.Tensor):
        boxes2 = boxes2.cpu().numpy()

    inter_y1 = np.maximum(boxes1[:, None, 0], boxes2[None, :, 0])
    inter_x1 = np.maximum(boxes1[:, None, 1], boxes2[None, :, 1])
    inter_y2 = np.minimum(boxes1[:, None, 2], boxes2[None, :, 2])
    inter_x2 = np.minimum(boxes1[:, None, 3], boxes2[None, :, 3])

    inter_h = np.maximum(inter_y2 - inter_y1, 0)
    inter_w = np.maximum(inter_x2 - inter_x1, 0)
    inter_area = inter_h * inter_w

    area1 = (boxes1[:,2]-boxes1[:,0])*(boxes1[:,3]-boxes1[:,1])
    area2 = (boxes2[:,2]-boxes2[:,0])*(boxes2[:,3]-boxes2[:,1])

    union = area1[:,None] + area2[None,:] - inter_area
    iou = inter_area / (union + 1e-6)
    return iou


def compute_recall_at_threshold(proposals, gt_boxes, iou_thresh=0.3):
    """
    Compute recall at a fixed IoU threshold.

    For each GT box, if the best IoU among proposals is >= iou_thresh, count it as a match.
    Recall = (number of matched GT boxes) / (total GT boxes)
    """
    if len(gt_boxes) == 0:
        return 0.0

    # Compute IoU between proposals and ground truth boxes
    ious = compute_iou_vectorized_yxyx(proposals, gt_boxes)  # shape (N, M)
    best_ious = ious.max(axis=0)  # best IoU for each GT
    recall = np.sum(best_ious >= iou_thresh) / float(len(gt_boxes))
    return recall



def compute_mean_best_iou(proposals, gt_boxes):
    """
    Compute the mean best IoU across all GT boxes.

    For each GT box, we take the average of the first 5 highest IoU achieved by any proposal.
    """
    if len(gt_boxes) == 0:
        return 0.0

    ious = compute_iou_vectorized_yxyx(proposals, gt_boxes)  # shape (N, M)
    best_ious = ious.max(axis=0)
    return np.mean(best_ious)


def compute_proposal_density(proposals, gt_boxes, iou_thresh=0.5):
    """
    Compute the average number of qualifying proposals PER GT BOX.
    Should return values >= 0 (typically >> 1 for good RPN performance).
    """
    if len(gt_boxes) == 0 or len(proposals) == 0:
        return 0.0

    # Ensure numpy arrays
    proposals = np.asarray(proposals, dtype=np.float32)
    gt_boxes = np.asarray(gt_boxes, dtype=np.float32)

    # Verify box format: (N,4) where columns are [y1,x1,y2,x2]
    assert proposals.shape[1] == 4 and gt_boxes.shape[1] == 4

    # Compute IoU matrix (N proposals x M GT boxes)
    ious = compute_iou_vectorized_yxyx(proposals, gt_boxes)

    # Count qualifying proposals PER GT BOX (sum across axis=0)
    qual_counts = (ious >= iou_thresh).sum(axis=0)

    # Return average count per GT box
    return float(np.mean(qual_counts))

def plot_training_metrics(train_losses, val_metrics, eval_every):
    plt.figure(figsize=(15, 5))

    # Plot training loss
    plt.subplot(1, 3, 1)
    plt.plot(train_losses, label='Training Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training Loss')
    plt.legend()

    # Plot validation metrics if available
    if len(val_metrics) > 0:
        # Get x-axis positions for validation points
        val_epochs = [(i+1)*eval_every for i in range(len(val_metrics))]

        # Plot Average Recall
        plt.subplot(1, 3, 2)
        ar_values = [np.mean(m['recall'])  for m in val_metrics]
        plt.plot(val_epochs, ar_values, 'o-', label='Avg Recall')
        plt.xlabel('Epoch')
        plt.ylabel('Average Recall')
        plt.title('Validation Recall')
        plt.ylim(0, 1)
        plt.legend()

        # Plot Mean IoU
        plt.subplot(1, 3, 3)
        iou_values = [np.mean(m['mean_best_iou']) for m in val_metrics]
        plt.plot(val_epochs, iou_values, 'o-', label='Mean IoU')
        plt.xlabel('Epoch')
        plt.ylabel('Mean Best IoU')
        plt.title('Localization Quality')
        plt.ylim(0, 1)
        plt.legend()

    plt.tight_layout()
    plt.show()


def decode_boxes(anchors, pred_locs):
    """
    Decode regression offsets (pred_locs) to actual box coordinates based on anchors.
    This function assumes that the offsets are in the form [dy, dx, dh, dw].

    Args:
        anchors (Tensor): Anchor boxes of shape (N, 4) in [y1, x1, y2, x2] format.
        pred_locs (Tensor): Regression offsets of shape (N, 4).

    Returns:
        Tensor: Decoded boxes of shape (N, 4) in [y1, x1, y2, x2] format.
    """
    anchors = anchors.float()
    # Compute widths, heights, and center coordinates of the anchors.
    heights = anchors[:, 2] - anchors[:, 0]
    widths = anchors[:, 3] - anchors[:, 1]
    ctr_y = anchors[:, 0] + 0.5 * heights
    ctr_x = anchors[:, 1] + 0.5 * widths

    # Get the predicted offsets
    dy = pred_locs[:, 0]
    dx = pred_locs[:, 1]
    dh = pred_locs[:, 2]
    dw = pred_locs[:, 3]

    # Apply the offsets to get the predicted center
    pred_ctr_y = ctr_y + dy * heights
    pred_ctr_x = ctr_x + dx * widths
    # Compute predicted width and height
    pred_h = heights * torch.exp(dh)
    pred_w = widths * torch.exp(dw)

    # Convert center coordinates back to box coordinates
    y1 = pred_ctr_y - 0.5 * pred_h
    x1 = pred_ctr_x - 0.5 * pred_w
    y2 = pred_ctr_y + 0.5 * pred_h
    x2 = pred_ctr_x + 0.5 * pred_w

    pred_boxes = torch.stack([y1, x1, y2, x2], dim=1)
    return pred_boxes

# def train_epochs_old(req_features, rpn_model, optimizer, train_dl, epochs=20, rpn_lambda=10):
#     rpn_model.train()
#     for epoch in range(epochs):
#         print("Hi")
#         total_samples = 0
#         sum_loss = 0.0
#         sum_loss_cls = 0.0
#         sum_loss_loc = 0.0
#         for batch in train_dl:
#             images = batch["images"].to(device) # shape (B,C,H,W)
#             targets = [{"boxes": b, "labels": l} for b, l in zip(batch["boxes"], batch["labels"])]
#             B = images.shape[0]
#             #print(f"image size: {B}")
#             total_samples += B
#             # Forward through frozen backbone
#             imgs = images.clone()
#             with torch.no_grad():
#                 feat = imgs
#                 for m in req_features:
#                     feat = m(feat)
#             X_FM, Y_FM = feat.shape[2], feat.shape[3]
#             # Compute GT targets (for all anchors)
#             gt_locs_np, gt_scores_np, anchors = bbox_generation([img for img in images], targets, X_FM, Y_FM)
#             print("Hmm")
#             gt_locs = torch.from_numpy(gt_locs_np.astype(np.float32)).to(device)
#             gt_scores = torch.from_numpy(gt_scores_np.astype(np.float32)).to(device)
#             # Forward RPN
#             pred_locs, pred_scores, objectness_score = rpn_model(feat)
#             # Compute classification loss: flatten predictions and GT.
#             cls_loss = F.cross_entropy(pred_scores.view(-1,2),
#                                        gt_scores.view(-1).long(),
#                                        ignore_index=-1)
#             # Compute regression loss for positive anchors only.
#             pos_mask = gt_scores > 0
#             if pos_mask.sum() > 0:
#                 pred_pos = pred_locs[pos_mask]
#                 gt_pos = gt_locs[pos_mask]
#                 diff = torch.abs(gt_pos - pred_pos)
#                 loc_loss = torch.where(diff < 1, 0.5 * diff**2, diff - 0.5)
#                 loc_loss = loc_loss.sum() / pos_mask.sum().float()
#             else:
#                 loc_loss = torch.tensor(0.0, device=device)
#             loss = cls_loss + rpn_lambda * loc_loss

#             optimizer.zero_grad()
#             loss.backward()
#             optimizer.step()

#             sum_loss += loss.item()
#             sum_loss_cls += cls_loss.item()
#             sum_loss_loc += (rpn_lambda * loc_loss).item()
#         print(f"Epoch {epoch+1}/{epochs}: Loss {sum_loss/total_samples:.3f} | Cls {sum_loss_cls/total_samples:.3f} | Loc {sum_loss_loc/total_samples:.3f}")
#         if (epoch+1)%5==0:
#             torch.save(rpn_model.state_dict(), f"./rpn_epoch_{epoch+1}.pth")
#     return rpn_model

def train_epochs_optimized(req_features, rpn_model, optimizer, train_dl, val_dl,
                           epochs=20, rpn_lambda=10, vis_after=True):
    rpn_model.train()
    train_loss_history = []
    val_metrics_history = []
    val_loss_history = []

    for epoch in range(epochs):
        epoch_loss = 0.0
        total_samples = 0
        sum_loss_cls = 0.0
        sum_loss_loc = 0.0

        # --- Training Phase ---
        for batch in train_dl:
            images = batch["images"].to(device)
            targets = [{
                "boxes": b.to(device),
                "labels": l.to(device)
            } for b, l in zip(batch["boxes"], batch["labels"])]

            B = images.shape[0]
            total_samples += B

            # Forward through backbone
            with torch.no_grad():
                feat = images.clone()
                for m in req_features:
                    feat = m(feat)
            X_FM, Y_FM = feat.shape[2], feat.shape[3]

            # Generate anchor targets
            cpu_images = [img.cpu() for img in images]
            cpu_targets = [{"boxes": t["boxes"].cpu()} for t in targets]
            gt_locs_np, gt_scores_np, _ = bbox_generation(cpu_images, cpu_targets, X_FM, Y_FM)

            # Convert to tensors
            gt_locs = torch.from_numpy(gt_locs_np).float().to(device)
            gt_scores = torch.from_numpy(gt_scores_np).long().to(device)

            # RPN forward pass
            pred_locs, pred_scores, _ = rpn_model(feat)

            # Calculate losses
            cls_loss = F.cross_entropy(pred_scores.view(-1, 2), gt_scores.view(-1), ignore_index=-1)

            pos_mask = gt_scores > 0
            loc_loss = torch.tensor(0.0, device=device)
            if pos_mask.sum() > 0:
                pred_pos = pred_locs[pos_mask]
                gt_pos = gt_locs[pos_mask]
                diff = torch.abs(gt_pos - pred_pos)
                loc_loss = torch.where(diff < 1, 0.5 * diff**2, diff - 0.5).sum() / pos_mask.sum().float()

            loss = cls_loss + rpn_lambda * loc_loss

            # Backpropagation
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(rpn_model.parameters(), 1.0)
            optimizer.step()

            epoch_loss += loss.item()
            sum_loss_cls += cls_loss.item()
            sum_loss_loc += loc_loss.item() * rpn_lambda

        train_loss_history.append(epoch_loss / len(train_dl))

        rpn_model.eval()
        val_metrics = validate_with_nms(rpn_model, val_dl, visualize=False)
        val_metrics_history.append(val_metrics)
        val_loss_history.append(val_metrics['val_loss'])

        # Epoch statistics
        avg_loss = epoch_loss / len(train_dl)
        avg_cls = sum_loss_cls / total_samples
        avg_loc = sum_loss_loc / total_samples

        print(f"\nEpoch {epoch+1}/{epochs}")
        print(f"Train Loss: {avg_loss:.4f} | Cls: {avg_cls:.4f} | Loc: {avg_loc:.4f}")
        # Print the weighted recall (which was computed across all images in the batch)
        print(f"Val Weighted Recall@0.5: {val_metrics['weighted_recall']:.3f}")
        print(f"Mean IoU: {np.nanmean(val_metrics['mean_best_iou']):.3f}")

    # --- Final visualization after all epochs ---
    if vis_after:
        print("\n=== Final Validation Visualization ===")
        validate_with_nms(rpn_model, val_dl, top_k=20, visualize=True)

    # --- Plot training curves ---
    plot_training_curves(train_loss_history, val_loss_history, val_metrics_history)

    return rpn_model


def plot_training_curves(train_loss, val_loss, val_metrics):
    import matplotlib.pyplot as plt
    plt.figure(figsize=(15, 5))

    # Plot Training and Validation Loss
    plt.subplot(1, 2, 1)
    plt.plot(train_loss, label='Training Loss')
    plt.plot(val_loss, label='Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training and Validation Loss Progression')
    plt.legend()
    plt.grid(True)

    # Validation metrics plot (using weighted recall)
    plt.subplot(1, 2, 2)
    weighted_recalls = [m['weighted_recall'] for m in val_metrics]
    ious = [np.nanmean(m['mean_best_iou']) for m in val_metrics]
    plt.plot(weighted_recalls, label='Weighted Recall@0.5', marker='o')
    plt.plot(ious, label='Mean Best IoU', marker='s')
    plt.xlabel('Epoch')
    plt.ylabel('Metric Value')
    plt.title('Validation Metrics')
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plt.show()

"""## Validation"""

# def validate(rpn_model, data_loader, n_images=7, top_k=20):
#     rpn_model.eval()
#     with torch.no_grad():
#         batch = next(iter(data_loader))
#         images = batch["images"][:n_images].to(device)
#         targets = [{"boxes": b, "labels": l} for b, l in zip(batch["boxes"], batch["labels"])][:n_images]

#         # Forward pass
#         imgs = images.clone()
#         for m in req_features:
#             imgs = m(imgs)
#         X_FM, Y_FM = imgs.shape[2], imgs.shape[3]
#         _, _, anchors = bbox_generation([img for img in images], targets, X_FM, Y_FM)
#         pred_locs, pred_scores, objectness_score = rpn_model(imgs)

#         for i in range(min(n_images, images.shape[0])):
#             rois = pred_bbox_to_xywh(pred_locs[i], anchors)

#             if top_k is not None:
#                 # Safe handling of top_k
#                 k = min(top_k, objectness_score[i].shape[0])
#                 top_k_idx = torch.topk(objectness_score[i], k=k).indices
#                 boxes_to_show = rois[top_k_idx.cpu().numpy()]
#                 print(f"Showing {k} proposals (requested top {top_k})")
#             else:
#                 boxes_to_show = rois

#             show_corner_bbs(images[i], boxes_to_show)

#             gt_boxes = targets[i]["boxes"].cpu().numpy()
#             if len(gt_boxes) > 0:
#                 show_ground_truth_bbs(images[i], gt_boxes)

#     rpn_model.train()

def validate_with_nms_and_metrics(rpn_model, data_loader, n_images=7, score_thresh=0.7, nms_thresh=0.5):
    """Validation with metrics only, no visualization"""
    rpn_model.eval()
    metrics = {'recall': [], 'mean_best_iou': [], 'density': [], 'num_proposals': []}
    total_loss = 0.0
    total_batches = 0
    with torch.no_grad():
        batch = next(iter(data_loader))
        images = batch["images"][:n_images].to(device)
        targets = [{"boxes": b.to(device), "labels": l.to(device)} for b, l in zip(batch["boxes"], batch["labels"])][:n_images]
        total_batches += 1

        # Forward through backbone
        feat = images.clone()
        for m in req_features:
            feat = m(feat)
        X_FM, Y_FM = feat.shape[2], feat.shape[3]

        # Generate proposals
        _, _, anchors = bbox_generation([img.cpu() for img in images],
                                      [{"boxes": t["boxes"].cpu()} for t in targets],
                                      X_FM, Y_FM)
        anchors = torch.from_numpy(anchors).float().to(device)
        pred_locs, pred_scores, objectness = rpn_model(feat)

        for i in range(len(images)):
            # Decode predictions
            proposals = decode_boxes(anchors, pred_locs[i])
            scores = objectness[i]

            # Apply thresholds
            keep = scores > score_thresh
            proposals = proposals[keep]
            scores = scores[keep]

            # Apply NMS
            if len(proposals) > 0:
                boxes_xyxy = proposals[:, [1, 0, 3, 2]]  # Convert to xyxy format
                keep = nms(boxes_xyxy, scores, nms_thresh)
                final_proposals = proposals[keep]
            else:
                final_proposals = proposals

            # Get ground truth
            gt_boxes = targets[i]["boxes"]

            # Calculate metrics
            if len(gt_boxes) > 0:
                final_np = final_proposals.cpu().numpy()
                gt_np = gt_boxes.cpu().numpy()

                # Compute IoU matrix
                ious = compute_iou_vectorized_yxyx(final_np, gt_np)
                if ious.size > 0:
                    best_ious = ious.max(axis=0)
                    recall = (best_ious >= 0.5).mean()
                    mean_iou = best_ious.mean()
                    density = (ious >= 0.5).sum(axis=0).mean()
                else:
                    recall = mean_iou = density = 0.0

                metrics['recall'].append(recall)
                metrics['mean_best_iou'].append(mean_iou)
                metrics['density'].append(density)
                metrics['num_proposals'].append(len(final_proposals))
            else:
                metrics['recall'].append(0.0)
                metrics['mean_best_iou'].append(0.0)
                metrics['density'].append(0.0)
                metrics['num_proposals'].append(0)

    rpn_model.train()
    return metrics

import matplotlib.pyplot as plt
from torchvision.ops import nms
import torch.nn.functional as F

def validate_with_nms(rpn_model, data_loader,
                      score_thresh=0.7, nms_iou_thresh=0.5,
                      top_k=None, min_box_size=20,
                      metric_iou_thresh=0.5, rpn_lambda=10, visualize=False):
    rpn_model.eval()
    metrics = {
        'recall': [],
        'mean_best_iou': [],
        'density': [],
        'num_proposals': [],
        'weighted_recall': None,
        'val_loss': None
    }

    total_matches = 0
    total_gt_boxes = 0
    total_loss = 0.0
    total_batches = 0

    with torch.no_grad():
        # Process one batch from the validation data loader
        batch = next(iter(data_loader))
        images = batch["images"].to(device)
        targets = [{"boxes": b, "labels": l} for b, l in zip(batch["boxes"], batch["labels"])]

        # Forward pass through the backbone
        imgs = images.clone()
        for m in req_features:
            imgs = m(imgs)
        X_FM, Y_FM = imgs.shape[2], imgs.shape[3]

        # Generate ground-truth targets for loss computation
        cpu_images = [img.cpu() for img in images]
        cpu_targets = [{"boxes": t["boxes"].cpu()} for t in targets]
        gt_locs_np, gt_scores_np, _ = bbox_generation(cpu_images, cpu_targets, X_FM, Y_FM)
        gt_locs = torch.from_numpy(gt_locs_np).float().to(device)
        gt_scores = torch.from_numpy(gt_scores_np).long().to(device)

        # Generate anchors and forward pass through RPN
        _, _, anchors = bbox_generation([img for img in images.cpu()], targets, X_FM, Y_FM)
        pred_locs, pred_scores, objectness_score = rpn_model(imgs)

        # Compute validation loss over the batch
        cls_loss = F.cross_entropy(pred_scores.view(-1, 2), gt_scores.view(-1), ignore_index=-1)
        pos_mask = gt_scores > 0
        loc_loss = torch.tensor(0.0, device=device)
        if pos_mask.sum() > 0:
            pred_pos = pred_locs[pos_mask]
            gt_pos = gt_locs[pos_mask]
            diff = torch.abs(gt_pos - pred_pos)
            loc_loss = torch.where(diff < 1, 0.5 * diff**2, diff - 0.5).sum() / pos_mask.sum().float()
        val_loss = cls_loss + rpn_lambda * loc_loss
        total_loss += val_loss.item()
        total_batches += 1

        # Continue with proposal filtering and metric calculations
        for i in range(images.shape[0]):
            # Get proposals in original y1,x1,y2,x2 format
            proposals = pred_bbox_to_xywh(pred_locs[i], anchors)
            proposals_tensor = torch.from_numpy(proposals).float().to(device)
            scores_tensor = objectness_score[i]

            # Filter proposals by score
            score_mask = scores_tensor > score_thresh
            filtered_proposals = proposals_tensor[score_mask]
            filtered_scores = scores_tensor[score_mask]

            if len(filtered_proposals) == 0:
                print(f"Image {i+1}: No boxes passed score threshold")
                continue

            # Filter by minimum size
            heights = filtered_proposals[:, 2] - filtered_proposals[:, 0]
            widths = filtered_proposals[:, 3] - filtered_proposals[:, 1]
            size_mask = (heights > min_box_size) & (widths > min_box_size)
            filtered_proposals = filtered_proposals[size_mask]
            filtered_scores = filtered_scores[size_mask]

            if len(filtered_proposals) == 0:
                print(f"Image {i+1}: No boxes passed size threshold")
                continue

            # Convert proposals to xyxy format for NMS (convert y1,x1,y2,x2 -> x1,y1,x2,y2)
            proposals_xyxy = filtered_proposals[:, [1, 0, 3, 2]]
            keep = nms(proposals_xyxy, filtered_scores, nms_iou_thresh)
            final_proposals = filtered_proposals[keep]
            final_scores = filtered_scores[keep]

            # Apply top-k filtering if needed
            if top_k is not None and len(final_proposals) > top_k:
                top_k_idx = torch.topk(final_scores, k=top_k).indices
                final_proposals = final_proposals[top_k_idx]
                final_scores = final_scores[top_k_idx]

            # Get ground truth boxes in y1,x1,y2,x2 format, then convert to xyxy
            gt_boxes = targets[i]["boxes"].cpu().numpy()
            gt_boxes = gt_boxes[:, [1, 0, 3, 2]]

            # Compute metrics only if ground truth boxes exist
            if len(gt_boxes) > 0:
                final_proposals_np = final_proposals.cpu().numpy()
                recall_val = compute_recall_at_threshold(final_proposals_np, gt_boxes, iou_thresh=metric_iou_thresh)
                mean_iou_val = compute_mean_best_iou(final_proposals_np, gt_boxes)
                density_val = compute_proposal_density(final_proposals_np, gt_boxes, iou_thresh=metric_iou_thresh)

                metrics['recall'].append(recall_val)
                metrics['mean_best_iou'].append(mean_iou_val)
                metrics['density'].append(density_val)
                metrics['num_proposals'].append(len(final_proposals))

                # Accumulate for weighted recall
                ious = compute_iou_vectorized_yxyx(final_proposals_np, gt_boxes)
                best_ious = np.max(ious, axis=0)
                matches = np.sum(best_ious >= metric_iou_thresh)
                total_matches += matches
                total_gt_boxes += len(gt_boxes)
            else:
                metrics['recall'].append(0)
                metrics['mean_best_iou'].append(0)
                metrics['density'].append(0)
                metrics['num_proposals'].append(0)

            if visualize:
                print(f"Image {i+1}: {len(final_proposals)} proposals")
                show_corner_bbs(images[i], final_proposals.cpu().numpy())
                if len(gt_boxes) > 0:
                    show_ground_truth_bbs(images[i], gt_boxes)

    weighted_recall = total_matches / total_gt_boxes if total_gt_boxes > 0 else 0.0
    metrics['weighted_recall'] = weighted_recall
    metrics['val_loss'] = total_loss / total_batches

    print("\nValidation Metrics:")
    print(f"Weighted Recall @ IoU {metric_iou_thresh}: {weighted_recall:.3f}")
    print(f"Mean Best IoU: {np.mean(metrics['mean_best_iou']):.3f}")
    print(f"Proposal Density: {np.mean(metrics['density']):.1f}")
    print(f"Avg Proposals per Image: {np.mean(metrics['num_proposals']):.1f}")
    print(f"Validation Loss: {metrics['val_loss']:.4f}")

    rpn_model.train()
    return metrics

"""## Testing"""

image_dir = 'trainA_original_700'
pt_dir = 'trainA_testing2'
json_file_path = 'bdd100k_labels_images_train.json'

# Extract labels from JSON (adjust number as desired)
all_labels = extract_first_n_labels(json_file_path, 20000)

# Create the custom dataset using your method
dataset = CustomDataset(image_dir, all_labels, pt_dir)

# Split using random_split (70% train, 15% val, 15% test)
train_size = int(0.7 * len(dataset))
val_size = int(0.15 * len(dataset))
test_size = len(dataset) - train_size - val_size
train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])
print(f"Training dataset size: {len(train_dataset)}")
print(f"Validation dataset size: {len(val_dataset)}")
print(f"Testing dataset size: {len(test_dataset)}")

# Create DataLoaders using your custom collate function
batch_size = 16
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn, num_workers=2)
val_loader   = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn, num_workers=2)

small_train_dataset = torch.utils.data.Subset(train_dataset, list(range(30)))
small_train_loader = torch.utils.data.DataLoader(small_train_dataset, batch_size=batch_size, shuffle=True,
                                           collate_fn=custom_collate_fn, num_workers=2)

# Train the RPN using the training DataLoader
# trained_rpn = train_epochs(req_features, rpn_model, optimizer, small_train_loader, epochs=20, rpn_lambda=10)
rpn_model = RPN().to(device)
optimizer = torch.optim.Adam(rpn_model.parameters(), lr=0.001)

train_epochs_optimized(req_features, rpn_model, optimizer, small_train_loader, val_loader, epochs=30, rpn_lambda=8, vis_after=True)

small_train_dataset = torch.utils.data.Subset(train_dataset, list(range(50)))
small_train_loader = torch.utils.data.DataLoader(small_train_dataset, batch_size=batch_size, shuffle=True,
                                           collate_fn=custom_collate_fn, num_workers=2)

for param in vgg_model.features[-4:].parameters():
    param.requires_grad = True

base_lr = 0.001
rpn_model = RPN().to(device)

# Get backbone parameters from the VGG model
backbone_params = list(vgg_model.features.parameters())
# Get new parameters from the custom layers (e.g., RPN)
new_params = list(rpn_model.parameters())

optimizer = torch.optim.Adam([
    {'params': backbone_params, 'lr': base_lr * 0.1},
    {'params': new_params, 'lr': base_lr}
])
# optimizer = torch.optim.Adam(rpn_model.parameters(), lr=0.0005)

train_epochs_optimized(req_features, rpn_model, optimizer, small_train_loader, val_loader, epochs=100, rpn_lambda=5, vis_after=True)

print("Validation on training data:")
validate_with_nms_and_metrics(trained_rpn, train_loader)

"""# ROI Integration"""

from torchvision.ops import RoIAlign, nms

class StandardROIHead(nn.Module):
    def __init__(self, in_channels=512, out_channels=512, roi_size=(7,7), num_classes=9):
      super().__init__()
      self.roi_pool = RoIAlign(roi_size, spatial_scale=1.0, sampling_ratio=2)
      self.num_classes = num_classes

      # Network layers
      self.fc1 = nn.Linear(in_channels * roi_size[0] * roi_size[1], out_channels)
      self.fc2 = nn.Linear(out_channels, out_channels)

      # Prediction heads
      self.cls_score = nn.Linear(out_channels, num_classes + 1)
      self.bbox_pred = nn.Linear(out_channels, 4 * num_classes)

      self._init_weights()

    def _init_weights(self):
        for m in [self.fc1, self.fc2]:
            nn.init.normal_(m.weight, std=0.01)
            nn.init.constant_(m.bias, 0)
        nn.init.normal_(self.cls_score.weight, std=0.01)
        nn.init.constant_(self.cls_score.bias, 0)
        nn.init.normal_(self.bbox_pred.weight, std=0.001)
        nn.init.constant_(self.bbox_pred.bias, 0)

    def forward(self, features, proposals, image_shapes):
        """
        Args:
            features: (N, C, H, W)
            proposals: List[Tensor] each in (y1, x1, y2, x2) format
            image_shapes: List[Tuple[height, width]]
        """
        # Convert proposals to xyxy for RoIAlign
        proposals_xyxy = [torch.stack([
            boxes[:, 1],  # x1
            boxes[:, 0],  # y1
            boxes[:, 3],  # x2
            boxes[:, 2]   # y2
        ], dim=1) for boxes in proposals]

        # Calculate spatial scale (fixed version)
        spatial_scales = []
        for img_shape in image_shapes:
            img_h, img_w = img_shape
            feat_h, feat_w = features.shape[2:]
            scale_h = feat_h / img_h
            scale_w = feat_w / img_w
            spatial_scales.append((scale_h + scale_w) / 2)

        spatial_scale = sum(spatial_scales) / len(spatial_scales)

        # Prepare ROIs in [batch_idx, x1, y1, x2, y2] format
        rois = []
        for i, (boxes, img_shape) in enumerate(zip(proposals_xyxy, image_shapes)):
            if len(boxes) > 0:
                boxes = self.clip_boxes_to_image(boxes, img_shape)
                batch_idx = torch.full((len(boxes), 1), i, dtype=torch.float32, device=features.device)
                rois.append(torch.cat([batch_idx, boxes], dim=1))

        rois = torch.cat(rois) if len(rois) > 0 else torch.empty((0, 5), device=features.device)

        # ROI Pooling with dynamic spatial scale
        self.roi_pool.spatial_scale = spatial_scale
        pooled_features = self.roi_pool(features, rois)

        # Forward pass
        x = pooled_features.flatten(1)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))

        return {
            'cls_scores': self.cls_score(x),
            'bbox_preds': self.bbox_pred(x),
            'roi_features': x,
            'proposals': proposals
        }

    def clip_boxes_to_image(self, boxes_xyxy, image_size):
        """Clip boxes in xyxy format to image boundaries"""
        boxes_xyxy[:, [0, 2]] = boxes_xyxy[:, [0, 2]].clamp(0, image_size[1])  # x coordinates
        boxes_xyxy[:, [1, 3]] = boxes_xyxy[:, [1, 3]].clamp(0, image_size[0])  # y coordinates
        return boxes_xyxy

class FasterRCNN(nn.Module):
    def __init__(self, backbone, rpn, roi_head):
        super().__init__()
        self.backbone = backbone
        self.rpn = rpn
        self.roi_head = roi_head

    def forward(self, images, targets=None):
        image_shapes = [img.shape[-2:] for img in images]  # (H, W) tuples

        # 1. Backbone
        features = self.backbone(images)

        # 2. RPN (using your existing implementation)
        # Forward pass through RPN
        pred_anchor_locs, pred_cls_scores, objectness_score = self.rpn(features)

        # Generate anchors (using your existing bbox_generation function)
        _, _, anchors = bbox_generation(
            [img for img in images],
            targets if self.training else [{"boxes": torch.zeros((0, 4)), "labels": torch.zeros((0,))}] * len(images),
            features.shape[3],  # X_FM
            features.shape[2]   # Y_FM
        )
        anchors = torch.from_numpy(anchors).float().to(images.device)

        # Convert RPN outputs to proposals
        proposals = []
        for i in range(len(images)):
            # Decode proposals for this image
            proposals_i = decode_boxes(anchors, pred_anchor_locs[i])
            proposals.append(proposals_i)

        if self.training:
            # Training - compute RPN losses and ROI losses
            assert targets is not None, "Targets must be provided during training"

            # Compute RPN losses (using your existing code)
            gt_locs_np, gt_scores_np, _ = bbox_generation(
                [img for img in images],
                targets,
                features.shape[3],  # X_FM
                features.shape[2]   # Y_FM
            )
            gt_locs = torch.from_numpy(gt_locs_np.astype(np.float32)).to(images.device)
            gt_scores = torch.from_numpy(gt_scores_np.astype(np.float32)).to(images.device)

            # RPN classification loss
            rpn_cls_loss = F.cross_entropy(
                pred_cls_scores.view(-1, 2),
                gt_scores.view(-1).long(),
                ignore_index=-1
            )

            # RPN regression loss
            pos_mask = gt_scores > 0
            if pos_mask.sum() > 0:
                pred_pos = pred_anchor_locs[pos_mask]
                gt_pos = gt_locs[pos_mask]
                diff = torch.abs(gt_pos - pred_pos)
                rpn_reg_loss = torch.where(diff < 1, 0.5 * diff**2, diff - 0.5)
                rpn_reg_loss = rpn_reg_loss.sum() / pos_mask.sum().float()
            else:
                rpn_reg_loss = torch.tensor(0.0, device=images.device)

            # ROI Head forward and losses
            roi_outputs = self.roi_head(features, proposals, image_shapes)
            roi_losses = self.compute_losses(roi_outputs, targets)

            return {
                'loss_objectness': rpn_cls_loss,
                'loss_rpn_box_reg': rpn_reg_loss,
                'loss_cls': roi_losses['loss_cls'],
                'loss_box_reg': roi_losses['loss_box_reg']
            }
        else:
            # Inference - return detections
            roi_outputs = self.roi_head(features, proposals, image_shapes)
            return self.postprocess_detections(roi_outputs, image_shapes)

    def compute_losses(self, roi_outputs, targets):
      """
      Compute classification and regression losses for ROI heads.
      Args:
          roi_outputs: Dict containing:
              - cls_scores (Tensor): Class logits [N, num_classes + 1]
              - bbox_preds (Tensor): Box offsets [N, 4 * num_classes] (dy, dx, dh, dw)
              - proposals (List[Tensor]): Proposal boxes in yxyx format
          targets: List[Dict] containing:
              - boxes (Tensor): Ground truth boxes in yxyx format
              - labels (Tensor): Class labels
      Returns:
          Dict with classification and regression losses
      """
      # 1. Prepare targets for sampled proposals
      sampled_proposals, gt_labels, gt_box_deltas = self._match_proposals_to_targets(
          roi_outputs['proposals'],
          targets
      )

      # 2. Classification loss (cross-entropy)
      loss_cls = F.cross_entropy(
          roi_outputs['cls_scores'],
          gt_labels,
          reduction='mean'
      )

      # 3. Box regression loss (smooth L1)
      pos_mask = gt_labels > 0  # Only positive samples contribute
      num_pos = pos_mask.sum().item()

      if num_pos > 0:
          # Get predictions for the correct class
          bbox_pred = roi_outputs['bbox_preds'].view(-1, self.roi_head.num_classes, 4)
          idx = gt_labels[pos_mask] - 1  # 0 is background
          pred_box_deltas = bbox_pred[pos_mask, idx, :]

          loss_box_reg = F.smooth_l1_loss(
              pred_box_deltas,
              gt_box_deltas[pos_mask],
              reduction='sum'
          ) / num_pos
      else:
          loss_box_reg = torch.tensor(0., device=roi_outputs['cls_scores'].device)

      return {
          'loss_cls': loss_cls,
          'loss_box_reg': loss_box_reg
      }

def _match_proposals_to_targets(self, proposals, targets):
    """
    Match proposals to ground truth and compute regression targets.

    Returns:
        sampled_proposals: Tensor of matched proposals in yxyx format
        labels: Tensor of class labels (0=background)
        box_deltas: Tensor of regression targets (dy, dx, dh, dw)
    """
    all_sampled_proposals = []
    all_labels = []
    all_box_deltas = []

    for proposals_per_img, target in zip(proposals, targets):
        gt_boxes = target['boxes']  # yxyx format
        gt_labels = target['labels']

        if len(gt_boxes) == 0:
            # No ground truth - mark all as background
            labels = torch.zeros(len(proposals_per_img),
                              dtype=torch.long,
                              device=proposals_per_img.device)
            box_deltas = torch.zeros((len(proposals_per_img), 4),
                                    device=proposals_per_img.device)
        else:
            # 1. Compute IoU between proposals and gt boxes
            ious = ops.box_iou(
                # Convert to xyxy for torchvision
                torch.stack([
                    proposals_per_img[:, 1],
                    proposals_per_img[:, 0],
                    proposals_per_img[:, 3],
                    proposals_per_img[:, 2]
                ], dim=1),
                torch.stack([
                    gt_boxes[:, 1],
                    gt_boxes[:, 0],
                    gt_boxes[:, 3],
                    gt_boxes[:, 2]
                ], dim=1)
            )

            # 2. Assign each proposal to best matching gt
            max_ious, gt_assignment = ious.max(dim=1)

            # 3. Set labels (0=background, 1...num_classes=object classes)
            labels = gt_labels[gt_assignment]
            labels[max_ious < 0.5] = 0  # Background threshold

            # 4. Compute box deltas for positive samples
            pos_mask = labels > 0
            pos_proposals = proposals_per_img[pos_mask]
            pos_gt_boxes = gt_boxes[gt_assignment[pos_mask]]

            # Convert boxes to (ctr_y, ctr_x, h, w)
            proposals_h = pos_proposals[:, 2] - pos_proposals[:, 0]
            proposals_w = pos_proposals[:, 3] - pos_proposals[:, 1]
            proposals_ctr_y = pos_proposals[:, 0] + 0.5 * proposals_h
            proposals_ctr_x = pos_proposals[:, 1] + 0.5 * proposals_w

            gt_h = pos_gt_boxes[:, 2] - pos_gt_boxes[:, 0]
            gt_w = pos_gt_boxes[:, 3] - pos_gt_boxes[:, 1]
            gt_ctr_y = pos_gt_boxes[:, 0] + 0.5 * gt_h
            gt_ctr_x = pos_gt_boxes[:, 1] + 0.5 * gt_w

            # Compute deltas (dy, dx, dh, dw)
            eps = 1e-5  # Avoid division by zero
            dy = (gt_ctr_y - proposals_ctr_y) / (proposals_h + eps)
            dx = (gt_ctr_x - proposals_ctr_x) / (proposals_w + eps)
            dh = torch.log(gt_h / (proposals_h + eps))
            dw = torch.log(gt_w / (proposals_w + eps))

            box_deltas = torch.stack([dy, dx, dh, dw], dim=1)

            # Initialize full tensors
            full_box_deltas = torch.zeros((len(proposals_per_img), 4),
                                      device=proposals_per_img.device)
            full_box_deltas[pos_mask] = box_deltas

        all_sampled_proposals.append(proposals_per_img)
        all_labels.append(labels)
        all_box_deltas.append(full_box_deltas)

    return (
        torch.cat(all_sampled_proposals),
        torch.cat(all_labels),
        torch.cat(all_box_deltas)
    )

    def postprocess_detections(self, roi_outputs, image_shapes):
        # Convert box offsets to final coordinates
        proposals = roi_outputs['proposals']  # yxyx format
        box_deltas = roi_outputs['bbox_preds']  # dy, dx, dh, dw

        # Apply deltas to proposals (in yxyx format)
        final_boxes = self.apply_box_deltas(proposals, box_deltas)

        # Clip to image boundaries
        final_boxes = [self.clip_boxes_to_image(boxes, img_shape)
                      for boxes, img_shape in zip(final_boxes, image_shapes)]

        # NMS and score thresholding
        return self.filter_detections(
            final_boxes,
            roi_outputs['cls_scores'],
            image_shapes
        )

    def apply_box_deltas(self, boxes, deltas):
        """Convert box deltas to final box coordinates (yxyx format)"""
        # boxes: List[Tensor[N, 4]] in yxyx
        # deltas: Tensor[N, 4*num_classes]
        results = []
        for i, (boxes_per_img, deltas_per_img) in enumerate(zip(boxes, deltas)):
            # Convert boxes to (ctr_y, ctr_x, h, w)
            heights = boxes_per_img[:, 2] - boxes_per_img[:, 0]
            widths = boxes_per_img[:, 3] - boxes_per_img[:, 1]
            ctr_y = boxes_per_img[:, 0] + 0.5 * heights
            ctr_x = boxes_per_img[:, 1] + 0.5 * widths

            # Apply deltas (dy, dx, dh, dw)
            dy = deltas_per_img[:, 0::4]  # For each class
            dx = deltas_per_img[:, 1::4]
            dh = deltas_per_img[:, 2::4]
            dw = deltas_per_img[:, 3::4]

            # Calculate new centers and dimensions
            pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]
            pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]
            pred_h = torch.exp(dh) * heights[:, None]
            pred_w = torch.exp(dw) * widths[:, None]

            # Convert back to yxyx format
            y1 = pred_ctr_y - 0.5 * pred_h
            x1 = pred_ctr_x - 0.5 * pred_w
            y2 = pred_ctr_y + 0.5 * pred_h
            x2 = pred_ctr_x + 0.5 * pred_w

            results.append(torch.stack([y1, x1, y2, x2], dim=2))
        return results

    def filter_detections(self, boxes, scores, image_shapes):
        """Apply NMS and score thresholding (boxes in yxyx format)"""
        # Convert to xyxy for NMS
        boxes_xyxy = [torch.stack([
            boxes_img[:, :, 1],  # x1
            boxes_img[:, :, 0],  # y1
            boxes_img[:, :, 3],  # x2
            boxes_img[:, :, 2]   # y2
        ], dim=2) for boxes_img in boxes]

        # Process each image independently
        detections = []
        for boxes_img, scores_img, img_shape in zip(boxes_xyxy, scores, image_shapes):
            # Flatten across classes
            boxes_img = boxes_img.view(-1, 4)
            scores_img = scores_img.view(-1)

            # Apply NMS
            keep = nms(boxes_img, scores_img, iou_threshold=0.7)
            boxes_img = boxes_img[keep]
            scores_img = scores_img[keep]

            # Convert back to yxyx
            detections.append({
                'boxes': torch.stack([
                    boxes_img[:, 1],  # y1
                    boxes_img[:, 0],  # x1
                    boxes_img[:, 3],  # y2
                    boxes_img[:, 2]   # x2
                ], dim=1),
                'scores': scores_img,
                'labels': keep % self.roi_head.num_classes
            })
        return detections

def train_faster_rcnn(model, optimizer, train_loader, epochs=10, print_every=50):
    model.train()
    for epoch in range(epochs):
        print(f"Epoch {epoch+1}/{epochs}")
        total_loss = 0.0
        total_rpn_loss = 0.0
        total_roi_loss = 0.0

        for i, batch in enumerate(train_loader):
            images = batch["images"].to(device)
            targets = []
            for boxes, labels in zip(batch["boxes"], batch["labels"]):
                targets.append({
                    "boxes": boxes.to(device),
                    "labels": labels.to(device)
                })

            # Forward pass
            losses = model(images, targets)

            # Sum all losses
            loss = losses['loss_objectness'] + losses['loss_rpn_box_reg'] + \
                   losses['loss_cls'] + losses['loss_box_reg']

            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            # Accumulate losses
            total_loss += loss.item()
            total_rpn_loss += (losses['loss_objectness'] + losses['loss_rpn_box_reg']).item()
            total_roi_loss += (losses['loss_cls'] + losses['loss_box_reg']).item()

            if (i+1) % print_every == 0:
                avg_loss = total_loss / print_every
                avg_rpn = total_rpn_loss / print_every
                avg_roi = total_roi_loss / print_every
                print(f"Batch {i+1} | Loss: {avg_loss:.4f} | RPN: {avg_rpn:.4f} | ROI: {avg_roi:.4f}")
                total_loss = 0.0
                total_rpn_loss = 0.0
                total_roi_loss = 0.0

        # Save model checkpoint
        torch.save(model.state_dict(), f"faster_rcnn_epoch_{epoch+1}.pth")

def validate_faster_rcnn(model, val_loader, score_thresh=0.7, n_images=5):
    model.eval()
    with torch.no_grad():
        batch = next(iter(val_loader))
        images = batch["images"][:n_images].to(device)
        targets = [{"boxes": b, "labels": l} for b, l in zip(batch["boxes"], batch["labels"])][:n_images]

        # Forward pass
        detections = model(images)

        # Visualize results
        for i in range(min(n_images, images.shape[0])):
            img = images[i]
            det = detections[i]
            gt = targets[i]

            # Filter detections by score
            keep = det['scores'] > score_thresh
            det_boxes = det['boxes'][keep].cpu().numpy()
            det_scores = det['scores'][keep].cpu().numpy()
            det_labels = det['labels'][keep].cpu().numpy()

            # Show image with detections
            plt.figure(figsize=(12, 8))
            im_np = unnormalize_tensor(img)
            plt.imshow(np.transpose(im_np, (1, 2, 0)))

            # Draw detections
            for box, score, label in zip(det_boxes, det_scores, det_labels):
                color = 'red'
                rect = plt.Rectangle((box[1], box[0]), box[3]-box[1], box[2]-box[0],
                                   fill=False, color=color, linewidth=2)
                plt.gca().add_patch(rect)
                plt.gca().text(box[1], box[0]-2, f'{label}: {score:.2f}',
                             bbox=dict(facecolor=color, alpha=0.5),
                             fontsize=8, color='white')

            # Draw ground truth
            gt_boxes = gt['boxes'].cpu().numpy()
            for box in gt_boxes:
                rect = plt.Rectangle((box[1], box[0]), box[3]-box[1], box[2]-box[0],
                                   fill=False, color='blue', linewidth=2)
                plt.gca().add_patch(rect)

            plt.title(f"Image {i+1} - Detections (red) vs GT (blue)")
            plt.axis('off')
            plt.show()

    model.train()

# Initialize components
backbone = nn.Sequential(*list(vgg_model.features.children())[:30])
roi_head = StandardROIHead(num_classes=9)  # Set to your actual number of classes
faster_rcnn = FasterRCNN(backbone, rpn_model, roi_head).to(device)

# Create optimizer
optimizer = torch.optim.Adam([
    {'params': faster_rcnn.rpn.parameters()},
    {'params': faster_rcnn.roi_head.parameters()}
], lr=0.001)

# Train
train_faster_rcnn(faster_rcnn, optimizer, train_loader, epochs=10)

# Validate
validate_faster_rcnn(faster_rcnn, val_loader)