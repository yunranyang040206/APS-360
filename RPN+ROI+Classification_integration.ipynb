{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.models as models\n",
    "import ijson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class mapping (same for training & inference)\n",
    "name_to_id = {\"traffic light\": 0,\n",
    "    \"traffic sign\": 1,\n",
    "    \"car\": 2,\n",
    "    \"person\": 3,\n",
    "    \"bus\": 4,\n",
    "    \"truck\": 5,\n",
    "    \"rider\": 6,\n",
    "    \"bike\": 7,\n",
    "    \"motor\": 8,\n",
    "    \"train\": 9\n",
    "}\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    \"\"\"Custom collate function for DataLoader.\"\"\"\n",
    "    all_patches = []\n",
    "    all_labels = []\n",
    "\n",
    "    for patches, labels in batch:\n",
    "        all_patches.append(patches)  # Each image has a different number of patches\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    return all_patches, all_labels  # Keep them as lists instead of stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RPN+ROI Model\n",
    "from sklearn.cluster import DBSCAN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.ops as ops\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, channels, reduction=4, kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channels, channels//reduction, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(channels//reduction, channels, bias=False)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.conv = nn.Conv2d(2,1,kernel_size,padding=kernel_size//2,bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b,c,h,w = x.shape\n",
    "        # Channel\n",
    "        y_avg = self.avg_pool(x).view(b,c)\n",
    "        y_max = self.max_pool(x).view(b,c)\n",
    "        y = self.fc(y_avg) + self.fc(y_max)\n",
    "        scale = self.sigmoid(y).view(b,c,1,1)\n",
    "        x = x*scale\n",
    "\n",
    "        # Spatial\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out,_ = torch.max(x, dim=1, keepdim=True)\n",
    "        y = torch.cat([avg_out,max_out], dim=1)\n",
    "        scale = self.sigmoid(self.conv(y))\n",
    "        return x*scale\n",
    "\n",
    "\n",
    "class EnhancedRPNWithROI(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels=512, \n",
    "                 mid_channels=256, \n",
    "                 n_anchor=15,  # must match len(ratios)*len(scales)\n",
    "                 pool_size=(7,7), \n",
    "                 nms_thresh=0.5,\n",
    "                 conf_thresh=0.5,\n",
    "                 top_n=400):\n",
    "        \"\"\"\n",
    "        Largely the same as your EnhancedRPN, but adds ROI pooling and a _process_proposals method.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # --- The same RPN body as your EnhancedRPN ---\n",
    "        self.conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(mid_channels, mid_channels, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(mid_channels, mid_channels, kernel_size=3, padding=1)\n",
    "\n",
    "        # If you want your CBAM modules:\n",
    "        self.cbam1 = CBAM(mid_channels, reduction=4, kernel_size=3)\n",
    "        self.cbam2 = CBAM(mid_channels, reduction=4, kernel_size=3)\n",
    "\n",
    "        self.reg_layer = nn.Conv2d(mid_channels, n_anchor*4, kernel_size=1)\n",
    "        self.cls_layer = nn.Conv2d(mid_channels, n_anchor*2, kernel_size=1)\n",
    "\n",
    "        self.skip_conv = nn.Conv2d(in_channels, mid_channels, kernel_size=1)\n",
    "        if in_channels == mid_channels:\n",
    "            nn.init.eye_(self.skip_conv.weight)\n",
    "            nn.init.zeros_(self.skip_conv.bias)\n",
    "            self.skip_conv.weight.requires_grad = False\n",
    "            self.skip_conv.bias.requires_grad = False\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "        # --- ROI pooling layer ---\n",
    "        #   You can use RoIPool or RoIAlign. \n",
    "        #   Below is ops.RoIPool, but if you want bilinear interpolation,\n",
    "        #   you might prefer RoIAlign(pooled_height=pool_size[0], pooled_width=pool_size[1], ...)\n",
    "        self.roi_pool = ops.RoIPool(output_size=pool_size, spatial_scale=1.0)\n",
    "\n",
    "        # Store thresholds for NMS, etc.\n",
    "        self.nms_thresh = nms_thresh\n",
    "        self.conf_thresh = conf_thresh\n",
    "        self.top_n = top_n\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for layer in [self.conv1, self.conv2, self.conv3, self.reg_layer, self.cls_layer]:\n",
    "            nn.init.normal_(layer.weight, std=0.01)\n",
    "            if layer.bias is not None:\n",
    "                nn.init.constant_(layer.bias, 0)\n",
    "\n",
    "    def forward(self, x, anchors=None, do_roi=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: feature map from the backbone, shape (B, C=512, H, W)\n",
    "            anchors: (total_anchors, 4) if you want to decode proposals\n",
    "            do_roi: bool. If True, we also do the ROI pooling step, returning pooled features.\n",
    "\n",
    "        Returns:\n",
    "            pred_locs: (B, #anchors, 4)\n",
    "            pred_scores: (B, #anchors, 2)\n",
    "            objectness_score: (B, #anchors)\n",
    "            (optionally) pooled_feats: if do_roi==True and anchors is not None\n",
    "        \"\"\"\n",
    "        residual = self.skip_conv(x)\n",
    "\n",
    "        # block1 + CBAM\n",
    "        x = Fn.relu(self.conv1(x))\n",
    "        x = self.cbam1(x)\n",
    "\n",
    "        # block2 + CBAM\n",
    "        x = Fn.relu(self.conv2(x))\n",
    "        x = self.cbam2(x)\n",
    "\n",
    "        # block3 + residual\n",
    "        x = Fn.relu(self.conv3(x) + residual)\n",
    "\n",
    "        # RPN heads\n",
    "        B = x.size(0)\n",
    "        pred_anchor_locs = self.reg_layer(x)      # shape (B, n_anchor*4, H, W)\n",
    "        pred_cls_scores  = self.cls_layer(x)      # shape (B, n_anchor*2, H, W)\n",
    "\n",
    "        pred_anchor_locs = pred_anchor_locs.permute(0,2,3,1).contiguous().view(B, -1, 4)\n",
    "        pred_cls_scores  = pred_cls_scores.permute(0,2,3,1).contiguous().view(B, -1, 2)\n",
    "\n",
    "        # optional tanh for loc\n",
    "        pred_anchor_locs = torch.tanh(pred_anchor_locs) * 2\n",
    "        objectness_score = Fn.softmax(pred_cls_scores, dim=-1)[..., 1]  # shape (B, #anchors)\n",
    "\n",
    "        if anchors is None or (not do_roi):\n",
    "            # If we don't want ROI pooling, just return the normal RPN outputs\n",
    "            return pred_anchor_locs, pred_cls_scores, objectness_score\n",
    "\n",
    "        # else we want proposals + ROI pooling\n",
    "        proposals = self._generate_proposals(pred_anchor_locs, anchors)  # shape (B, #anchors, 4)\n",
    "\n",
    "        # _process_proposals will do NMS, thresholding, and ROI pooling\n",
    "        pooled_feats = self._process_proposals(x, proposals, objectness_score)\n",
    "\n",
    "        return pred_anchor_locs, pred_cls_scores, objectness_score, pooled_feats\n",
    "\n",
    "    def _generate_proposals(self, pred_locs, anchors):\n",
    "        \"\"\"\n",
    "        Convert anchor offsets to box coords [x1, y1, x2, y2].\n",
    "        pred_locs: (B, N, 4) => offsets [dy, dx, dh, dw]\n",
    "        anchors:   (N, 4) in [x1, y1, x2, y2]\n",
    "        \"\"\"\n",
    "        B, N, _ = pred_locs.size()\n",
    "        proposals = torch.zeros_like(pred_locs)  # (B, N, 4)\n",
    "\n",
    "        # anchors => float on same device\n",
    "        anchors = anchors.to(pred_locs.device)\n",
    "\n",
    "        # anchor geometry\n",
    "        anc_w = anchors[:, 2] - anchors[:, 0]  # x2 - x1\n",
    "        anc_h = anchors[:, 3] - anchors[:, 1]  # y2 - y1\n",
    "        anc_ctr_x = anchors[:, 0] + 0.5*anc_w\n",
    "        anc_ctr_y = anchors[:, 1] + 0.5*anc_h\n",
    "\n",
    "        dy = pred_locs[..., 0]\n",
    "        dx = pred_locs[..., 1]\n",
    "        dh = pred_locs[..., 2]\n",
    "        dw = pred_locs[..., 3]\n",
    "\n",
    "        # decode\n",
    "        ctr_y = dy * anc_h[None, :] + anc_ctr_y[None, :]\n",
    "        ctr_x = dx * anc_w[None, :] + anc_ctr_x[None, :]\n",
    "        h = torch.exp(dh) * anc_h[None, :]\n",
    "        w = torch.exp(dw) * anc_w[None, :]\n",
    "\n",
    "        # final\n",
    "        proposals[..., 0] = ctr_x - 0.5*w\n",
    "        proposals[..., 1] = ctr_y - 0.5*h\n",
    "        proposals[..., 2] = ctr_x + 0.5*w\n",
    "        proposals[..., 3] = ctr_y + 0.5*h\n",
    "\n",
    "        return proposals\n",
    "\n",
    "    def _process_proposals(self, conv_features, proposals, scores):\n",
    "        \"\"\"\n",
    "        For each image in the batch:\n",
    "          - Filter proposals by self.conf_thresh\n",
    "          - NMS\n",
    "          - Keep top_n\n",
    "          - ROI Pool\n",
    "        Return pooled features.\n",
    "        \"\"\"\n",
    "        B = conv_features.size(0)\n",
    "        pooled_list = []\n",
    "\n",
    "        for b_idx in range(B):\n",
    "            cur_scores = scores[b_idx]      # shape (#anchors,)\n",
    "            cur_props = proposals[b_idx]    # shape (#anchors, 4)\n",
    "\n",
    "            # 1) Confidence threshold\n",
    "            conf_mask = cur_scores > self.conf_thresh\n",
    "            filtered_boxes  = cur_props[conf_mask]\n",
    "            filtered_scores = cur_scores[conf_mask]\n",
    "\n",
    "            if filtered_boxes.size(0) == 0:\n",
    "                # no proposals left\n",
    "                pooled_list.append(torch.empty(0, device=conv_features.device))\n",
    "                continue\n",
    "\n",
    "            # 2) NMS\n",
    "            keep_idx = ops.nms(filtered_boxes, filtered_scores, self.nms_thresh)\n",
    "            keep_idx = keep_idx[:self.top_n]  # top top_n after NMS\n",
    "            final_boxes = filtered_boxes[keep_idx]\n",
    "\n",
    "            # 3) ROI Pool\n",
    "            # Format => [batch_ind, x1, y1, x2, y2]\n",
    "            roi_input = torch.cat([\n",
    "                torch.full((final_boxes.size(0),1), b_idx, device=conv_features.device, dtype=torch.float32),\n",
    "                final_boxes\n",
    "            ], dim=1)\n",
    "\n",
    "            # shape => (N_proposals, C, pool_size[0], pool_size[1])\n",
    "            pooled = self.roi_pool(conv_features, roi_input)\n",
    "            pooled_list.append(pooled)\n",
    "\n",
    "        # Combine into a single tensor if you want\n",
    "        pooled_feats = torch.cat(pooled_list, dim=0)\n",
    "        return pooled_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification model\n",
    "# Transfer learning with resnet18 - will be fine tune in training\n",
    "class ObjectClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=10, dropout_p=0.5, freeze_backbone=True):\n",
    "        super(ObjectClassifier, self).__init__()\n",
    "        self.model = models.resnet18(pretrained=True)\n",
    "\n",
    "        # Optionally freeze early layers\n",
    "        if freeze_backbone:\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.model.layer4.parameters():\n",
    "                param.requires_grad = True\n",
    "            for param in self.model.fc.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "        # Replace the classification head with dropout + linear\n",
    "        in_features = self.model.fc.in_features\n",
    "        self.model.fc = nn.Sequential(\n",
    "            nn.Dropout(p=dropout_p),\n",
    "            nn.Linear(in_features, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image and image processing\n",
    "import torch\n",
    "import torch.nn.functional as Fn\n",
    "import torchvision.transforms.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# loading image\n",
    "image_dir = \"trainA_original_700\"\n",
    "image_filename = \"00fc910e-bce87172.jpg\"  # Replace with an actual image name\n",
    "image_path = os.path.join(image_dir, image_filename)\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "image_tensor = transforms.ToTensor()(image).unsqueeze(0)  # .pt file, (3, 720, 1280) dim\n",
    "\n",
    "\n",
    "def contrast_stretch(image, low_percentile=10, high_percentile=90):\n",
    "    \"\"\"\n",
    "    Perform contrast stretching while printing debug info to avoid full black images.\n",
    "\n",
    "    :param image: PyTorch tensor of shape (C, H, W)\n",
    "    :param low_percentile: Lower percentile for clipping\n",
    "    :param high_percentile: Upper percentile for clipping\n",
    "    :return: Contrast-stretched tensor\n",
    "    \"\"\"\n",
    "    image_np = image.cpu().numpy()\n",
    "\n",
    "    # Compute percentiles\n",
    "    min_val = np.percentile(image_np, low_percentile)\n",
    "    max_val = np.percentile(image_np, high_percentile)\n",
    "\n",
    "    print(f\"Debug: Min percentile value = {min_val}, Max percentile value = {max_val}\")\n",
    "\n",
    "    if max_val - min_val < 1e-6:\n",
    "        print(\"Warning: Min and max values are too close! Returning original image.\")\n",
    "        return image  # Return original image to avoid black output\n",
    "\n",
    "    # Apply contrast stretching\n",
    "    stretched = (image_np - min_val) / (max_val - min_val + 1e-8)\n",
    "\n",
    "    # Clip values to avoid over-brightening\n",
    "    stretched = np.clip(stretched, 0, 1)\n",
    "\n",
    "    return torch.tensor(stretched, dtype=image.dtype, device=image.device)\n",
    "\n",
    "# image_tensor = contrast_stretch(image_tensor).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "ISIZE = (720, 1280)\n",
    "# Anchor generation\n",
    "ratios = [0.5, 1, 2]\n",
    "anchor_scales = [4, 8, 16, 32, 64]\n",
    "\n",
    "_, _, H_IMG, W_IMG = image_tensor.shape\n",
    "_, C, Y_FM, X_FM = feature_maps.shape\n",
    "\n",
    "# 5) Show boxes in [x1,y1,x2,y2]\n",
    "def create_corner_rect(bb, color='red'):\n",
    "    x1,y1,x2,y2 = bb\n",
    "    return plt.Rectangle((x1,y1), x2 - x1, y2 - y1, color=color,\n",
    "                         fill=False, lw=2)\n",
    "\n",
    "def show_corner_bbs(img, bbs):\n",
    "    # Expect [x1,y1,x2,y2]\n",
    "    img_np = (img*255.0).clamp(0,255).byte().cpu().numpy()  # (C,H,W)\n",
    "    img_np = np.transpose(img_np, (1,2,0))  # (H,W,C)\n",
    "    plt.imshow(img_np)\n",
    "    for bb in bbs:\n",
    "        plt.gca().add_patch(create_corner_rect(bb))\n",
    "    plt.show()\n",
    "\n",
    "def generate_anchor_grid_np(X_FM, Y_FM, ratios, scales):\n",
    "    \"\"\"\n",
    "    Generate a (N,4) NumPy array of anchor boxes over an X_FM x Y_FM feature map.\n",
    "\n",
    "    The final shape is (X_FM * Y_FM * len(ratios)*len(scales), 4),\n",
    "    where each row is [y1, x1, y2, x2].\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    # X_FM => width of feature map (number of columns)\n",
    "    # Y_FM => height of feature map (number of rows)\n",
    "\n",
    "    total_positions = X_FM * Y_FM\n",
    "    num_anchor_per_pos = len(ratios) * len(scales)\n",
    "    total_anchors = total_positions * num_anchor_per_pos\n",
    "\n",
    "    # We assume your input image is (H=ISIZE[0], W=ISIZE[1])\n",
    "    # You may also do something like:\n",
    "    # sub_sampling_x = float(W_IMG) / X_FM\n",
    "    # sub_sampling_y = float(H_IMG) / Y_FM\n",
    "    # But in your original code, you used the global ISIZE.\n",
    "    # Adjust as needed if your shape is dynamic.\n",
    "\n",
    "    # If you’re using a fixed ISIZE = (height=720, width=1280), do:\n",
    "    H_IMG, W_IMG = ISIZE[0], ISIZE[1]\n",
    "\n",
    "    sub_sampling_x = W_IMG / float(X_FM)\n",
    "    sub_sampling_y = H_IMG / float(Y_FM)\n",
    "\n",
    "    # Create a grid of center positions\n",
    "    shift_x = np.arange(sub_sampling_x, (X_FM + 1) * sub_sampling_x, sub_sampling_x)\n",
    "    shift_y = np.arange(sub_sampling_y, (Y_FM + 1) * sub_sampling_y, sub_sampling_y)\n",
    "\n",
    "    shift_x, shift_y = np.meshgrid(shift_x, shift_y)  # shape (Y_FM, X_FM)\n",
    "    # Now each cell center is (cy, cx) = (shift_y[r,c] - sub_sampling_y/2, shift_x[r,c] - sub_sampling_x/2)\n",
    "    centers = np.stack([\n",
    "        shift_y.ravel() - sub_sampling_y / 2.0,\n",
    "        shift_x.ravel() - sub_sampling_x / 2.0\n",
    "    ], axis=1)  # shape (total_positions, 2)\n",
    "\n",
    "    anchors = []\n",
    "    for cy, cx in centers:\n",
    "        for ratio in ratios:\n",
    "            for scale in scales:\n",
    "                h = sub_sampling_y * scale * np.sqrt(ratio)\n",
    "                w = sub_sampling_x * scale * np.sqrt(1. / ratio)\n",
    "\n",
    "                y1 = cy - h * 0.5\n",
    "                x1 = cx - w * 0.5\n",
    "                y2 = cy + h * 0.5\n",
    "                x2 = cx + w * 0.5\n",
    "\n",
    "                anchors.append([x1, y1, x2, y2])\n",
    "\n",
    "    anchors = np.array(anchors, dtype=np.float32)  # shape (total_anchors, 4)\n",
    "    return anchors\n",
    "\n",
    "\n",
    "np_anchors = generate_anchor_grid_np(X_FM, Y_FM, ratios, anchor_scales)\n",
    "anchors = torch.from_numpy(np_anchors).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "# extracting feature map with vgg backbone\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "vgg_model = torchvision.models.vgg16(pretrained=True).features[:30].to(device) # same as faster rcnn\n",
    "vgg_model.eval()\n",
    "with torch.no_grad():\n",
    "    feature_maps = vgg_model(image_tensor)  # Shape: [1, C, Hf, Wf]\n",
    "\n",
    "# Classification model\n",
    "model_classifier = ObjectClassifier(num_classes=len(name_to_id)).to(device)\n",
    "model_classifier.load_state_dict(torch.load(\"classification_3_2000_60.pth\",map_location=device))\n",
    "model_classifier.eval()\n",
    "\n",
    "# RPN+ROI model\n",
    "model_RPNROI = EnhancedRPNWithROI().to(device)\n",
    "model_RPNROI.load_state_dict(torch.load(\"final_model.pth\",map_location=device))\n",
    "model_RPNROI.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mode helper functions\n",
    "from typing_extensions import final\n",
    "from PIL import ImageFont\n",
    "from torchvision.ops import box_iou\n",
    "\n",
    "# Define transformations for the image patches\n",
    "IMAGE_SIZE = (128, 128)  # Resize patches for CNN input\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "\n",
    "# 3) Utility for decoding predicted offsets -> [x1,y1,x2,y2]\n",
    "def pred_bbox_to_xywh(bbox_offsets, anchors):\n",
    "    \"\"\"\n",
    "    bbox_offsets: (N,4) predicted offsets [dy, dx, dh, dw]\n",
    "    anchors: (N,4) in [x1,y1,x2,y2]\n",
    "    return (N,4) boxes in [x1,y1,x2,y2]\n",
    "    \"\"\"\n",
    "    # Convert to numpy\n",
    "    anchors_np = anchors.detach().cpu().numpy()\n",
    "    bbox_np    = bbox_offsets.detach().cpu().numpy()\n",
    "\n",
    "    anc_w = anchors_np[:,2] - anchors_np[:,0]  # x2 - x1\n",
    "    anc_h = anchors_np[:,3] - anchors_np[:,1]  # y2 - y1\n",
    "    anc_ctr_x = anchors_np[:,0] + 0.5*anc_w\n",
    "    anc_ctr_y = anchors_np[:,1] + 0.5*anc_h\n",
    "\n",
    "    dy = bbox_np[:,0]\n",
    "    dx = bbox_np[:,1]\n",
    "    dh = bbox_np[:,2]\n",
    "    dw = bbox_np[:,3]\n",
    "\n",
    "    # decode\n",
    "    ctr_y = dy*anc_h + anc_ctr_y\n",
    "    ctr_x = dx*anc_w + anc_ctr_x\n",
    "    h = np.exp(dh)*anc_h\n",
    "    w = np.exp(dw)*anc_w\n",
    "\n",
    "    out = np.zeros_like(bbox_np, dtype=np.float32)\n",
    "    out[:,0] = ctr_x - 0.5*w  # x1\n",
    "    out[:,1] = ctr_y - 0.5*h  # y1\n",
    "    out[:,2] = ctr_x + 0.5*w  # x2\n",
    "    out[:,3] = ctr_y + 0.5*h  # y2\n",
    "    return out\n",
    "\n",
    "# 5) Show boxes in [x1,y1,x2,y2]\n",
    "def create_corner_rect(bb, color='red'):\n",
    "    x1,y1,x2,y2 = bb\n",
    "    return plt.Rectangle((x1,y1), x2 - x1, y2 - y1, color=color,\n",
    "                         fill=False, lw=2)\n",
    "\n",
    "def show_corner_bbs(img, bbs):\n",
    "    # Expect [x1,y1,x2,y2]\n",
    "    img_np = (img*255.0).clamp(0,255).byte().cpu().numpy()  # (C,H,W)\n",
    "    img_np = np.transpose(img_np, (1,2,0))  # (H,W,C)\n",
    "    plt.imshow(img_np)\n",
    "    for bb in bbs:\n",
    "        plt.gca().add_patch(create_corner_rect(bb))\n",
    "    plt.show()\n",
    "\n",
    "def combine_box_group(boxes, scores):\n",
    "    \"\"\"\n",
    "    Combine a group of boxes into one representative box using score-weighted average\n",
    "    \"\"\"\n",
    "    weights = scores / scores.sum()\n",
    "    combined_box = torch.sum(boxes * weights.view(-1, 1), dim=0)\n",
    "    return combined_box\n",
    "\n",
    "def recursive_nms(boxes, scores, iou_threshold=0.5, recursion_limit=10):\n",
    "    \"\"\"\n",
    "    Custom NMS that recursively combines overlapping boxes by comparing all pairs.\n",
    "    \n",
    "    Args:\n",
    "        boxes: Tensor of shape [N, 4] (x1, y1, x2, y2 format)\n",
    "        scores: Tensor of shape [N] containing confidence scores\n",
    "        iou_threshold: IoU threshold for combining boxes\n",
    "        recursion_limit: Maximum number of recursive passes\n",
    "        \n",
    "    Returns:\n",
    "        combined_boxes: Tensor of combined boxes\n",
    "        keep_indices: Indices of kept boxes from original input\n",
    "    \"\"\"\n",
    "    if len(boxes) == 0:\n",
    "        return boxes, torch.empty(0, dtype=torch.long, device=boxes.device)\n",
    "    \n",
    "    # Convert to float32 if needed\n",
    "    boxes = boxes.float()\n",
    "    \n",
    "    # Initialize list to track which boxes to keep\n",
    "    keep = torch.ones(len(boxes), dtype=torch.bool, device=boxes.device)\n",
    "    \n",
    "    # Recursive combining\n",
    "    changed = True\n",
    "    recursion_count = 0\n",
    "    \n",
    "    while changed and recursion_count < recursion_limit:\n",
    "        changed = False\n",
    "        iou_matrix = box_iou(boxes, boxes)  # [N, N] matrix\n",
    "        \n",
    "        # Zero out diagonal (self-comparisons)\n",
    "        iou_matrix.fill_diagonal_(0)\n",
    "        \n",
    "        # Find all pairs that exceed IoU threshold\n",
    "        overlaps = iou_matrix > iou_threshold\n",
    "        \n",
    "        for i in range(len(boxes)):\n",
    "            if not keep[i]:\n",
    "                continue\n",
    "                \n",
    "            # Find all boxes that overlap with current box\n",
    "            overlapping_indices = torch.where(overlaps[i])[0]\n",
    "            \n",
    "            if len(overlapping_indices) > 0:\n",
    "                # Get the overlapping boxes and their scores\n",
    "                overlapping_boxes = boxes[overlapping_indices]\n",
    "                overlapping_scores = scores[overlapping_indices]\n",
    "                \n",
    "                # Combine with current box (weighted average by scores)\n",
    "                combined_box = combine_box_group(\n",
    "                    torch.cat([boxes[i].unsqueeze(0), overlapping_boxes]),\n",
    "                    torch.cat([scores[i].unsqueeze(0), overlapping_scores])\n",
    "                )\n",
    "                \n",
    "                # Replace current box with combined version\n",
    "                boxes[i] = combined_box\n",
    "                \n",
    "                # Mark overlapping boxes for removal\n",
    "                keep[overlapping_indices] = False\n",
    "                changed = True\n",
    "        \n",
    "        # Filter boxes after each pass\n",
    "        boxes = boxes[keep]\n",
    "        scores = scores[keep]\n",
    "        keep = torch.ones(len(boxes), dtype=torch.bool, device=boxes.device)\n",
    "        recursion_count += 1\n",
    "    \n",
    "    return boxes, torch.where(keep)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final integration function\n",
    "def detect_objects(feature_maps, anchors, image, image_tensor, RPNROI, classifier):\n",
    "    with torch.no_grad():\n",
    "      pred_locs, pred_scores, objectness_score, pooled_feats = RPNROI(feature_maps, anchors, do_roi=True)\n",
    "\n",
    "    top_k = 40\n",
    "\n",
    "    # Decode from anchors + predicted offsets => actual boxes\n",
    "    rois = pred_bbox_to_xywh(pred_locs[0], anchors)\n",
    "\n",
    "    k = min(top_k, objectness_score[0].shape[0])\n",
    "    topk_inds = torch.topk(objectness_score[0], k=k).indices\n",
    "    proposals = rois[topk_inds.cpu().numpy()]\n",
    "    scores = objectness_score[0][topk_inds].cpu().numpy()\n",
    "\n",
    "    # Apply recursive NMS to reduce overlapping proposals\n",
    "    proposals_tensor = torch.from_numpy(proposals).float().to(device)\n",
    "    scores_tensor = torch.from_numpy(scores).float().to(device)\n",
    "    proposals, _ = recursive_nms(proposals_tensor, scores_tensor, iou_threshold=0.5, recursion_limit=top_k)\n",
    "    proposals = proposals.cpu().numpy()  # convert back to numpy for visualization\n",
    "    \n",
    "    image_width, image_height = image.size\n",
    "    crops = []\n",
    "    box_list = []\n",
    "\n",
    "    for box in proposals:\n",
    "        x1, y1, x2, y2 = [int(coord) for coord in box]\n",
    "        y1, x1 = max(0, y1), max(0, x1)\n",
    "        y2, x2 = min(image_height, y2), min(image_width, x2)\n",
    "\n",
    "        if x1 >= x2 or y1 >= y2:\n",
    "            continue\n",
    "        else:\n",
    "            patch = image.crop((x1, y1, x2, y2))\n",
    "            patch = transform(patch)\n",
    "            crops.append(patch)\n",
    "            box_list.append((x1, y1, x2, y2))  # Store box for later drawing\n",
    "\n",
    "    # If no objects are detected\n",
    "    if not crops:\n",
    "        print(\"No objects detected after ROI filtering.\")\n",
    "        return\n",
    "\n",
    "    crops = torch.stack(crops).to(device)\n",
    "\n",
    "    # Classifier\n",
    "    classifier.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = classifier(crops)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    # Convert predictions to class names\n",
    "    predicted_labels = [list(name_to_id.keys())[p] for p in predicted.cpu().numpy()]\n",
    "\n",
    "    # Draw Bounding Boxes and Labels on Image\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", 16)  # Load font (if available)\n",
    "    except:\n",
    "        font = ImageFont.load_default()  # Use default font if Arial not available\n",
    "\n",
    "    for (x1, y1, x2, y2), label in zip(box_list, predicted_labels):\n",
    "        draw.rectangle([x1, y1, x2, y2], outline=\"red\", width=2)  # Draw bounding box\n",
    "        font_path = \"/usr/share/fonts/truetype/liberation/LiberationSans-Bold.ttf\"\n",
    "        font1 = ImageFont.truetype(font_path, 32)\n",
    "        draw.text((x1, y1 - 10), label, fill=\"red\", font=font1)  # Draw label\n",
    "\n",
    "    image_draw = image.copy()\n",
    "    draw = ImageDraw.Draw(image_draw)\n",
    "\n",
    "    for box in box_list:\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        draw.rectangle([x1, y1, x2, y2], outline=\"red\", width=3)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(image_draw)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Prediction Result\")\n",
    "    plt.show()\n",
    "\n",
    "detect_objects(feature_maps, anchors, image, image_tensor, model_RPNROI, model_classifier)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
