{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wACR2UWY-HiX",
        "outputId": "6fbfad08-d974-48b8-b8ab-d0e27a7917de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ijson\n",
            "  Downloading ijson-3.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Downloading ijson-3.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (119 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/119.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.2/119.2 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ijson\n",
            "Successfully installed ijson-3.3.0\n"
          ]
        }
      ],
      "source": [
        "! pip install ijson"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: connect to google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "pit5cjY8N4KM",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd120ee4-d352-4bbc-e8e8-b2bed51a39e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "fFV-BIq3_PsT",
        "outputId": "75acf094-99be-4e84-8997-9ccfe89a826c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open /content/drive/MyDrive/APS/trainA_original_2000-20250403T012447Z-001.zip, /content/drive/MyDrive/APS/trainA_original_2000-20250403T012447Z-001.zip.zip or /content/drive/MyDrive/APS/trainA_original_2000-20250403T012447Z-001.zip.ZIP.\n"
          ]
        }
      ],
      "source": [
        "!unzip '/content/drive/MyDrive/APS/trainA_original_2000-20250403T012447Z-001.zip' # initial image tensors without resize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAhG-UXl8NOT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import copy\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# -----------------------\n",
        "# Global settings & helpers\n",
        "# -----------------------\n",
        "\n",
        "# Input image size (height, width)\n",
        "ISIZE = (720, 1280)\n",
        "\n",
        "# ImageNet statistics (for VGG16)\n",
        "# imagenet_mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "# imagenet_std  = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
        "\n",
        "def normalize_tensor(img):\n",
        "    \"\"\"Normalize a tensor image (C, H, W) with values in [0,255].\"\"\"\n",
        "    img = img / 255.0\n",
        "    return img\n",
        "\n",
        "def unnormalize_tensor(img):\n",
        "    \"\"\"Convert a normalized tensor back to a displayable numpy image.\"\"\"\n",
        "    img = img * 255.0\n",
        "    return img.clamp(0, 255).byte().cpu().numpy()\n",
        "\n",
        "# Global anchor parameters\n",
        "ratios = [0.5, 1, 2]\n",
        "anchor_scales = [8, 16, 32]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZE0o51FA8ubf"
      },
      "outputs": [],
      "source": [
        "import ijson\n",
        "\n",
        "\n",
        "def extract_first_n_labels(json_file_path, n):\n",
        "    labels = []\n",
        "    with open(json_file_path, 'rb') as f:\n",
        "        parser = ijson.items(f, 'item')\n",
        "        for i, item in enumerate(parser):\n",
        "            if i >= n:\n",
        "                break\n",
        "            filtered_labels = [\n",
        "                {\"category\": li.get(\"category\"), \"box2d\": li.get(\"box2d\")}\n",
        "                for li in item.get(\"labels\", []) if \"box2d\" in li\n",
        "            ]\n",
        "            labels.append({\n",
        "                \"name\": item.get(\"name\"),\n",
        "                \"timestamp\": item.get(\"timestamp\"),\n",
        "                \"labels\": filtered_labels\n",
        "            })\n",
        "    return labels\n",
        "\n",
        "def standardize_filename(path_or_name):\n",
        "    base = os.path.basename(path_or_name)\n",
        "    base, _ = os.path.splitext(base)\n",
        "    return base\n",
        "\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, image_dir, labels, pt_dir='pt_files'):\n",
        "        self.image_dir = image_dir\n",
        "        self.pt_dir = pt_dir\n",
        "        os.makedirs(self.pt_dir, exist_ok=True)\n",
        "        self.image_files = sorted([\n",
        "            os.path.join(image_dir, f)\n",
        "            for f in os.listdir(image_dir)\n",
        "            if f.lower().endswith(('.jpg', '.png', '.jpeg'))\n",
        "        ])\n",
        "        self.label_dict = {}\n",
        "        for item in labels:\n",
        "            key = standardize_filename(item[\"name\"])\n",
        "            self.label_dict[key] = item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_files[idx]\n",
        "        pt_path = os.path.join(\n",
        "            self.pt_dir,\n",
        "            os.path.basename(image_path)\n",
        "            .replace('.jpg', '.pt')\n",
        "            .replace('.png', '.pt')\n",
        "            .replace('.jpeg', '.pt')\n",
        "        )\n",
        "        if os.path.exists(pt_path):\n",
        "            image_tensor = torch.load(pt_path)\n",
        "        else:\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "            if image.size != (ISIZE[1], ISIZE[0]):  # PIL: (width, height)\n",
        "                image = image.resize((ISIZE[1], ISIZE[0]))\n",
        "            image_tensor = transforms.PILToTensor()(image).float()\n",
        "            torch.save(image_tensor, pt_path)\n",
        "        image_tensor = normalize_tensor(image_tensor)\n",
        "\n",
        "        base_key = standardize_filename(image_path)\n",
        "        matched = self.label_dict.get(base_key, None)\n",
        "        if matched is None or \"labels\" not in matched:\n",
        "            target = {\"boxes\": torch.zeros((0, 4), dtype=torch.float32),\n",
        "                      \"labels\": torch.zeros((0,), dtype=torch.int64),\n",
        "                      \"names\": [],\n",
        "                      \"index\": idx}\n",
        "        else:\n",
        "            boxes = []\n",
        "            cats = []\n",
        "            for obj in matched[\"labels\"]:\n",
        "                if \"box2d\" in obj:\n",
        "                    b2d = obj[\"box2d\"]\n",
        "                    boxes.append([float(b2d[\"y1\"]), float(b2d[\"x1\"]),\n",
        "                                  float(b2d[\"y2\"]), float(b2d[\"x2\"])])\n",
        "                    cats.append(obj[\"category\"])\n",
        "            boxes_tensor = torch.tensor(boxes, dtype=torch.float32) if boxes else torch.zeros((0,4), dtype=torch.float32)\n",
        "            labels_tensor = torch.tensor([1] * len(cats), dtype=torch.int64)\n",
        "            target = {\"boxes\": boxes_tensor, \"labels\": labels_tensor, \"names\": cats, \"index\": idx}\n",
        "        return {\"image\": image_tensor, \"boxes\": target[\"boxes\"], \"labels\": target[\"labels\"],\n",
        "                \"names\": target[\"names\"], \"index\": target[\"index\"]}\n",
        "\n",
        "# Custom collate function (your version)\n",
        "def custom_collate_fn(batch):\n",
        "    images = [item[\"image\"] for item in batch]\n",
        "    boxes = [item[\"boxes\"] for item in batch]\n",
        "    labels = [item[\"labels\"] for item in batch]\n",
        "    names = [item[\"names\"] for item in batch]\n",
        "    indices = [item[\"index\"] for item in batch]\n",
        "    return {\"images\": torch.stack(images, 0),\n",
        "            \"boxes\": boxes,\n",
        "            \"labels\": labels,\n",
        "            \"names\": names,\n",
        "            \"indices\": indices}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwx4b0s88zoK"
      },
      "outputs": [],
      "source": [
        "def create_corner_rect(bb, color='red'):\n",
        "    bb = np.array(bb, dtype=np.float32)\n",
        "    return plt.Rectangle((bb[0], bb[1]), bb[2]-bb[0], bb[3]-bb[1], color=color,\n",
        "                         fill=False, lw=3)\n",
        "\n",
        "def show_corner_bbs(im, bbs):\n",
        "    # im expected to be (C, H, W) tensor; convert to numpy image for plotting\n",
        "    im_np = unnormalize_tensor(im)\n",
        "    plt.imshow(np.transpose(im_np, (1, 2, 0)))\n",
        "    for bb in bbs:\n",
        "        plt.gca().add_patch(create_corner_rect(bb))\n",
        "    plt.show()\n",
        "\n",
        "def create_ground_truth_rect(bb, color='blue'):\n",
        "    # Now handles [y1,x1,y2,x2] format\n",
        "    bb = np.array(bb, dtype=np.float32)\n",
        "    return plt.Rectangle((bb[1], bb[0]),  # (x1,y1)\n",
        "                        bb[3]-bb[1],     # width (x2-x1)\n",
        "                        bb[2]-bb[0],     # height (y2-y1)\n",
        "                        color=color, fill=False, lw=3)\n",
        "\n",
        "def show_ground_truth_bbs(im, bbs):\n",
        "    im_np = unnormalize_tensor(im)\n",
        "    plt.imshow(np.transpose(im_np, (1, 2, 0)))\n",
        "\n",
        "    for bb in bbs:\n",
        "        plt.gca().add_patch(create_ground_truth_rect(bb))\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# -----------------------\n",
        "# Vectorized IoU Computation\n",
        "# -----------------------\n",
        "\n",
        "def compute_iou_matrix(pred_boxes, gt_boxes):\n",
        "    \"\"\"\n",
        "    Compute IoU between predicted boxes (N,4) and ground truth boxes (M,4).\n",
        "    Handles edge cases where arrays might be empty.\n",
        "\n",
        "    Args:\n",
        "        pred_boxes: np.array of shape (N,4) [y1,x1,y2,x2]\n",
        "        gt_boxes: np.array of shape (M,4) [y1,x1,y2,x2]\n",
        "\n",
        "    Returns:\n",
        "        IoU matrix of shape (N,M)\n",
        "    \"\"\"\n",
        "    # Handle empty inputs\n",
        "    if len(pred_boxes) == 0 or len(gt_boxes) == 0:\n",
        "        return np.zeros((len(pred_boxes), len(gt_boxes)))\n",
        "\n",
        "    # Ensure arrays are numpy and float32\n",
        "    pred_boxes = np.asarray(pred_boxes, dtype=np.float32)\n",
        "    gt_boxes = np.asarray(gt_boxes, dtype=np.float32)\n",
        "\n",
        "    # Compute intersection areas\n",
        "    inter_ymin = np.maximum(pred_boxes[:, None, 0], gt_boxes[:, 0])\n",
        "    inter_xmin = np.maximum(pred_boxes[:, None, 1], gt_boxes[:, 1])\n",
        "    inter_ymax = np.minimum(pred_boxes[:, None, 2], gt_boxes[:, 2])\n",
        "    inter_xmax = np.minimum(pred_boxes[:, None, 3], gt_boxes[:, 3])\n",
        "\n",
        "    inter_h = np.maximum(inter_ymax - inter_ymin, 0.)\n",
        "    inter_w = np.maximum(inter_xmax - inter_xmin, 0.)\n",
        "    inter_area = inter_h * inter_w\n",
        "\n",
        "    # Compute union areas\n",
        "    pred_area = (pred_boxes[:, 2] - pred_boxes[:, 0]) * (pred_boxes[:, 3] - pred_boxes[:, 1])\n",
        "    gt_area = (gt_boxes[:, 2] - gt_boxes[:, 0]) * (gt_boxes[:, 3] - gt_boxes[:, 1])\n",
        "\n",
        "    union_area = pred_area[:, None] + gt_area - inter_area\n",
        "\n",
        "    # Compute IoU\n",
        "    iou_matrix = inter_area / (union_area + 1e-8)  # Add epsilon to avoid division by zero\n",
        "\n",
        "    return iou_matrix\n",
        "\n",
        "def compute_iou_loss(anchors, gt_boxes):\n",
        "    \"\"\"\n",
        "    Compute IoU loss for matched bounding boxes with list inputs.\n",
        "\n",
        "    Parameters:\n",
        "        pred_boxes_list: List of lists [[y1,x1,y2,x2], ...] (N boxes)\n",
        "        gt_boxes_list: List of lists [[y1,x1,y2,x2], ...] (M boxes)\n",
        "\n",
        "    Returns:\n",
        "        IoU Loss (float): 1 - mean IoU of matched boxes\n",
        "    \"\"\"\n",
        "    anchors = anchors.astype(np.float32)\n",
        "    gt_boxes = gt_boxes.astype(np.float32)\n",
        "    # Compute IoU matrix\n",
        "    iou_matrix = compute_iou_matrix(anchors, gt_boxes)\n",
        "\n",
        "    # Find best matches (each ground truth gets its best matching prediction)\n",
        "    best_iou_for_each_gt = np.max(iou_matrix, axis=0)\n",
        "\n",
        "    # Calculate mean IoU of the best matches\n",
        "    mean_iou = np.mean(best_iou_for_each_gt) if len(best_iou_for_each_gt) > 0 else 0\n",
        "\n",
        "    # Compute IoU loss\n",
        "    iou_loss = 1 - mean_iou\n",
        "\n",
        "    return iou_loss\n",
        "\n",
        "def compute_iou_vectorized(anchors, gt_boxes):\n",
        "    \"\"\"\n",
        "    Compute IoU between anchors (N,4) and gt_boxes (M,4).\n",
        "    Boxes in [y1,x1,y2,x2] format.\n",
        "    Returns IoU matrix of shape (N, M).\n",
        "    \"\"\"\n",
        "    anchors = anchors.astype(np.float32)\n",
        "    gt_boxes = gt_boxes.astype(np.float32)\n",
        "    inter_y1 = np.maximum(anchors[:, None, 0], gt_boxes[None, :, 0])\n",
        "    inter_x1 = np.maximum(anchors[:, None, 1], gt_boxes[None, :, 1])\n",
        "    inter_y2 = np.minimum(anchors[:, None, 2], gt_boxes[None, :, 2])\n",
        "    inter_x2 = np.minimum(anchors[:, None, 3], gt_boxes[None, :, 3])\n",
        "    inter_h = np.maximum(inter_y2 - inter_y1, 0)\n",
        "    inter_w = np.maximum(inter_x2 - inter_x1, 0)\n",
        "    inter_area = inter_h * inter_w\n",
        "    anchor_area = (anchors[:,2]-anchors[:,0])*(anchors[:,3]-anchors[:,1])\n",
        "    gt_area = (gt_boxes[:,2]-gt_boxes[:,0])*(gt_boxes[:,3]-gt_boxes[:,1])\n",
        "    union = anchor_area[:,None] + gt_area[None,:] - inter_area\n",
        "    iou = inter_area / union\n",
        "    return iou\n",
        "\n",
        "# -----------------------\n",
        "# Revised bbox_generation Function (Vectorized and Padded)\n",
        "# -----------------------\n",
        "\n",
        "def bbox_generation(images, targets, X_FM, Y_FM):\n",
        "    \"\"\"\n",
        "    Compute regression targets and classification labels for all anchors.\n",
        "    All anchors (generated over the full feature map) receive a target;\n",
        "    anchors outside the image are ignored (label = -1).\n",
        "    Returns:\n",
        "       anchor_locations_all_merge: (B, total_anchors, 4)\n",
        "       anchor_labels_all_merge: (B, total_anchors)\n",
        "       anchors: (total_anchors, 4)\n",
        "    \"\"\"\n",
        "    num_batch = len(images)\n",
        "    C, H_IMG, W_IMG = images[0].shape\n",
        "\n",
        "    # Generate full grid anchors over feature map\n",
        "    total_positions = X_FM * Y_FM\n",
        "    num_anchor_per_pos = len(ratios) * len(anchor_scales)\n",
        "    total_anchors = total_positions * num_anchor_per_pos\n",
        "\n",
        "    # Generate grid centers (using strides)\n",
        "    sub_sampling_x = float(W_IMG) / X_FM\n",
        "    sub_sampling_y = float(H_IMG) / Y_FM\n",
        "\n",
        "    shift_x = np.arange(sub_sampling_x, (X_FM+1)*sub_sampling_x, sub_sampling_x)\n",
        "    shift_y = np.arange(sub_sampling_y, (Y_FM+1)*sub_sampling_y, sub_sampling_y)\n",
        "    shift_x, shift_y = np.meshgrid(shift_x, shift_y)  # shape (Y_FM, X_FM)\n",
        "    centers = np.stack([shift_y.ravel() - sub_sampling_y/2, shift_x.ravel() - sub_sampling_x/2], axis=1)  # (total_positions, 2)\n",
        "\n",
        "    # For each center, generate anchors\n",
        "    anchors = []\n",
        "    for center in centers:\n",
        "        cy, cx = center\n",
        "        for ratio in ratios:\n",
        "            for scale in anchor_scales:\n",
        "                h = sub_sampling_y * scale * np.sqrt(ratio)\n",
        "                w = sub_sampling_x * scale * np.sqrt(1.0/ratio)\n",
        "                y1 = cy - h/2.\n",
        "                x1 = cx - w/2.\n",
        "                y2 = cy + h/2.\n",
        "                x2 = cx + w/2.\n",
        "                anchors.append([y1, x1, y2, x2])\n",
        "    anchors = np.array(anchors, dtype=np.float32)  # shape (total_anchors, 4)\n",
        "\n",
        "    # Create ground-truth arrays for all anchors (padded to total_anchors)\n",
        "    # Initialize labels to -1 (ignore) and loc targets to zeros.\n",
        "    anchor_labels_all = []\n",
        "    anchor_locs_all = []\n",
        "    pos_iou_threshold = 0.7\n",
        "    neg_iou_threshold = 0.3\n",
        "    n_sample = 256\n",
        "    pos_ratio = 0.5\n",
        "\n",
        "    # Compute valid indices: those anchors that are completely inside the image.\n",
        "    valid_idx = np.where((anchors[:,0] >= 0) & (anchors[:,1] >= 0) &\n",
        "                         (anchors[:,2] <= H_IMG) & (anchors[:,3] <= W_IMG))[0]\n",
        "\n",
        "    for i in range(num_batch):\n",
        "        # Create full target arrays for this image.\n",
        "        labels = -1 * np.ones((anchors.shape[0],), dtype=np.int32)\n",
        "        locs = np.zeros((anchors.shape[0], 4), dtype=np.float32)\n",
        "        gt_boxes_tensor = targets[i][\"boxes\"]\n",
        "        if gt_boxes_tensor.numel() > 0:\n",
        "            gt_boxes = gt_boxes_tensor.cpu().numpy()  # shape (M,4)\n",
        "            # Compute IoU for valid anchors only.\n",
        "            valid_anchors = anchors[valid_idx]\n",
        "            ious = compute_iou_vectorized(valid_anchors, gt_boxes)  # (N_valid, M)\n",
        "            max_ious = np.max(ious, axis=1)if ious.size else np.array([])\n",
        "            argmax_ious = np.argmax(ious, axis=1)\n",
        "            # Set targets for valid anchors.\n",
        "            valid_labels = -1 * np.ones((valid_anchors.shape[0],), dtype=np.int32)\n",
        "            valid_labels[max_ious >= pos_iou_threshold] = 1\n",
        "            valid_labels[max_ious < neg_iou_threshold] = 0\n",
        "            # Ensure every GT box gets at least one positive anchor.\n",
        "            gt_max_ious = np.max(ious, axis=0)  # (M,)\n",
        "            for j in range(gt_boxes.shape[0]):\n",
        "                inds = np.where(ious[:, j] == gt_max_ious[j])[0]\n",
        "                valid_labels[inds] = 1\n",
        "\n",
        "            # Subsample positives and negatives in valid region.\n",
        "            pos_inds = np.where(valid_labels == 1)[0]\n",
        "            neg_inds = np.where(valid_labels == 0)[0]\n",
        "            if len(pos_inds) > int(pos_ratio * n_sample):\n",
        "                disable = np.random.choice(pos_inds, size=(len(pos_inds) - int(pos_ratio * n_sample)), replace=False)\n",
        "                valid_labels[disable] = -1\n",
        "            remaining = n_sample - np.sum(valid_labels == 1)\n",
        "            if len(neg_inds) > remaining:\n",
        "                disable = np.random.choice(neg_inds, size=(len(neg_inds) - remaining), replace=False)\n",
        "                valid_labels[disable] = -1\n",
        "\n",
        "            # Compute regression targets for positive valid anchors.\n",
        "            valid_locs = np.zeros((valid_anchors.shape[0], 4), dtype=np.float32)\n",
        "            pos_valid_inds = np.where(valid_labels == 1)[0]\n",
        "            if len(pos_valid_inds) > 0:\n",
        "                pos_anchors = valid_anchors[pos_valid_inds]\n",
        "                anchor_heights = pos_anchors[:,2] - pos_anchors[:,0]\n",
        "                anchor_widths = pos_anchors[:,3] - pos_anchors[:,1]\n",
        "                anchor_ctr_y = pos_anchors[:,0] + 0.5 * anchor_heights\n",
        "                anchor_ctr_x = pos_anchors[:,1] + 0.5 * anchor_widths\n",
        "\n",
        "                target_gt = gt_boxes[argmax_ious[pos_valid_inds]]\n",
        "                gt_heights = target_gt[:,2] - target_gt[:,0]\n",
        "                gt_widths = target_gt[:,3] - target_gt[:,1]\n",
        "                gt_ctr_y = target_gt[:,0] + 0.5 * gt_heights\n",
        "                gt_ctr_x = target_gt[:,1] + 0.5 * gt_widths\n",
        "\n",
        "                dy = (gt_ctr_y - anchor_ctr_y) / anchor_heights\n",
        "                dx = (gt_ctr_x - anchor_ctr_x) / anchor_widths\n",
        "                dh = np.log(gt_heights / anchor_heights)\n",
        "                dw = np.log(gt_widths / anchor_widths)\n",
        "                valid_locs[pos_valid_inds, :] = np.stack([dy, dx, dh, dw], axis=1)\n",
        "            # Assign computed valid targets into the full arrays.\n",
        "            labels[valid_idx] = valid_labels\n",
        "            locs[valid_idx, :] = valid_locs\n",
        "        # Append for current image.\n",
        "        anchor_labels_all.append(labels)\n",
        "        anchor_locs_all.append(locs)\n",
        "    anchor_labels_all_merge = np.stack(anchor_labels_all, axis=0)  # (B, total_anchors)\n",
        "    anchor_locs_all_merge = np.stack(anchor_locs_all, axis=0)        # (B, total_anchors, 4)\n",
        "    return anchor_locs_all_merge, anchor_labels_all_merge, anchors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aW_6fzN3ndZ"
      },
      "outputs": [],
      "source": [
        "### Alternative way for IOU calculation\n",
        "\n",
        "# define the iou calculation function\n",
        "def compute_iou(box1, box2):\n",
        "    \"\"\"Compute IoU between two bounding boxes.\"\"\"\n",
        "    x1 = max(box1[0], box2[0])\n",
        "    y1 = max(box1[1], box2[1])\n",
        "    x2 = min(box1[2], box2[2])\n",
        "    y2 = min(box1[3], box2[3])\n",
        "\n",
        "    inter_area = max(0, x2 - x1) * max(0, y2 - y1)\n",
        "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
        "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
        "    union_area = box1_area + box2_area - inter_area\n",
        "\n",
        "    return inter_area / union_area if union_area > 0 else 0\n",
        "\n",
        "# define the function that finds the best match from predicted bounding boxes for each ground truth box\n",
        "def match_predictions_to_ground_truth(pred_boxes, gt_boxes, iou_threshold=0.5):\n",
        "    \"\"\"Match predicted boxes to ground truth boxes based on IoU.\"\"\"\n",
        "    matched_preds = {}\n",
        "    gt_matched = set()\n",
        "    pred_boxes = pred_boxes.astype(np.float32)\n",
        "    gt_boxes = gt_boxes.astype(np.float32)\n",
        "\n",
        "    for i, pred in enumerate(pred_boxes):\n",
        "        best_iou = 0\n",
        "        best_gt_idx = -1\n",
        "        for j, gt in enumerate(gt_boxes):\n",
        "            if j in gt_matched:\n",
        "                continue  # Skip matched gt boxes\n",
        "            iou = compute_iou(pred, gt)\n",
        "            if iou > best_iou:\n",
        "                best_iou = iou\n",
        "                best_gt_idx = j\n",
        "\n",
        "        if best_iou >= iou_threshold:\n",
        "            matched_preds[i] = best_gt_idx\n",
        "            gt_matched.add(best_gt_idx)\n",
        "\n",
        "    return matched_preds # return a dictionary of predicted bounding boxes that each matches to one ground truth box\n",
        "               # with format: {i}: box, where i is the order of corresponding ground truth box\n",
        "\n",
        "# can directly use this function in the training code to get the iou loss for parameter updates\n",
        "# this function calls match_prediction_to_ground_truth(pred_boxes, gt_boxes, iou_threshold=0.5) and compute_iou(box1, box2)\n",
        "# ****** if want to change IOU threshold, change it in the input to match_prediction_to_ground_truth(pred_boxes, gt_boxes, iou_threshold=0.5)\n",
        "\n",
        "'''def compute_iou_loss_vectorized(pred_boxes, gt_boxes):\n",
        "    \"\"\"\n",
        "    Compute IoU loss for matched bounding boxes with vectorized inputs.\n",
        "\n",
        "    Parameters:\n",
        "        pred_boxes (N,4): Predicted bounding boxes [y1, x1, y2, x2]\n",
        "        gt_boxes (M,4): Ground truth bounding boxes [y1, x1, y2, x2]\n",
        "\n",
        "    Returns:\n",
        "        IoU Loss (float): 1 - mean IoU of matched boxes\n",
        "    \"\"\"\n",
        "    # Compute IoU matrix between all predictions and ground truth boxes\n",
        "    iou_matrix = compute_iou_vectorized(pred_boxes, gt_boxes)\n",
        "\n",
        "    # Find best matches - each gt box gets its best matching prediction\n",
        "    # First, find for each gt box, the prediction with highest IoU\n",
        "    best_pred_for_gt = np.argmax(iou_matrix, axis=0)\n",
        "    best_iou_for_gt = np.max(iou_matrix, axis=0)\n",
        "\n",
        "    # Get unique predictions that were matched to any gt box\n",
        "    matched_pred_indices = np.unique(best_pred_for_gt)\n",
        "\n",
        "    if len(matched_pred_indices) == 0:  # If no matches, return max loss (1)\n",
        "        return 1.0\n",
        "\n",
        "    # For each matched prediction, get its best IoU with any gt box\n",
        "    matched_iou_values = []\n",
        "    for pred_idx in matched_pred_indices:\n",
        "        # Get the best IoU this prediction has with any gt box\n",
        "        best_iou = np.max(iou_matrix[pred_idx])\n",
        "        matched_iou_values.append(best_iou)\n",
        "\n",
        "    mean_iou = np.mean(matched_iou_values) if matched_iou_values else 0\n",
        "    iou_loss = 1 - mean_iou\n",
        "\n",
        "    return iou_loss\n",
        "'''\n",
        "  # compute true positive, false positive, and false negative if we need for precision and recall calculation\n",
        "def compute_tp_fp_fn(matched_preds, num_pred, num_gt): # num_pred is total number of predictions.\n",
        "                                # num_gt is total number of ground boxes.\n",
        "    \"\"\"Calculate TP, FP, FN from matched predictions.\"\"\"\n",
        "    TP = len(matched_preds)\n",
        "    FP = num_pred - TP\n",
        "    FN = num_gt - TP\n",
        "    return TP, FP, FN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qPO1WJ79q0L",
        "outputId": "26b7508e-3350-45d6-bf46-5086af99826b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:04<00:00, 121MB/s]\n"
          ]
        }
      ],
      "source": [
        "vgg_model = torchvision.models.vgg16(pretrained=True).to(device)\n",
        "vgg_model.eval()\n",
        "for param in vgg_model.features.parameters():\n",
        "    param.requires_grad = False\n",
        "req_features = [layer for layer in list(vgg_model.features)[:30]]\n",
        "\n",
        "class RPN(nn.Module):\n",
        "    def __init__(self, in_channels=512, mid_channels=512, n_anchor=9):\n",
        "        super(RPN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.reg_layer = nn.Conv2d(mid_channels, n_anchor*4, kernel_size=1, stride=1)\n",
        "        self.cls_layer = nn.Conv2d(mid_channels, n_anchor*2, kernel_size=1, stride=1)\n",
        "        nn.init.normal_(self.conv1.weight, std=0.01)\n",
        "        nn.init.constant_(self.conv1.bias, 0)\n",
        "        nn.init.normal_(self.reg_layer.weight, std=0.01)\n",
        "        nn.init.constant_(self.reg_layer.bias, 0)\n",
        "        nn.init.normal_(self.cls_layer.weight, std=0.01)\n",
        "        nn.init.constant_(self.cls_layer.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.shape[0]\n",
        "        x = self.conv1(x)\n",
        "        pred_anchor_locs = self.reg_layer(x)\n",
        "        pred_cls_scores = self.cls_layer(x)\n",
        "        pred_anchor_locs = pred_anchor_locs.permute(0,2,3,1).contiguous().view(batch_size, -1, 4)\n",
        "        pred_cls_scores = pred_cls_scores.permute(0,2,3,1).contiguous().view(batch_size, -1, 2)\n",
        "        objectness_score = pred_cls_scores[:,:,1]\n",
        "        return pred_anchor_locs, pred_cls_scores, objectness_score\n",
        "\n",
        "rpn_model = RPN().to(device)\n",
        "optimizer = torch.optim.Adam(rpn_model.parameters(), lr=0.0015)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hB2vjq3S-RyG"
      },
      "outputs": [],
      "source": [
        "def pred_bbox_to_xywh(bbox, anchors):\n",
        "    anchors = anchors.astype(np.float32)\n",
        "    anc_height = anchors[:,2] - anchors[:,0]\n",
        "    anc_width  = anchors[:,3] - anchors[:,1]\n",
        "    anc_ctr_y = anchors[:,0] + 0.5 * anc_height\n",
        "    anc_ctr_x = anchors[:,1] + 0.5 * anc_width\n",
        "    bbox_np = bbox.detach().cpu().numpy()\n",
        "    dy = bbox_np[:,0]\n",
        "    dx = bbox_np[:,1]\n",
        "    dh = bbox_np[:,2]\n",
        "    dw = bbox_np[:,3]\n",
        "    ctr_y = dy * anc_height + anc_ctr_y\n",
        "    ctr_x = dx * anc_width + anc_ctr_x\n",
        "    h = np.exp(dh) * anc_height\n",
        "    w = np.exp(dw) * anc_width\n",
        "    roi = np.zeros(bbox_np.shape, dtype=np.float32)\n",
        "    roi[:,0] = ctr_x - 0.5 * w\n",
        "    roi[:,1] = ctr_y - 0.5 * h\n",
        "    roi[:,2] = ctr_x + 0.5 * w\n",
        "    roi[:,3] = ctr_y + 0.5 * h\n",
        "    return roi\n",
        "\n",
        "def train_epochs(req_features, rpn_model, optimizer, train_dl, epochs=20, rpn_lambda=10, device = None):\n",
        "    if device is None:  # If device is not specified, use the default device\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    rpn_model.load_state_dict(torch.load(\"./rpn_epoch_200.pth\", map_location=device))\n",
        "    rpn_model.train()\n",
        "    for epoch in range(epochs):\n",
        "        print(\"Hi\")\n",
        "    rpn_model.train()\n",
        "    for epoch in range(epochs):\n",
        "        print(\"Hi\")\n",
        "        total_samples = 0\n",
        "        sum_loss = 0.0\n",
        "        sum_loss_cls = 0.0\n",
        "        sum_loss_loc = 0.0\n",
        "        for batch in train_dl:\n",
        "            images = batch[\"images\"].to(device) # shape (B,C,H,W)\n",
        "            targets = [{\"boxes\": b, \"labels\": l} for b, l in zip(batch[\"boxes\"], batch[\"labels\"])]\n",
        "            B = images.shape[0]\n",
        "            #print(f\"image size: {B}\")\n",
        "            total_samples += B\n",
        "            # Forward through frozen backbone\n",
        "            imgs = images.clone()\n",
        "            with torch.no_grad():\n",
        "                feat = imgs\n",
        "                for m in req_features:\n",
        "                    feat = m(feat)\n",
        "            X_FM, Y_FM = feat.shape[2], feat.shape[3]\n",
        "            # Compute GT targets (for all anchors)\n",
        "            gt_locs_np, gt_scores_np, anchors = bbox_generation([img for img in images], targets, X_FM, Y_FM)\n",
        "            print(\"Hmm\")\n",
        "            gt_locs = torch.from_numpy(gt_locs_np.astype(np.float32)).to(device)\n",
        "            gt_scores = torch.from_numpy(gt_scores_np.astype(np.float32)).to(device)\n",
        "            # Forward RPN\n",
        "            pred_locs, pred_scores, objectness_score = rpn_model(feat)\n",
        "            # Compute classification loss: flatten predictions and GT.\n",
        "            cls_loss = F.cross_entropy(pred_scores.view(-1,2),\n",
        "                                       gt_scores.view(-1).long(),\n",
        "                                       ignore_index=-1)\n",
        "            # Compute regression loss for positive anchors only.\n",
        "            pos_mask = gt_scores > 0\n",
        "            if pos_mask.sum() > 0:\n",
        "                pred_pos = pred_locs[pos_mask]\n",
        "                gt_pos = gt_locs[pos_mask]\n",
        "                diff = torch.abs(gt_pos - pred_pos)\n",
        "                loc_loss = torch.where(diff < 1, 0.5 * diff**2, diff - 0.5)\n",
        "                loc_loss = loc_loss.sum() / pos_mask.sum().float()\n",
        "            else:\n",
        "                loc_loss = torch.tensor(0.0, device=device)\n",
        "            loss = cls_loss + rpn_lambda * loc_loss\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            sum_loss += loss.item()\n",
        "            sum_loss_cls += cls_loss.item()\n",
        "            sum_loss_loc += (rpn_lambda * loc_loss).item()\n",
        "        print(f\"Epoch {epoch+1}/{epochs}: Loss {sum_loss/total_samples:.3f} | Cls {sum_loss_cls/total_samples:.3f} | Loc {sum_loss_loc/total_samples:.3f}\")\n",
        "        if (epoch+1)%5==0:\n",
        "            torch.save(rpn_model.state_dict(), f\"./rpn_epoch_{epoch+1}.pth\")\n",
        "    return rpn_model\n",
        "\n",
        "def validate(rpn_model, data_loader, n_images=7, top_k=20):\n",
        "    rpn_model.eval()\n",
        "    with torch.no_grad():\n",
        "        batch = next(iter(data_loader))\n",
        "        images = batch[\"images\"][:n_images].to(device)\n",
        "        targets = [{\"boxes\": b, \"labels\": l} for b, l in zip(batch[\"boxes\"], batch[\"labels\"])][:n_images]\n",
        "\n",
        "        # Forward pass\n",
        "        imgs = images.clone()\n",
        "        for m in req_features:\n",
        "            imgs = m(imgs)\n",
        "        X_FM, Y_FM = imgs.shape[2], imgs.shape[3]\n",
        "        _, _, anchors = bbox_generation([img for img in images], targets, X_FM, Y_FM)\n",
        "        pred_locs, pred_scores, objectness_score = rpn_model(imgs)\n",
        "\n",
        "        for i in range(min(n_images, images.shape[0])):\n",
        "            rois = pred_bbox_to_xywh(pred_locs[i], anchors)\n",
        "\n",
        "            if top_k is not None:\n",
        "                # Safe handling of top_k\n",
        "                k = min(top_k, objectness_score[i].shape[0])\n",
        "                top_k_idx = torch.topk(objectness_score[i], k=k).indices\n",
        "                boxes_to_show = rois[top_k_idx.cpu().numpy()]\n",
        "                print(f\"Showing {k} proposals (requested top {top_k})\")\n",
        "            else:\n",
        "                boxes_to_show = rois\n",
        "\n",
        "            show_corner_bbs(images[i], boxes_to_show)\n",
        "\n",
        "            gt_boxes = targets[i][\"boxes\"].cpu().numpy()\n",
        "            if len(gt_boxes) > 0:\n",
        "                show_ground_truth_bbs(images[i], gt_boxes)\n",
        "\n",
        "    rpn_model.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGdUMVYM-nV7",
        "outputId": "3f1579b5-330f-4270-c73f-d4bc71330cdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataset size: 489\n",
            "Validation dataset size: 105\n",
            "Testing dataset size: 106\n"
          ]
        }
      ],
      "source": [
        "image_dir = '/content/drive/MyDrive/APS360_Project/trainA_original_700'\n",
        "pt_dir = 'trainA_testing'\n",
        "json_file_path = '/content/drive/MyDrive/APS360_Project/bdd100k_labels_images_train.json'\n",
        "\n",
        "# Extract labels from JSON (adjust number as desired)\n",
        "all_labels = extract_first_n_labels(json_file_path, 20000)\n",
        "\n",
        "# Create the custom dataset using your method\n",
        "dataset = CustomDataset(image_dir, all_labels, pt_dir)\n",
        "\n",
        "# Split using random_split (70% train, 15% val, 15% test)\n",
        "train_size = int(0.7 * len(dataset))\n",
        "val_size = int(0.15 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
        "print(f\"Training dataset size: {len(train_dataset)}\")\n",
        "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
        "print(f\"Testing dataset size: {len(test_dataset)}\")\n",
        "\n",
        "# Create DataLoaders using your custom collate function\n",
        "batch_size = 8\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn, num_workers=2)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtUATIzFipKE"
      },
      "outputs": [],
      "source": [
        "small_train_dataset = torch.utils.data.Subset(train_dataset, list(range(50)))\n",
        "small_train_loader = torch.utils.data.DataLoader(small_train_dataset, batch_size=batch_size, shuffle=True,\n",
        "                                           collate_fn=custom_collate_fn, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "collapsed": true,
        "id": "UK7yd-U2ADKO",
        "outputId": "0919dac8-bb00-43ff-cffb-064e7dba4b2a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for RPN:\n\tUnexpected key(s) in state_dict: \"conv2.weight\", \"conv2.bias\", \"conv3.weight\", \"conv3.bias\", \"cbam1.fc.0.weight\", \"cbam1.fc.2.weight\", \"cbam1.conv.weight\", \"cbam2.fc.0.weight\", \"cbam2.fc.2.weight\", \"cbam2.conv.weight\", \"skip_conv.weight\", \"skip_conv.bias\". \n\tsize mismatch for conv1.weight: copying a param with shape torch.Size([256, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3]).\n\tsize mismatch for conv1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reg_layer.weight: copying a param with shape torch.Size([36, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([36, 512, 1, 1]).\n\tsize mismatch for cls_layer.weight: copying a param with shape torch.Size([18, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([18, 512, 1, 1]).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-f340a9ad2229>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train the RPN using the training DataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrained_rpn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrpn_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmall_train_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrpn_lambda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Validate (visualize predictions) on both training and validation sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation on training data:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-b252c10530be>\u001b[0m in \u001b[0;36mtrain_epochs\u001b[0;34m(req_features, rpn_model, optimizer, train_dl, epochs, rpn_lambda, device)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# If device is not specified, use the default device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mrpn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./rpn_epoch_200.pth\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mrpn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2581\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   2582\u001b[0m                 \"Error(s) in loading state_dict for {}:\\n\\t{}\".format(\n\u001b[1;32m   2583\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\\t\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for RPN:\n\tUnexpected key(s) in state_dict: \"conv2.weight\", \"conv2.bias\", \"conv3.weight\", \"conv3.bias\", \"cbam1.fc.0.weight\", \"cbam1.fc.2.weight\", \"cbam1.conv.weight\", \"cbam2.fc.0.weight\", \"cbam2.fc.2.weight\", \"cbam2.conv.weight\", \"skip_conv.weight\", \"skip_conv.bias\". \n\tsize mismatch for conv1.weight: copying a param with shape torch.Size([256, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3]).\n\tsize mismatch for conv1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reg_layer.weight: copying a param with shape torch.Size([36, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([36, 512, 1, 1]).\n\tsize mismatch for cls_layer.weight: copying a param with shape torch.Size([18, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([18, 512, 1, 1])."
          ]
        }
      ],
      "source": [
        "# Train the RPN using the training DataLoader\n",
        "trained_rpn = train_epochs(req_features, rpn_model, optimizer, small_train_loader, epochs=5, rpn_lambda=10)\n",
        "\n",
        "# Validate (visualize predictions) on both training and validation sets\n",
        "print(\"Validation on training data:\")\n",
        "validate(trained_rpn, train_loader)\n",
        "print(\"Validation on validation data:\")\n",
        "validate(trained_rpn, val_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8XuAnF4fhd5"
      },
      "source": [
        "###ROI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "uXuKSFCRBKdd"
      },
      "outputs": [],
      "source": [
        "import torchvision.ops as ops\n",
        "from sklearn.cluster import DBSCAN\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.ops as ops\n",
        "\n",
        "class ROIPooling(nn.Module):\n",
        "    def __init__(self, output_size):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            output_size (tuple or int): Size of the output (height, width)\n",
        "        \"\"\"\n",
        "        super(ROIPooling, self).__init__()\n",
        "        if isinstance(output_size, int):\n",
        "            self.output_size = (output_size, output_size)\n",
        "        else:\n",
        "            self.output_size = output_size\n",
        "\n",
        "    def forward(self, features, rois):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            features (Tensor): Input features of shape (N, C, H, W)\n",
        "            rois (Tensor): Regions of Interest in format\n",
        "                          [batch_index, x1, y1, x2, y2] with shape (K, 5)\n",
        "\n",
        "        Returns:\n",
        "            Pooled features of shape (K, C, output_size[0], output_size[1])\n",
        "        \"\"\"\n",
        "        # Ensure rois are on same device as features\n",
        "        rois = rois.to(features.device)\n",
        "\n",
        "        # Calculate spatial scale (feature map size / original image size)\n",
        "        spatial_scale_h = features.size(2) / 224  # Assuming input image size is 224\n",
        "        spatial_scale_w = features.size(3) / 224\n",
        "        spatial_scale = min(spatial_scale_h, spatial_scale_w)\n",
        "\n",
        "        # Perform ROI pooling\n",
        "        pooled_features = ops.roi_pool(\n",
        "            features,\n",
        "            rois,\n",
        "            output_size=self.output_size,\n",
        "            spatial_scale=spatial_scale\n",
        "        )\n",
        "\n",
        "        return pooled_features\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(output_size={})'.format(self.output_size)\n",
        "\n",
        "\n",
        "\n",
        "class RPNWithROI(nn.Module):\n",
        "    def __init__(self, in_channels=512, mid_channels=512, n_anchor=9, roi_size=(7,7)):\n",
        "        super(RPNWithROI, self).__init__()\n",
        "        # Original RPN layers\n",
        "        self.conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.reg_layer = nn.Conv2d(mid_channels, n_anchor*4, kernel_size=1, stride=1)\n",
        "        self.cls_layer = nn.Conv2d(mid_channels, n_anchor*2, kernel_size=1, stride=1)\n",
        "\n",
        "        # ROI Pooling layer\n",
        "        self.roi_pool = ops.RoIPool(roi_size, spatial_scale=1.0)\n",
        "\n",
        "        # Initialize weights\n",
        "        for layer in [self.conv1, self.reg_layer, self.cls_layer]:\n",
        "            nn.init.normal_(layer.weight, std=0.01)\n",
        "            nn.init.constant_(layer.bias, 0)\n",
        "\n",
        "    def forward(self, x, anchors=None, gt_boxes=None):\n",
        "        # Original RPN forward pass\n",
        "        batch_size = x.shape[0]\n",
        "        x = self.conv1(x)\n",
        "        pred_anchor_locs = self.reg_layer(x)  # [B, n_anchor*4, H, W]\n",
        "        pred_cls_scores = self.cls_layer(x)   # [B, n_anchor*2, H, W]\n",
        "\n",
        "        # Reshape outputs\n",
        "        pred_anchor_locs = pred_anchor_locs.permute(0,2,3,1).contiguous().view(batch_size, -1, 4)  # [B, H*W*n_anchor, 4]\n",
        "        pred_cls_scores = pred_cls_scores.permute(0,2,3,1).contiguous().view(batch_size, -1, 2)     # [B, H*W*n_anchor, 2]\n",
        "        objectness_score = pred_cls_scores[:,:,1]  # [B, H*W*n_anchor]\n",
        "\n",
        "        if anchors is not None and gt_boxes is not None:\n",
        "            # Convert predictions to boxes\n",
        "            proposal_boxes = self._generate_proposals(pred_anchor_locs, anchors)  # [B, H*W*n_anchor, 4]\n",
        "\n",
        "            # Perform NMS and ROI pooling\n",
        "            pooled_features = self._process_proposals(x, proposal_boxes, objectness_score)\n",
        "            return pred_anchor_locs, pred_cls_scores, objectness_score, pooled_features\n",
        "\n",
        "        return pred_anchor_locs, pred_cls_scores, objectness_score\n",
        "\n",
        "    def _generate_proposals(self, pred_locs, anchors):\n",
        "        \"\"\"Convert anchor offsets to boxes in (x1,y1,x2,y2) format\"\"\"\n",
        "        # pred_locs: [B, N, 4] (dy, dx, dh, dw)\n",
        "        # anchors: [N, 4] (y1, x1, y2, x2)\n",
        "        proposals = torch.zeros_like(pred_locs)\n",
        "\n",
        "        # Convert anchors to (ctr_y, ctr_x, h, w)\n",
        "        anchors_cpu = anchors.cpu()\n",
        "        anchor_h = anchors[:, 2] - anchors[:, 0]\n",
        "        anchor_w = anchors[:, 3] - anchors[:, 1]\n",
        "        anchor_ctr_y = anchors[:, 0] + 0.5 * anchor_h\n",
        "        anchor_ctr_x = anchors[:, 1] + 0.5 * anchor_w\n",
        "\n",
        "        # Apply predictions\n",
        "        dy = pred_locs[..., 0]\n",
        "        dx = pred_locs[..., 1]\n",
        "        dh = pred_locs[..., 2]\n",
        "        dw = pred_locs[..., 3]\n",
        "\n",
        "        ctr_y = dy * anchor_h[None, :] + anchor_ctr_y[None, :]\n",
        "        ctr_x = dx * anchor_w[None, :] + anchor_ctr_x[None, :]\n",
        "        h = torch.exp(dh) * anchor_h[None, :]\n",
        "        w = torch.exp(dw) * anchor_w[None, :]\n",
        "\n",
        "        # Convert back to (x1,y1,x2,y2)\n",
        "        proposals[..., 0] = ctr_x - 0.5 * w  # x1\n",
        "        proposals[..., 1] = ctr_y - 0.5 * h  # y1\n",
        "        proposals[..., 2] = ctr_x + 0.5 * w  # x2\n",
        "        proposals[..., 3] = ctr_y + 0.5 * h  # y2\n",
        "\n",
        "        return proposals\n",
        "\n",
        "    def _process_proposals(self, features, boxes, scores, conf_thresh=0.7, iou_thresh=0.5, top_n=50):\n",
        "        \"\"\"Process proposals through NMS and ROI pooling\"\"\"\n",
        "        batch_size = features.shape[0]\n",
        "        pooled_features = []\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            # Filter by confidence\n",
        "            mask = scores[i] > conf_thresh\n",
        "            filtered_boxes = boxes[i][mask]\n",
        "            filtered_scores = scores[i][mask]\n",
        "\n",
        "            if len(filtered_boxes) > 0:\n",
        "                # Apply NMS\n",
        "                keep = ops.nms(filtered_boxes, filtered_scores, iou_thresh)\n",
        "                final_boxes = filtered_boxes[keep][:top_n]\n",
        "\n",
        "                # Format for ROI pooling [batch_idx, x1, y1, x2, y2]\n",
        "                roi_input = torch.cat([\n",
        "                    torch.full((len(final_boxes), 1), i, dtype=torch.float32, device=features.device),\n",
        "                    final_boxes\n",
        "                ], dim=1)\n",
        "\n",
        "                # ROI Pooling\n",
        "                pooled = self.roi_pool(features[i:i+1], roi_input)  # Maintain batch dim\n",
        "                pooled_features.append(pooled)\n",
        "            else:\n",
        "                pooled_features.append(torch.empty(0, device=features.device))\n",
        "\n",
        "        return torch.cat(pooled_features, dim=0) if batch_size > 1 else pooled_features[0]\n",
        "\n",
        "def combine_boxes(boxes, scores=None, iou_threshold=0.5, min_cluster_size=2):\n",
        "    # Convert to numpy if needed (maintaining [y1,x1,y2,x2] format)\n",
        "    boxes_np = boxes.cpu().numpy() if isinstance(boxes, torch.Tensor) else np.array(boxes)\n",
        "\n",
        "    if len(boxes_np) == 0:\n",
        "        return boxes, np.array([])\n",
        "\n",
        "    # Calculate IoU matrix using your existing function\n",
        "    iou_matrix = compute_iou_vectorized(boxes_np, boxes_np)  # Compare all boxes to all boxes\n",
        "\n",
        "    # Convert IoU to distance (1-IoU) for DBSCAN\n",
        "    distance_matrix = 1 - iou_matrix\n",
        "\n",
        "    # Cluster boxes using DBSCAN\n",
        "    clustering = DBSCAN(\n",
        "        eps=1-iou_threshold,\n",
        "        min_samples=min_cluster_size,\n",
        "        metric='precomputed'\n",
        "    ).fit(distance_matrix)\n",
        "\n",
        "    cluster_labels = clustering.labels_\n",
        "\n",
        "    # Combine boxes in each cluster\n",
        "    combined_boxes = []\n",
        "    for cluster_id in np.unique(cluster_labels):\n",
        "        if cluster_id == -1:  # Skip noise points (boxes not in any cluster)\n",
        "            continue\n",
        "\n",
        "        cluster_mask = (cluster_labels == cluster_id)\n",
        "        cluster_boxes = boxes_np[cluster_mask]\n",
        "\n",
        "        # Calculate combined box coordinates (still in [y1,x1,y2,x2] format)\n",
        "        y1 = np.min(cluster_boxes[:, 0])\n",
        "        x1 = np.min(cluster_boxes[:, 1])\n",
        "        y2 = np.max(cluster_boxes[:, 2])\n",
        "        x2 = np.max(cluster_boxes[:, 3])\n",
        "        combined_boxes.append([y1, x1, y2, x2])\n",
        "\n",
        "    # Add unclustered boxes (if any)\n",
        "    unclustered_mask = (cluster_labels == -1)\n",
        "    if np.any(unclustered_mask):\n",
        "        combined_boxes.extend(boxes_np[unclustered_mask])\n",
        "\n",
        "    combined_boxes = np.array(combined_boxes) if len(combined_boxes) > 0 else np.empty((0, 4))\n",
        "\n",
        "    # Convert back to tensor if input was tensor\n",
        "    if isinstance(boxes, torch.Tensor):\n",
        "        combined_boxes = torch.from_numpy(combined_boxes).float().to(boxes.device)\n",
        "\n",
        "    return combined_boxes, cluster_labels\n",
        "\n",
        "\n",
        "def train_epochs(req_features, rpn_model, optimizer, train_dl, epochs=20, rpn_lambda=10, device=None):\n",
        "    if device is None:  # If device is not specified, use the default device\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    rpn_model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_samples = 0\n",
        "        sum_loss = 0.0\n",
        "        sum_loss_cls = 0.0\n",
        "        sum_loss_loc = 0.0\n",
        "        for batch in train_dl:\n",
        "            images = batch[\"images\"].to(device) # shape (B,C,H,W)\n",
        "            targets = [{\"boxes\": b, \"labels\": l} for b, l in zip(batch[\"boxes\"], batch[\"labels\"])]\n",
        "            B = images.shape[0]\n",
        "            #print(f\"image size: {B}\")\n",
        "            total_samples += B\n",
        "            # Forward through frozen backbone\n",
        "            imgs = images.clone()\n",
        "            with torch.no_grad():\n",
        "                feat = imgs\n",
        "                for m in req_features:\n",
        "                    feat = m(feat)\n",
        "            X_FM, Y_FM = feat.shape[2], feat.shape[3]\n",
        "            # Compute GT targets (for all anchors)\n",
        "            gt_locs_np, gt_scores_np, anchors = bbox_generation([img for img in images], targets, X_FM, Y_FM)\n",
        "            print(\"Hmm\")\n",
        "            gt_locs = torch.from_numpy(gt_locs_np.astype(np.float32)).to(device)\n",
        "            gt_scores = torch.from_numpy(gt_scores_np.astype(np.float32)).to(device)\n",
        "            # Forward RPN\n",
        "            pred_locs, pred_scores, objectness_score = rpn_model(feat)\n",
        "            # Compute classification loss: flatten predictions and GT.\n",
        "            cls_loss = F.cross_entropy(pred_scores.view(-1,2),\n",
        "                                       gt_scores.view(-1).long(),\n",
        "                                       ignore_index=-1)\n",
        "            # Compute regression loss for positive anchors only.\n",
        "            pos_mask = gt_scores > 0\n",
        "            if pos_mask.sum() > 0:\n",
        "                pred_pos = pred_locs[pos_mask]\n",
        "                gt_pos = gt_locs[pos_mask]\n",
        "                diff = torch.abs(gt_pos - pred_pos)\n",
        "                loc_loss = torch.where(diff < 1, 0.5 * diff**2, diff - 0.5)\n",
        "                loc_loss = loc_loss.sum() / pos_mask.sum().float()\n",
        "            else:\n",
        "                loc_loss = torch.tensor(0.0, device=device)\n",
        "            loss = cls_loss + rpn_lambda * loc_loss\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # Logging\n",
        "            sum_loss += loss.item() * B\n",
        "            sum_loss_cls += cls_loss.item() * B\n",
        "            sum_loss_loc += (rpn_lambda * loc_loss.item()) * B\n",
        "\n",
        "        # Epoch summary\n",
        "        avg_loss = sum_loss / total_samples\n",
        "        avg_cls = sum_loss_cls / total_samples\n",
        "        avg_loc = sum_loss_loc / total_samples\n",
        "        print(f\"Epoch {epoch+1}/{epochs}: Loss {avg_loss:.3f} | Cls {avg_cls:.3f} | Loc {avg_loc:.3f}\")\n",
        "\n",
        "        # Save checkpoint\n",
        "        if (epoch+1) % 5 == 0:\n",
        "            torch.save(rpn_model.state_dict(), f\"./rpn_epoch_{epoch+1}.pth\")\n",
        "\n",
        "    return rpn_model\n",
        "\n",
        "def validate(rpn_model, data_loader, n_images=15, top_k=20, conf_thresh=0.5, iou_thresh=0.5, combine_thresh=0.4):\n",
        "    rpn_model.eval()\n",
        "    roi_pool = ROIPooling(output_size=(14, 14)).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        batch = next(iter(data_loader))\n",
        "        images = batch[\"images\"][:n_images].to(device)\n",
        "        targets = [{\"boxes\": b.to(device), \"labels\": l.to(device)}\n",
        "                 for b, l in zip(batch[\"boxes\"], batch[\"labels\"])][:n_images]\n",
        "\n",
        "        # Forward pass\n",
        "        imgs = images.clone()\n",
        "        for m in req_features:\n",
        "            imgs = m(imgs)\n",
        "\n",
        "        X_FM, Y_FM = imgs.shape[2], imgs.shape[3]\n",
        "        _, _, anchors_np = bbox_generation([img for img in images], targets, X_FM, Y_FM)\n",
        "        anchors = torch.from_numpy(anchors_np).float().to(device)\n",
        "\n",
        "        pred_locs, pred_scores, objectness_score = rpn_model(imgs)\n",
        "\n",
        "        for i in range(min(n_images, images.shape[0])):\n",
        "            # 1. Convert predictions to boxes\n",
        "            rois_xyxy_np = pred_bbox_to_xywh(pred_locs[i].cpu(), anchors.cpu().numpy())\n",
        "            rois_xyxy = torch.from_numpy(rois_xyxy_np).float().to(device)\n",
        "\n",
        "            # 2. Select top-k highest scoring proposals\n",
        "            if top_k:\n",
        "                k = min(top_k, len(objectness_score[i]))\n",
        "                top_k_idx = torch.topk(objectness_score[i], k=k).indices\n",
        "                top_k_boxes = rois_xyxy[top_k_idx]\n",
        "                top_k_scores = objectness_score[i][top_k_idx]\n",
        "            else:\n",
        "                top_k_boxes = rois_xyxy\n",
        "                top_k_scores = objectness_score[i]\n",
        "\n",
        "            # 3. Confidence threshold filtering\n",
        "            conf_mask = top_k_scores > conf_thresh\n",
        "            filtered_boxes = top_k_boxes[conf_mask]\n",
        "            filtered_scores = top_k_scores[conf_mask]\n",
        "\n",
        "            if len(filtered_boxes) > 0:\n",
        "                # 4. ROI Pooling on filtered boxes\n",
        "                roi_input = torch.cat([\n",
        "                    torch.zeros((len(filtered_boxes), 1), device=device),\n",
        "                    filtered_boxes\n",
        "                ], dim=1)\n",
        "                pooled_features = roi_pool(imgs[i].unsqueeze(0), roi_input)\n",
        "\n",
        "                # 5. Combine boxes after ROI pooling\n",
        "                combined_boxes, _ = combine_boxes(\n",
        "                    filtered_boxes[:, [1, 0, 3, 2]],  # Convert to [y1,x1,y2,x2]\n",
        "                    iou_threshold=combine_thresh,\n",
        "                    min_cluster_size=2\n",
        "                )\n",
        "\n",
        "                combined_boxes = combined_boxes[:, [1, 0, 3, 2]]  # Back to [x1,y1,x2,y2]\n",
        "\n",
        "                print(f\"Original {len(filtered_boxes)} proposals → Combined to {len(combined_boxes)} boxes\")\n",
        "\n",
        "                # Visualization of final combined proposals\n",
        "                show_corner_bbs(images[i].cpu(), combined_boxes.cpu().numpy())\n",
        "\n",
        "                if len(targets[i][\"boxes\"]) > 0:\n",
        "                    show_ground_truth_bbs(images[i].cpu(), targets[i][\"boxes\"].cpu().numpy())\n",
        "            else:\n",
        "                print(f\"No proposals met confidence threshold {conf_thresh} for image {i}\")\n",
        "\n",
        "    rpn_model.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize and run training\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "vgg_model = torchvision.models.vgg16(pretrained=True).to(device)\n",
        "vgg_model.eval()\n",
        "\n",
        "# Freeze VGG features\n",
        "for param in vgg_model.features.parameters():\n",
        "    param.requires_grad = False\n",
        "req_features = [layer for layer in list(vgg_model.features)[:30]]\n",
        "\n",
        "# Create RPN model\n",
        "rpn_model = RPNWithROI().to(device)\n",
        "optimizer = torch.optim.Adam(rpn_model.parameters(), lr=0.001)\n",
        "for param in vgg_model.features[-4:].parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "base_lr = 0.001\n",
        "\n",
        "# Get backbone parameters from the VGG model\n",
        "backbone_params = list(vgg_model.features.parameters())\n",
        "# Get new parameters from the custom layers (e.g., RPN)\n",
        "new_params = list(rpn_model.parameters())\n",
        "\n",
        "optimizer = torch.optim.Adam([\n",
        "    {'params': backbone_params, 'lr': base_lr * 0.1},\n",
        "    {'params': new_params, 'lr': base_lr}\n",
        "])\n",
        "\n",
        "# Alternative optimizer (commented out):\n",
        "# optimizer = torch.optim.Adam(rpn_model.parameters(), lr=0.0005)\n",
        "# Train and validate\n",
        "trained_rpn = train_epochs(req_features, rpn_model, optimizer, train_loader,epochs=3, rpn_lambda=5, device=device)\n",
        "\n",
        "print(\"Validation on training data:\")\n",
        "validate(trained_rpn, train_loader)\n",
        "\n",
        "print(\"Validation on validation data:\")\n",
        "validate(trained_rpn, val_loader)"
      ],
      "metadata": {
        "id": "p5lsiZIE7udE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}