# -*- coding: utf-8 -*-
"""YOLO.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VBYe_82gvNnOHwPDsFV-M4C-IpKyeLlG

###Print YOLO Output
"""

import numpy as np
import cv2
import os
import time
from google.colab.patches import cv2_imshow  # For displaying images in Colab

# Mount Google Drive (if your images are stored in Google Drive)
from google.colab import drive
drive.mount('/content/drive')

# Define the directory containing the images
image_dir = '/content/drive/MyDrive/Project/trainA_original_700'  # Replace with your directory path

# Load the COCO class labels our YOLO model was trained on
labelsPath = 'coco.names'
LABELS = open(labelsPath).read().strip().split("\n")

# Initialize a list of colors to represent each possible class label
np.random.seed(42)
COLORS = np.random.randint(0, 255, size=(len(LABELS), 3), dtype="uint8")

# Derive the paths to the YOLO weights and model configuration
weightsPath = 'yolov3.weights'
configPath = 'yolov3.cfg'

# Load our YOLO object detector trained on COCO dataset (80 classes)
print("[INFO] loading YOLO from disk...")
net = cv2.dnn.readNetFromDarknet(configPath, weightsPath)

# Define confidence and threshold values
confidence_threshold = 0.5
nms_threshold = 0.3

# Initialize a list to store the results
results = []

# Iterate over all images in the directory
for image_name in os.listdir(image_dir):
    if not image_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):
        continue  # Skip non-image files

    # Load the image
    image_path = os.path.join(image_dir, image_name)
    image = cv2.imread(image_path)
    if image is None:
        print(f"[WARNING] Could not load image: {image_name}")
        continue

    (H, W) = image.shape[:2]

    ln = net.getLayerNames()
    if len(net.getUnconnectedOutLayers().shape) == 1:
      ln = [ln[i - 1] for i in net.getUnconnectedOutLayers()]  # Reshape to (N,)
    else:
      ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]  # Expected behaviour

    # Construct a blob from the input image and perform a forward pass
    blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416), swapRB=True, crop=False)
    net.setInput(blob)
    start = time.time()
    layerOutputs = net.forward(ln)
    end = time.time()

    print(f"[INFO] YOLO took {end - start:.6f} seconds for {image_name}")

    # Initialize lists to store detected bounding boxes, confidences, and class IDs
    boxes = []
    confidences = []
    classIDs = []

    # Loop over each of the layer outputs
    for output in layerOutputs:
        # Loop over each of the detections
        for detection in output:
            # Extract the class ID and confidence of the current object detection
            scores = detection[5:]
            classID = np.argmax(scores)
            confidence = scores[classID]

            # Filter out weak predictions
            if confidence > confidence_threshold:
                # Scale the bounding box coordinates back relative to the image size
                box = detection[0:4] * np.array([W, H, W, H])
                (centerX, centerY, width, height) = box.astype("int")

                # Derive the top-left corner of the bounding box
                x = int(centerX - (width / 2))
                y = int(centerY - (height / 2))

                # Update our lists
                boxes.append([x, y, int(width), int(height)])
                confidences.append(float(confidence))
                classIDs.append(classID)

    # Apply non-maxima suppression to suppress weak, overlapping bounding boxes
    idxs = cv2.dnn.NMSBoxes(boxes, confidences, confidence_threshold, nms_threshold)

    # Initialize a list to store labels for the current image
    image_labels = []

    # Ensure at least one detection exists
    if len(idxs) > 0:
        # Loop over the indexes we are keeping
        for i in idxs.flatten():
            # Extract the bounding box coordinates
            (x, y) = (boxes[i][0], boxes[i][1])
            (w, h) = (boxes[i][2], boxes[i][3])

            # Get the class label and confidence
            label = LABELS[classIDs[i]]
            confidence = confidences[i]

            # Append the label and confidence to the image_labels list
            image_labels.append(f"{label}: {confidence:.4f}")

            # Draw the bounding box and label on the image (optional)
            color = [int(c) for c in COLORS[classIDs[i]]]
            cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)
            text = f"{label}: {confidence:.4f}"
            cv2.putText(image, text, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

    # Append the results for the current image to the results list
    results.append([image_name, *image_labels])

    # Display the image with bounding boxes (optional)
    print(f"Detections for {image_name}:")
    cv2_imshow(image)  # Use cv2_imshow in Colab
    cv2.waitKey(0)

# Print the results
print("\nResults:")
for result in results:
    print(result)

"""###YOLO Accuracy Testing on 100 Sample Images"""

!pip install ijson

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
from torch.utils.data import Dataset, random_split
from torchvision.datasets import ImageFolder
import numpy as np
import matplotlib.pyplot as plt
import os
import ijson

def extract_first_n_labels(json_file_path, n):
    labels = []
    with open(json_file_path, 'rb') as f:
        parser = ijson.items(f, 'item')
        for i, item in enumerate(parser):
            if i >= n: # ensuring only labels of n images are extracted from the json file
                break
            # Filter out objects with poly2d labels and keep only box2d labels
            filtered_labels = [
                {
                    "category": label_item.get("category"),
                    "box2d": label_item.get("box2d")
                }
                for label_item in item.get("labels", [])
                if "box2d" in label_item # only get label_item with box2d
            ]
            labels.append({
                "name": item.get("name"),
                "timestamp": item.get("timestamp"),
                "labels": filtered_labels
            })
    return labels

json_file_path = '/content/drive/MyDrive/Project/bdd100k_labels_images_train.json'

# Extract the first 700 labels
first_100_labels = extract_first_n_labels(json_file_path, 100)

# Print a few labels to verify
for i, label in enumerate(first_100_labels[:2]):
    print(f"Label {i + 1}:")
    print(f"  Name: {label['name']}")
    print(f"  Timestamp: {label['timestamp']}")
    print("  Labels:")
    for obj in label['labels']:
        print(f"    Category: {obj['category']}")
        print(f"    2D Box: {obj['box2d']}")
    print("-" * 40)

import numpy as np
import cv2
import os
import time
from google.colab.patches import cv2_imshow  # For displaying images in Colab


# Define the directory containing the images
image_dir = '/content/drive/MyDrive/Project/trainA_original_700'  # Replace with your directory path

# Load the COCO class labels our YOLO model was trained on
labelsPath = 'coco.names'
LABELS = open(labelsPath).read().strip().split("\n")

# Initialize a list of colors to represent each possible class label
np.random.seed(42)
COLORS = np.random.randint(0, 255, size=(len(LABELS), 3), dtype="uint8")

# Derive the paths to the YOLO weights and model configuration
weightsPath = 'yolov3.weights'
configPath = 'yolov3.cfg'

# Load our YOLO object detector trained on COCO dataset (80 classes)
print("[INFO] loading YOLO from disk...")
net = cv2.dnn.readNetFromDarknet(configPath, weightsPath)

# Define confidence and threshold values
confidence_threshold = 0.5
nms_threshold = 0.3

# Initialize a list to store the results
results = []

# Load the first 100 labels from the JSON file
def extract_first_n_labels(json_file_path, n):
    import ijson
    labels = []
    with open(json_file_path, 'rb') as f:
        parser = ijson.items(f, 'item')
        for i, item in enumerate(parser):
            if i >= n:  # Stop after extracting n labels
                break
            # Filter out objects with poly2d labels and keep only box2d labels
            filtered_labels = [
                {
                    "category": label_item.get("category"),
                    "box2d": label_item.get("box2d")
                }
                for label_item in item.get("labels", [])
                if "box2d" in label_item  # Only get label_item with box2d
            ]
            labels.append({
                "name": item.get("name"),
                "timestamp": item.get("timestamp"),
                "labels": filtered_labels
            })
    return labels

json_file_path = '/content/drive/MyDrive/Project/bdd100k_labels_images_train.json'
first_100_labels = extract_first_n_labels(json_file_path, 100)

# Function to calculate Intersection over Union (IoU)
def calculate_iou(box1, box2):
    """
    Calculate Intersection over Union (IoU) between two bounding boxes.
    Each box is in the format [x1, y1, x2, y2].
    """
    # Calculate intersection coordinates
    x1 = max(box1[0], box2[0])
    y1 = max(box1[1], box2[1])
    x2 = min(box1[2], box2[2])
    y2 = min(box1[3], box2[3])

    # Calculate intersection area
    intersection_area = max(0, x2 - x1) * max(0, y2 - y1)

    # Calculate areas of the boxes
    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])
    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])

    # Calculate IoU
    iou = intersection_area / (box1_area + box2_area - intersection_area)
    return iou

# Function to calculate accuracy
def calculate_accuracy(predicted_labels, ground_truth_labels, iou_threshold=0.5):
    """
    Calculate accuracy by comparing predicted labels with ground truth labels.
    """
    correct = 0
    total = len(ground_truth_labels)

    for gt in ground_truth_labels:
        gt_class = gt['category']
        gt_box = [gt['box2d']['x1'], gt['box2d']['y1'], gt['box2d']['x2'], gt['box2d']['y2']]

        for pred in predicted_labels:
            pred_class, pred_box = pred

            # Check if the predicted class matches the ground truth class
            if pred_class == gt_class:
                # Calculate IoU
                iou = calculate_iou(gt_box, pred_box)
                if iou >= iou_threshold:
                    correct += 1
                    break  # Only count one correct detection per ground truth

    accuracy = correct / total if total > 0 else 0
    return accuracy

# Initialize variables to accumulate accuracy
total_accuracy = 0.0
num_images_with_ground_truth = 0

# Iterate over the first 100 labels
for item in first_100_labels:
    image_name = item['name']
    ground_truth_labels = item['labels']

    # Load the image
    image_path = os.path.join(image_dir, image_name)
    image = cv2.imread(image_path)
    if image is None:
        print(f"[WARNING] Could not load image: {image_name}")
        continue

    (H, W) = image.shape[:2]

    # Determine only the *output* layer names that we need from YOLO
    ln = net.getLayerNames()
    ln = [ln[i - 1] for i in net.getUnconnectedOutLayers().flatten()]

    # Construct a blob from the input image and perform a forward pass
    blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416), swapRB=True, crop=False)
    net.setInput(blob)
    start = time.time()
    layerOutputs = net.forward(ln)
    end = time.time()

    print(f"[INFO] YOLO took {end - start:.6f} seconds for {image_name}")

    # Initialize lists to store detected bounding boxes, confidences, and class IDs
    boxes = []
    confidences = []
    classIDs = []

    # Loop over each of the layer outputs
    for output in layerOutputs:
        # Loop over each of the detections
        for detection in output:
            # Extract the class ID and confidence of the current object detection
            scores = detection[5:]
            classID = np.argmax(scores)
            confidence = scores[classID]

            # Filter out weak predictions
            if confidence > confidence_threshold:
                # Scale the bounding box coordinates back relative to the image size
                box = detection[0:4] * np.array([W, H, W, H])
                (centerX, centerY, width, height) = box.astype("int")

                # Derive the top-left corner of the bounding box
                x = int(centerX - (width / 2))
                y = int(centerY - (height / 2))

                # Update our lists
                boxes.append([x, y, int(width), int(height)])
                confidences.append(float(confidence))
                classIDs.append(classID)

    # Apply non-maxima suppression to suppress weak, overlapping bounding boxes
    idxs = cv2.dnn.NMSBoxes(boxes, confidences, confidence_threshold, nms_threshold)

    # Initialize a list to store predicted labels for the current image
    predicted_labels = []

    # Ensure at least one detection exists
    if len(idxs) > 0:
        # Loop over the indexes we are keeping
        for i in idxs.flatten():
            # Extract the bounding box coordinates
            (x, y) = (boxes[i][0], boxes[i][1])
            (w, h) = (boxes[i][2], boxes[i][3])

            # Get the class label and confidence
            label = LABELS[classIDs[i]]
            confidence = confidences[i]

            # Append the predicted label and bounding box to the predicted_labels list
            predicted_labels.append((label, [x, y, x + w, y + h]))

            # Draw the bounding box and label on the image (optional)
            color = [int(c) for c in COLORS[classIDs[i]]]
            cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)
            text = f"{label}: {confidence:.4f}"
            cv2.putText(image, text, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

    # Calculate accuracy for the current image
    if ground_truth_labels:  # Only calculate accuracy if ground truth labels exist
        accuracy = calculate_accuracy(predicted_labels, ground_truth_labels)
        total_accuracy += accuracy
        num_images_with_ground_truth += 1

    # Append the results for the current image to the results list
    results.append([image_name, *predicted_labels])

    # Display only the first image with bounding boxes
    if num_images_with_ground_truth == 1:
        print(f"Detections for {image_name}:")
        cv2_imshow(image)  # Use cv2_imshow in Colab
        cv2.waitKey(0)

# Calculate and print the average overall accuracy
if num_images_with_ground_truth > 0:
    average_accuracy = total_accuracy / num_images_with_ground_truth
    print(f"\nAverage Overall Accuracy: {average_accuracy:.2f}")
else:
    print("\nNo images with ground truth labels found.")

# Return the results list
results