{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import ijson\n",
    "from torchvision.ops import nms\n",
    "import random\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Input image size (height, width)\n",
    "ISIZE = (720, 1280)\n",
    "\n",
    "# ImageNet statistics (for VGG16)\n",
    "# imagenet_mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "# imagenet_std  = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "\n",
    "def normalize_tensor(img):\n",
    "    \"\"\"Normalize a tensor image (C, H, W) with values in [0,255].\"\"\"\n",
    "    img = img / 255.0\n",
    "    return img\n",
    "\n",
    "def unnormalize_tensor(img):\n",
    "    \"\"\"Convert a normalized tensor back to a displayable numpy image.\"\"\"\n",
    "    img = img * 255.0\n",
    "    return img.clamp(0, 255).byte().cpu().numpy()\n",
    "\n",
    "# Global anchor parameters\n",
    "ratios = [0.5, 1, 2]\n",
    "anchor_scales = [4, 8, 16, 32, 64] # generate more anchors at each location\n",
    "\n",
    "\n",
    "#################################################################\n",
    "def extract_first_n_labels(json_file_path, n):\n",
    "    labels = []\n",
    "    with open(json_file_path, 'rb') as f:\n",
    "        parser = ijson.items(f, 'item')\n",
    "        for i, item in enumerate(parser):\n",
    "            if i >= n:\n",
    "                break\n",
    "            filtered_labels = [\n",
    "                {\"category\": li.get(\"category\"), \"box2d\": li.get(\"box2d\")}\n",
    "                for li in item.get(\"labels\", []) if \"box2d\" in li\n",
    "            ]\n",
    "            labels.append({\n",
    "                \"name\": item.get(\"name\"),\n",
    "                \"timestamp\": item.get(\"timestamp\"),\n",
    "                \"labels\": filtered_labels\n",
    "            })\n",
    "    return labels\n",
    "\n",
    "def standardize_filename(path_or_name):\n",
    "    base = os.path.basename(path_or_name)\n",
    "    base, _ = os.path.splitext(base)\n",
    "    return base\n",
    "\n",
    "\n",
    "def contrast_stretch(image, low_percentile=10, high_percentile=90):\n",
    "    \"\"\"\n",
    "    Perform contrast stretching while printing debug info to avoid full black images.\n",
    "\n",
    "    :param image: PyTorch tensor of shape (C, H, W)\n",
    "    :param low_percentile: Lower percentile for clipping\n",
    "    :param high_percentile: Upper percentile for clipping\n",
    "    :return: Contrast-stretched tensor\n",
    "    \"\"\"\n",
    "    image_np = image.cpu().numpy()\n",
    "\n",
    "    # Compute percentiles\n",
    "    min_val = np.percentile(image_np, low_percentile)\n",
    "    max_val = np.percentile(image_np, high_percentile)\n",
    "\n",
    "    # print(f\"Debug: Min percentile value = {min_val}, Max percentile value = {max_val}\")\n",
    "\n",
    "    if max_val - min_val < 1e-6:\n",
    "        print(\"Warning: Min and max values are too close! Returning original image.\")\n",
    "        return image  # Return original image to avoid black output\n",
    "\n",
    "    # Apply contrast stretching\n",
    "    stretched = (image_np - min_val) / (max_val - min_val + 1e-8)\n",
    "\n",
    "    # Clip values to avoid over-brightening\n",
    "    stretched = np.clip(stretched, 0, 1)\n",
    "\n",
    "    return torch.tensor(stretched, dtype=torch.float32)\n",
    "\n",
    "\n",
    "######################################################################\n",
    "\n",
    "# 1) \"CustomDataset\" with [x1, y1, x2, y2]\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_dir, labels, pt_dir='pt_files'):\n",
    "        self.image_dir = image_dir\n",
    "        self.pt_dir = pt_dir\n",
    "        os.makedirs(self.pt_dir, exist_ok=True)\n",
    "        self.image_files = sorted([\n",
    "            os.path.join(image_dir, f)\n",
    "            for f in os.listdir(image_dir)\n",
    "            if f.lower().endswith(('.jpg', '.png', '.jpeg'))\n",
    "        ])\n",
    "        self.label_dict = {}\n",
    "        for item in labels:\n",
    "            key = os.path.splitext(os.path.basename(item[\"name\"]))[0]\n",
    "            self.label_dict[key] = item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_files[idx]\n",
    "        pt_path = os.path.join(\n",
    "            self.pt_dir,\n",
    "            os.path.basename(image_path)\n",
    "                .replace('.jpg', '.pt')\n",
    "                .replace('.png', '.pt')\n",
    "                .replace('.jpeg', '.pt')\n",
    "        )\n",
    "        if os.path.exists(pt_path):\n",
    "            image_tensor = torch.load(pt_path)\n",
    "        else:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            if image.size != (ISIZE[1], ISIZE[0]):  # (width, height)\n",
    "                image = image.resize((ISIZE[1], ISIZE[0]))\n",
    "            image_tensor = transforms.PILToTensor()(image).float()\n",
    "            torch.save(image_tensor, pt_path)\n",
    "        \n",
    "        # Contrast Stretching \n",
    "        # image_tensor = contrast_stretch(image_tensor)\n",
    "\n",
    "        # Normalize to [0,1]\n",
    "\n",
    "\n",
    "        # Parse ground truth\n",
    "        key = os.path.splitext(os.path.basename(image_path))[0]\n",
    "        matched = self.label_dict.get(key, None)\n",
    "        if matched is None or \"labels\" not in matched:\n",
    "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            labels = torch.zeros((0,), dtype=torch.int64)\n",
    "            names = []\n",
    "        else:\n",
    "            box_list = []\n",
    "            cat_list = []\n",
    "            for obj in matched[\"labels\"]:\n",
    "                if \"box2d\" in obj:\n",
    "                    b2d = obj[\"box2d\"]\n",
    "                    # *** IMPORTANT *** interpret these as [x1, y1, x2, y2]\n",
    "                    # If your JSON is truly y1,x1,y2,x2, then flip them here.\n",
    "                    # But let's assume the user wants x1= b2d[\"x1\"], y1= b2d[\"y1\"], etc.\n",
    "                    x1 = float(b2d[\"x1\"])\n",
    "                    y1 = float(b2d[\"y1\"])\n",
    "                    x2 = float(b2d[\"x2\"])\n",
    "                    y2 = float(b2d[\"y2\"])\n",
    "                    box_list.append([x1, y1, x2, y2])\n",
    "                    cat_list.append(obj[\"category\"])\n",
    "            if len(box_list) == 0:\n",
    "                boxes = torch.zeros((0,4), dtype=torch.float32)\n",
    "                labels = torch.zeros((0,), dtype=torch.int64)\n",
    "                names = []\n",
    "            else:\n",
    "                boxes = torch.tensor(box_list, dtype=torch.float32)\n",
    "                labels = torch.ones((len(box_list),), dtype=torch.int64)  # e.g. '1' for object\n",
    "                names = cat_list\n",
    "\n",
    "        return {\n",
    "            \"image\": image_tensor,  # (C,H,W) in [0,1]\n",
    "            \"boxes\": boxes,         # shape (N,4) in [x1,y1,x2,y2]\n",
    "            \"labels\": labels,       # shape (N,)\n",
    "            \"names\": names,\n",
    "            \"index\": idx,\n",
    "            \"img_name\": os.path.basename(image_path) \n",
    "        }\n",
    "\n",
    "# 2) Collate\n",
    "def custom_collate_fn(batch):\n",
    "    # Move to device later in the training loop\n",
    "    images = [item[\"image\"] for item in batch]\n",
    "    boxes  = [item[\"boxes\"] for item in batch]\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "    names  = [item[\"names\"] for item in batch]\n",
    "    idxs   = [item[\"index\"] for item in batch]\n",
    "    img_ids = [item[\"img_name\"] for item in batch] \n",
    "    # Stack images => shape (B, C, H, W)\n",
    "    return {\n",
    "        \"images\": torch.stack(images, dim=0),\n",
    "        \"boxes\": boxes,\n",
    "        \"labels\": labels,\n",
    "        \"names\": names,\n",
    "        \"indices\": idxs,\n",
    "        \"img_ids\": img_ids\n",
    "    }\n",
    "\n",
    "# 3) Utility for decoding predicted offsets -> [x1,y1,x2,y2]\n",
    "def pred_bbox_to_xywh(bbox_offsets, anchors):\n",
    "    \"\"\"\n",
    "    bbox_offsets: (N,4) predicted offsets [dy, dx, dh, dw]\n",
    "    anchors: (N,4) in [x1,y1,x2,y2]\n",
    "    return (N,4) boxes in [x1,y1,x2,y2]\n",
    "    \"\"\"\n",
    "    # Convert to numpy\n",
    "    anchors_np = anchors.detach().cpu().numpy()\n",
    "    bbox_np    = bbox_offsets.detach().cpu().numpy()\n",
    "\n",
    "    anc_w = anchors_np[:,2] - anchors_np[:,0]  # x2 - x1\n",
    "    anc_h = anchors_np[:,3] - anchors_np[:,1]  # y2 - y1\n",
    "    anc_ctr_x = anchors_np[:,0] + 0.5*anc_w\n",
    "    anc_ctr_y = anchors_np[:,1] + 0.5*anc_h\n",
    "\n",
    "    dy = bbox_np[:,0]\n",
    "    dx = bbox_np[:,1]\n",
    "    dh = bbox_np[:,2]\n",
    "    dw = bbox_np[:,3]\n",
    "\n",
    "    # decode\n",
    "    ctr_y = dy*anc_h + anc_ctr_y\n",
    "    ctr_x = dx*anc_w + anc_ctr_x\n",
    "    h = np.exp(dh)*anc_h\n",
    "    w = np.exp(dw)*anc_w\n",
    "\n",
    "    out = np.zeros_like(bbox_np, dtype=np.float32)\n",
    "    out[:,0] = ctr_x - 0.5*w  # x1\n",
    "    out[:,1] = ctr_y - 0.5*h  # y1\n",
    "    out[:,2] = ctr_x + 0.5*w  # x2\n",
    "    out[:,3] = ctr_y + 0.5*h  # y2\n",
    "    return out\n",
    "\n",
    "# 4) IoU with [x1,y1,x2,y2]\n",
    "def compute_iou_vectorized(boxes1, boxes2):\n",
    "    \"\"\"boxes1, boxes2 in [x1,y1,x2,y2]. Return IoU matrix.\"\"\"\n",
    "    boxes1 = boxes1.astype(np.float32)\n",
    "    boxes2 = boxes2.astype(np.float32)\n",
    "\n",
    "    inter_x1 = np.maximum(boxes1[:, None, 0], boxes2[None, :, 0])\n",
    "    inter_y1 = np.maximum(boxes1[:, None, 1], boxes2[None, :, 1])\n",
    "    inter_x2 = np.minimum(boxes1[:, None, 2], boxes2[None, :, 2])\n",
    "    inter_y2 = np.minimum(boxes1[:, None, 3], boxes2[None, :, 3])\n",
    "\n",
    "    inter_w = np.maximum(inter_x2 - inter_x1, 0)\n",
    "    inter_h = np.maximum(inter_y2 - inter_y1, 0)\n",
    "    inter_area = inter_w*inter_h\n",
    "\n",
    "    area1 = (boxes1[:,2] - boxes1[:,0])*(boxes1[:,3] - boxes1[:,1])\n",
    "    area2 = (boxes2[:,2] - boxes2[:,0])*(boxes2[:,3] - boxes2[:,1])\n",
    "    union = area1[:,None] + area2 - inter_area\n",
    "    iou = inter_area/union\n",
    "    return iou\n",
    "\n",
    "# 5) Show boxes in [x1,y1,x2,y2]\n",
    "def create_corner_rect(bb, color='red'):\n",
    "    x1,y1,x2,y2 = bb\n",
    "    return plt.Rectangle((x1,y1), x2 - x1, y2 - y1, color=color,\n",
    "                         fill=False, lw=2)\n",
    "\n",
    "def show_corner_bbs(img, bbs):\n",
    "    # Expect [x1,y1,x2,y2]\n",
    "    img_np = (img*255.0).clamp(0,255).byte().cpu().numpy()  # (C,H,W)\n",
    "    img_np = np.transpose(img_np, (1,2,0))  # (H,W,C)\n",
    "    plt.imshow(img_np)\n",
    "    for bb in bbs:\n",
    "        plt.gca().add_patch(create_corner_rect(bb))\n",
    "    plt.show()\n",
    "\n",
    "# 6) Example Anchor Generation in [x1,y1,x2,y2]\n",
    "def generate_anchor_grid_np(X_FM, Y_FM, ratios, scales):\n",
    "    \"\"\"\n",
    "    Return anchors as shape (N,4), each row [x1,y1,x2,y2].\n",
    "    \"\"\"\n",
    "    H_IMG, W_IMG = ISIZE[0], ISIZE[1]\n",
    "    sub_sampling_x = W_IMG / float(X_FM)\n",
    "    sub_sampling_y = H_IMG / float(Y_FM)\n",
    "\n",
    "    shift_x = np.arange(sub_sampling_x, (X_FM+1)*sub_sampling_x, sub_sampling_x)\n",
    "    shift_y = np.arange(sub_sampling_y, (Y_FM+1)*sub_sampling_y, sub_sampling_y)\n",
    "    shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n",
    "\n",
    "    centers = np.stack([\n",
    "        shift_x.ravel() - sub_sampling_x/2.0,\n",
    "        shift_y.ravel() - sub_sampling_y/2.0\n",
    "    ], axis=1)  # shape (total_positions,2) => (cx,cy)\n",
    "\n",
    "    anchors = []\n",
    "    for (cx, cy) in centers:\n",
    "        for ratio in ratios:\n",
    "            for scale in scales:\n",
    "                h = sub_sampling_y*scale*np.sqrt(ratio)\n",
    "                w = sub_sampling_x*scale*np.sqrt(1.0/ratio)\n",
    "                x1 = cx - 0.5*w\n",
    "                y1 = cy - 0.5*h\n",
    "                x2 = cx + 0.5*w\n",
    "                y2 = cy + 0.5*h\n",
    "                anchors.append([x1,y1,x2,y2])\n",
    "    return np.array(anchors, dtype=np.float32)\n",
    "\n",
    "# 7) The RPN with CBAM, in-channels=512\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, channels, reduction=4, kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channels, channels//reduction, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(channels//reduction, channels, bias=False)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.conv = nn.Conv2d(2,1,kernel_size,padding=kernel_size//2,bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b,c,h,w = x.shape\n",
    "        # Channel\n",
    "        y_avg = self.avg_pool(x).view(b,c)\n",
    "        y_max = self.max_pool(x).view(b,c)\n",
    "        y = self.fc(y_avg) + self.fc(y_max)\n",
    "        scale = self.sigmoid(y).view(b,c,1,1)\n",
    "        x = x*scale\n",
    "\n",
    "        # Spatial\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out,_ = torch.max(x, dim=1, keepdim=True)\n",
    "        y = torch.cat([avg_out,max_out], dim=1)\n",
    "        scale = self.sigmoid(self.conv(y))\n",
    "        return x*scale\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.ops as ops\n",
    "\n",
    "class ROIPooling(nn.Module):\n",
    "    def __init__(self, output_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            output_size (tuple or int): Size of the output (height, width)\n",
    "        \"\"\"\n",
    "        super(ROIPooling, self).__init__()\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def forward(self, features, rois):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features (Tensor): Input features of shape (N, C, H, W)\n",
    "            rois (Tensor): Regions of Interest in format\n",
    "                          [batch_index, x1, y1, x2, y2] with shape (K, 5)\n",
    "\n",
    "        Returns:\n",
    "            Pooled features of shape (K, C, output_size[0], output_size[1])\n",
    "        \"\"\"\n",
    "        # Ensure rois are on same device as features\n",
    "        rois = rois.to(features.device)\n",
    "\n",
    "        # Calculate spatial scale (feature map size / original image size)\n",
    "        spatial_scale_h = features.size(2) / 224  # Assuming input image size is 224\n",
    "        spatial_scale_w = features.size(3) / 224\n",
    "        spatial_scale = min(spatial_scale_h, spatial_scale_w)\n",
    "\n",
    "        # Perform ROI pooling\n",
    "        pooled_features = ops.roi_pool(\n",
    "            features,\n",
    "            rois,\n",
    "            output_size=self.output_size,\n",
    "            spatial_scale=spatial_scale\n",
    "        )\n",
    "\n",
    "        return pooled_features\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(output_size={})'.format(self.output_size)\n",
    "\n",
    "class EnhancedRPNWithROI(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels=512, \n",
    "                 mid_channels=256, \n",
    "                 n_anchor=15,  # must match len(ratios)*len(scales)\n",
    "                 pool_size=(7,7), \n",
    "                 nms_thresh=0.5,\n",
    "                 conf_thresh=0.5,\n",
    "                 top_n=400):\n",
    "        \"\"\"\n",
    "        Largely the same as your EnhancedRPN, but adds ROI pooling and a _process_proposals method.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # --- The same RPN body as your EnhancedRPN ---\n",
    "        self.conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(mid_channels, mid_channels, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(mid_channels, mid_channels, kernel_size=3, padding=1)\n",
    "\n",
    "        # If you want your CBAM modules:\n",
    "        self.cbam1 = CBAM(mid_channels, reduction=4, kernel_size=3)\n",
    "        self.cbam2 = CBAM(mid_channels, reduction=4, kernel_size=3)\n",
    "\n",
    "        self.reg_layer = nn.Conv2d(mid_channels, n_anchor*4, kernel_size=1)\n",
    "        self.cls_layer = nn.Conv2d(mid_channels, n_anchor*2, kernel_size=1)\n",
    "\n",
    "        self.skip_conv = nn.Conv2d(in_channels, mid_channels, kernel_size=1)\n",
    "        if in_channels == mid_channels:\n",
    "            nn.init.eye_(self.skip_conv.weight)\n",
    "            nn.init.zeros_(self.skip_conv.bias)\n",
    "            self.skip_conv.weight.requires_grad = False\n",
    "            self.skip_conv.bias.requires_grad = False\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "        # --- ROI pooling layer ---\n",
    "        #   You can use RoIPool or RoIAlign. \n",
    "        #   Below is ops.RoIPool, but if you want bilinear interpolation,\n",
    "        #   you might prefer RoIAlign(pooled_height=pool_size[0], pooled_width=pool_size[1], ...)\n",
    "        self.roi_pool = ops.RoIPool(output_size=pool_size, spatial_scale=1.0)\n",
    "\n",
    "        # Store thresholds for NMS, etc.\n",
    "        self.nms_thresh = nms_thresh\n",
    "        self.conf_thresh = conf_thresh\n",
    "        self.top_n = top_n\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for layer in [self.conv1, self.conv2, self.conv3, self.reg_layer, self.cls_layer]:\n",
    "            nn.init.normal_(layer.weight, std=0.01)\n",
    "            if layer.bias is not None:\n",
    "                nn.init.constant_(layer.bias, 0)\n",
    "\n",
    "    def forward(self, x, anchors=None, do_roi=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: feature map from the backbone, shape (B, C=512, H, W)\n",
    "            anchors: (total_anchors, 4) if you want to decode proposals\n",
    "            do_roi: bool. If True, we also do the ROI pooling step, returning pooled features.\n",
    "\n",
    "        Returns:\n",
    "            pred_locs: (B, #anchors, 4)\n",
    "            pred_scores: (B, #anchors, 2)\n",
    "            objectness_score: (B, #anchors)\n",
    "            (optionally) pooled_feats: if do_roi==True and anchors is not None\n",
    "        \"\"\"\n",
    "        residual = self.skip_conv(x)\n",
    "\n",
    "        # block1 + CBAM\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.cbam1(x)\n",
    "\n",
    "        # block2 + CBAM\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.cbam2(x)\n",
    "\n",
    "        # block3 + residual\n",
    "        x = F.relu(self.conv3(x) + residual)\n",
    "\n",
    "        # RPN heads\n",
    "        B = x.size(0)\n",
    "        pred_anchor_locs = self.reg_layer(x)      # shape (B, n_anchor*4, H, W)\n",
    "        pred_cls_scores  = self.cls_layer(x)      # shape (B, n_anchor*2, H, W)\n",
    "\n",
    "        pred_anchor_locs = pred_anchor_locs.permute(0,2,3,1).contiguous().view(B, -1, 4)\n",
    "        pred_cls_scores  = pred_cls_scores.permute(0,2,3,1).contiguous().view(B, -1, 2)\n",
    "\n",
    "        # optional tanh for loc\n",
    "        pred_anchor_locs = torch.tanh(pred_anchor_locs) * 2\n",
    "        objectness_score = F.softmax(pred_cls_scores, dim=-1)[..., 1]  # shape (B, #anchors)\n",
    "\n",
    "        if anchors is None or (not do_roi):\n",
    "            # If we don't want ROI pooling, just return the normal RPN outputs\n",
    "            return pred_anchor_locs, pred_cls_scores, objectness_score\n",
    "\n",
    "        # else we want proposals + ROI pooling\n",
    "        proposals = self._generate_proposals(pred_anchor_locs, anchors)  # shape (B, #anchors, 4)\n",
    "\n",
    "        # _process_proposals will do NMS, thresholding, and ROI pooling\n",
    "        pooled_feats = self._process_proposals(x, proposals, objectness_score)\n",
    "\n",
    "        return pred_anchor_locs, pred_cls_scores, objectness_score, pooled_feats\n",
    "\n",
    "    def _generate_proposals(self, pred_locs, anchors):\n",
    "        \"\"\"\n",
    "        Convert anchor offsets to box coords [x1, y1, x2, y2].\n",
    "        pred_locs: (B, N, 4) => offsets [dy, dx, dh, dw]\n",
    "        anchors:   (N, 4) in [x1, y1, x2, y2]\n",
    "        \"\"\"\n",
    "        B, N, _ = pred_locs.size()\n",
    "        proposals = torch.zeros_like(pred_locs)  # (B, N, 4)\n",
    "\n",
    "        # anchors => float on same device\n",
    "        anchors = anchors.to(pred_locs.device)\n",
    "\n",
    "        # anchor geometry\n",
    "        anc_w = anchors[:, 2] - anchors[:, 0]  # x2 - x1\n",
    "        anc_h = anchors[:, 3] - anchors[:, 1]  # y2 - y1\n",
    "        anc_ctr_x = anchors[:, 0] + 0.5*anc_w\n",
    "        anc_ctr_y = anchors[:, 1] + 0.5*anc_h\n",
    "\n",
    "        dy = pred_locs[..., 0]\n",
    "        dx = pred_locs[..., 1]\n",
    "        dh = pred_locs[..., 2]\n",
    "        dw = pred_locs[..., 3]\n",
    "\n",
    "        # decode\n",
    "        ctr_y = dy * anc_h[None, :] + anc_ctr_y[None, :]\n",
    "        ctr_x = dx * anc_w[None, :] + anc_ctr_x[None, :]\n",
    "        h = torch.exp(dh) * anc_h[None, :]\n",
    "        w = torch.exp(dw) * anc_w[None, :]\n",
    "\n",
    "        # final\n",
    "        proposals[..., 0] = ctr_x - 0.5*w\n",
    "        proposals[..., 1] = ctr_y - 0.5*h\n",
    "        proposals[..., 2] = ctr_x + 0.5*w\n",
    "        proposals[..., 3] = ctr_y + 0.5*h\n",
    "\n",
    "        return proposals\n",
    "\n",
    "    def _process_proposals(self, conv_features, proposals, scores):\n",
    "        \"\"\"\n",
    "        For each image in the batch:\n",
    "          - Filter proposals by self.conf_thresh\n",
    "          - NMS\n",
    "          - Keep top_n\n",
    "          - ROI Pool\n",
    "        Return pooled features.\n",
    "        \"\"\"\n",
    "        B = conv_features.size(0)\n",
    "        pooled_list = []\n",
    "\n",
    "        for b_idx in range(B):\n",
    "            cur_scores = scores[b_idx]      # shape (#anchors,)\n",
    "            cur_props = proposals[b_idx]    # shape (#anchors, 4)\n",
    "\n",
    "            # 1) Confidence threshold\n",
    "            conf_mask = cur_scores > self.conf_thresh\n",
    "            filtered_boxes  = cur_props[conf_mask]\n",
    "            filtered_scores = cur_scores[conf_mask]\n",
    "\n",
    "            if filtered_boxes.size(0) == 0:\n",
    "                # no proposals left\n",
    "                pooled_list.append(torch.empty(0, device=conv_features.device))\n",
    "                continue\n",
    "\n",
    "            # 2) NMS\n",
    "            keep_idx = ops.nms(filtered_boxes, filtered_scores, self.nms_thresh)\n",
    "            keep_idx = keep_idx[:self.top_n]  # top top_n after NMS\n",
    "            final_boxes = filtered_boxes[keep_idx]\n",
    "\n",
    "            # 3) ROI Pool\n",
    "            # Format => [batch_ind, x1, y1, x2, y2]\n",
    "            roi_input = torch.cat([\n",
    "                torch.full((final_boxes.size(0),1), b_idx, device=conv_features.device, dtype=torch.float32),\n",
    "                final_boxes\n",
    "            ], dim=1)\n",
    "\n",
    "            # shape => (N_proposals, C, pool_size[0], pool_size[1])\n",
    "            pooled = self.roi_pool(conv_features, roi_input)\n",
    "            pooled_list.append(pooled)\n",
    "\n",
    "        # Combine into a single tensor if you want\n",
    "        pooled_feats = torch.cat(pooled_list, dim=0)\n",
    "        return pooled_feats\n",
    "\n",
    "\n",
    "\n",
    "##################################\n",
    "\n",
    "def compute_iou_torch(boxes1, boxes2):\n",
    "    \"\"\"\n",
    "    Compute IoU between two sets of boxes (PyTorch).\n",
    "    boxes1: (N,4)  [y1, x1, y2, x2]\n",
    "    boxes2: (M,4)\n",
    "    Returns an (N, M) tensor of IoU values.\n",
    "    \"\"\"\n",
    "    inter_y1 = torch.max(boxes1[:, None, 0], boxes2[:, 0])  # (N, M)\n",
    "    inter_x1 = torch.max(boxes1[:, None, 1], boxes2[:, 1])\n",
    "    inter_y2 = torch.min(boxes1[:, None, 2], boxes2[:, 2])\n",
    "    inter_x2 = torch.min(boxes1[:, None, 3], boxes2[:, 3])\n",
    "\n",
    "    inter_h = (inter_y2 - inter_y1).clamp(min=0)\n",
    "    inter_w = (inter_x2 - inter_x1).clamp(min=0)\n",
    "    inter_area = inter_h * inter_w\n",
    "\n",
    "    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])  # shape [N]\n",
    "    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])  # shape [M]\n",
    "    union_area = area1[:, None] + area2 - inter_area\n",
    "\n",
    "    iou = inter_area / union_area\n",
    "    return iou\n",
    "\n",
    "def hierarchical_sample_anchors(all_anchor_idxs, anchors, num_to_sample=256):\n",
    "    \"\"\"\n",
    "    Given a set of anchor indices (e.g. pos or neg) and the anchor boxes themselves,\n",
    "    split them by size (small/medium/large) and sample from each group equally.\n",
    "\n",
    "    Inputs:\n",
    "      all_anchor_idxs: 1D tensor of indices into 'anchors' that we want to sample from.\n",
    "      anchors: (N, 4) the full anchor set as a torch.Tensor.\n",
    "      num_to_sample: how many total anchors we want.\n",
    "\n",
    "    Returns:\n",
    "      chosen_idxs: the indices (subset of all_anchor_idxs) that we keep.\n",
    "    \"\"\"\n",
    "    # Subset just the anchors we care about\n",
    "    subset_anchors = anchors[all_anchor_idxs]  # shape: (K, 4)\n",
    "    # Compute sizes = (y2-y1)*(x2-x1)\n",
    "    sizes = (subset_anchors[:, 2] - subset_anchors[:, 0]) * (subset_anchors[:, 3] - subset_anchors[:, 1])\n",
    "\n",
    "    # Percentile thresholds for small/medium/large\n",
    "    small_thresh = torch.quantile(sizes, 0.33)\n",
    "    large_thresh = torch.quantile(sizes, 0.66)\n",
    "\n",
    "    small_mask = sizes <= small_thresh\n",
    "    large_mask = sizes > large_thresh\n",
    "    medium_mask = (~small_mask) & (~large_mask)\n",
    "\n",
    "    # Split into three groups\n",
    "    small_idxs  = all_anchor_idxs[ small_mask ]\n",
    "    medium_idxs = all_anchor_idxs[ medium_mask ]\n",
    "    large_idxs  = all_anchor_idxs[ large_mask ]\n",
    "    per_group = num_to_sample // 3\n",
    "\n",
    "    # If a group doesn’t have enough anchors, sample with replacement or clamp\n",
    "    chosen_small  = sample_with_clamp(small_idxs,  per_group)\n",
    "    chosen_medium = sample_with_clamp(medium_idxs, per_group)\n",
    "    chosen_large  = sample_with_clamp(large_idxs,  per_group)\n",
    "\n",
    "    chosen_idxs = torch.cat([chosen_small, chosen_medium, chosen_large], dim=0)\n",
    "    return chosen_idxs\n",
    "\n",
    "\n",
    "def sample_with_clamp(indices, num_needed):\n",
    "    \"\"\"\n",
    "    Randomly choose 'num_needed' items from 'indices' (1D tensor).\n",
    "    If we have fewer than num_needed, sample with replacement to keep it simple.\n",
    "    \"\"\"\n",
    "    if len(indices) == 0:\n",
    "        # If no anchors in that category, return empty\n",
    "        return indices\n",
    "    if len(indices) >= num_needed:\n",
    "        rand = torch.randperm(len(indices))[:num_needed]\n",
    "        return indices[rand]\n",
    "    else:\n",
    "        # sample with replacement\n",
    "        extra = num_needed - len(indices)\n",
    "        rand_base = torch.randperm(len(indices))\n",
    "        # fill up\n",
    "        chosen = torch.cat([indices, indices[rand_base[:extra]]], dim=0)\n",
    "        return chosen\n",
    "\n",
    "####################\n",
    "\n",
    "def create_ground_truth_rect(bb, color='blue'):\n",
    "    # Now handles [x1,y1,x2,y2]:\n",
    "    bb = np.array(bb, dtype=np.float32)\n",
    "    x1, y1, x2, y2 = bb\n",
    "    return plt.Rectangle(\n",
    "        (x1, y1),            # top-left corner\n",
    "        x2 - x1,             # width\n",
    "        y2 - y1,             # height\n",
    "        color=color, fill=False, lw=3\n",
    "    )\n",
    "\n",
    "def show_ground_truth_bbs(im, bbs):\n",
    "    im_np = unnormalize_tensor(im)\n",
    "    plt.imshow(np.transpose(im_np, (1, 2, 0)))\n",
    "    for bb in bbs:\n",
    "        plt.gca().add_patch(create_ground_truth_rect(bb))\n",
    "    plt.show()\n",
    "\n",
    "############################\n",
    "\n",
    "\n",
    "def bbox_generation(images, targets, X_FM, Y_FM):\n",
    "    B = len(images)\n",
    "    C, H_IMG, W_IMG = images[0].shape  # (C, H, W)\n",
    "    \n",
    "    total_positions = X_FM * Y_FM\n",
    "    num_anchors_per_pos = len(ratios) * len(anchor_scales)\n",
    "    total_anchors = total_positions * num_anchors_per_pos\n",
    "\n",
    "    # Strides\n",
    "    stride_x = float(W_IMG)/X_FM\n",
    "    stride_y = float(H_IMG)/Y_FM\n",
    "\n",
    "    # Make grid centers\n",
    "    shift_x = np.arange(stride_x, (X_FM+1)*stride_x, stride_x)\n",
    "    shift_y = np.arange(stride_y, (Y_FM+1)*stride_y, stride_y)\n",
    "    shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n",
    "    centers = np.stack([\n",
    "        shift_x.ravel() - stride_x/2.0,\n",
    "        shift_y.ravel() - stride_y/2.0\n",
    "    ], axis=1)  # shape (total_positions,2) => (cx, cy)\n",
    "\n",
    "    # Build anchors in [x1,y1,x2,y2]\n",
    "    anchors_list = []\n",
    "    for (cx, cy) in centers:\n",
    "        for ratio in ratios:\n",
    "            for scale in anchor_scales:\n",
    "                h = stride_y*scale*np.sqrt(ratio)\n",
    "                w = stride_x*scale*np.sqrt(1.0/ratio)\n",
    "                x1 = cx - 0.5*w\n",
    "                y1 = cy - 0.5*h\n",
    "                x2 = cx + 0.5*w\n",
    "                y2 = cy + 0.5*h\n",
    "                anchors_list.append([x1, y1, x2, y2])\n",
    "    anchors = np.array(anchors_list, dtype=np.float32)\n",
    "\n",
    "    # Valid anchors fully in the image\n",
    "    valid_idx = np.where(\n",
    "        (anchors[:,0] >= 0) & (anchors[:,1] >= 0) &\n",
    "        (anchors[:,2] <= W_IMG) & (anchors[:,3] <= H_IMG)\n",
    "\n",
    "    )[0]\n",
    "\n",
    "    # For each image in the batch, label anchors, etc.\n",
    "    anchor_locs_all, anchor_labels_all = [], []\n",
    "    pos_iou_th = 0.7\n",
    "    neg_iou_th = 0.3\n",
    "    n_sample = 256\n",
    "    pos_ratio = 0.5\n",
    "\n",
    "    for i in range(B):\n",
    "        labels = -1*np.ones((total_anchors,), dtype=np.int32)\n",
    "        locs   = np.zeros((total_anchors, 4), dtype=np.float32)\n",
    "\n",
    "        gt_boxes = targets[i][\"boxes\"].cpu().numpy()  # shape (M,4) in [x1,y1,x2,y2]\n",
    "        if gt_boxes.shape[0] > 0:\n",
    "            # IoU only among valid anchors\n",
    "            valid_anchors = anchors[valid_idx]\n",
    "            ious = compute_iou_vectorized(valid_anchors, gt_boxes)  # shape [N_valid, M]\n",
    "            max_ious  = np.max(ious, axis=1)\n",
    "            argmax_ious = np.argmax(ious, axis=1)\n",
    "\n",
    "            # Label: pos => iou>=0.7, neg => iou<0.3\n",
    "            valid_labels = -1*np.ones_like(max_ious, dtype=np.int32)\n",
    "            valid_labels[max_ious >= pos_iou_th] = 1\n",
    "            valid_labels[max_ious <  neg_iou_th] = 0\n",
    "\n",
    "            # Force each gt box to have at least one positive anchor\n",
    "            gt_max_ious = np.max(ious, axis=0)\n",
    "            for j, g_iou in enumerate(gt_max_ious):\n",
    "                idxs = np.where(ious[:,j] == g_iou)[0]\n",
    "                valid_labels[idxs] = 1\n",
    "\n",
    "            # Subsample\n",
    "            pos_inds = np.where(valid_labels == 1)[0]\n",
    "            neg_inds = np.where(valid_labels == 0)[0]\n",
    "\n",
    "            num_pos = int(n_sample*pos_ratio)\n",
    "            if len(pos_inds) > num_pos:\n",
    "                disable_pos = np.random.choice(pos_inds, size=len(pos_inds)-num_pos, replace=False)\n",
    "                valid_labels[disable_pos] = -1\n",
    "\n",
    "            num_neg = n_sample - np.sum(valid_labels==1)\n",
    "            if len(neg_inds) > num_neg:\n",
    "                disable_neg = np.random.choice(neg_inds, size=len(neg_inds)-num_neg, replace=False)\n",
    "                valid_labels[disable_neg] = -1\n",
    "\n",
    "            # Compute loc for positives\n",
    "            valid_locs = np.zeros((valid_anchors.shape[0], 4), dtype=np.float32)\n",
    "            pos_final = np.where(valid_labels == 1)[0]\n",
    "            if len(pos_final)>0:\n",
    "                posA = valid_anchors[pos_final]\n",
    "                # anchor center\n",
    "                anc_w = posA[:,2] - posA[:,0]\n",
    "                anc_h = posA[:,3] - posA[:,1]\n",
    "                anc_ctr_x = posA[:,0] + 0.5*anc_w\n",
    "                anc_ctr_y = posA[:,1] + 0.5*anc_h\n",
    "\n",
    "                # matched GT\n",
    "                match_gt = gt_boxes[argmax_ious[pos_final]]\n",
    "                gt_w = match_gt[:,2] - match_gt[:,0]\n",
    "                gt_h = match_gt[:,3] - match_gt[:,1]\n",
    "                gt_ctr_x = match_gt[:,0] + 0.5*gt_w\n",
    "                gt_ctr_y = match_gt[:,1] + 0.5*gt_h\n",
    "\n",
    "                dx = (gt_ctr_x - anc_ctr_x)/anc_w\n",
    "                dy = (gt_ctr_y - anc_ctr_y)/anc_h\n",
    "                dw = np.log(gt_w/anc_w)\n",
    "                dh = np.log(gt_h/anc_h)\n",
    "                valid_locs[pos_final] = np.stack([dy, dx, dh, dw], axis=1)\n",
    "\n",
    "            # Fill in the big array\n",
    "            labels[valid_idx] = valid_labels\n",
    "            locs[valid_idx]   = valid_locs\n",
    "\n",
    "        anchor_labels_all.append(labels)\n",
    "        anchor_locs_all.append(locs)\n",
    "\n",
    "    anchor_labels_all = np.stack(anchor_labels_all, axis=0)  # (B, total_anchors)\n",
    "    anchor_locs_all   = np.stack(anchor_locs_all,   axis=0)  # (B, total_anchors, 4)\n",
    "\n",
    "    return anchor_locs_all, anchor_labels_all, anchors  # anchors in [x1,y1,x2,y2]\n",
    "\n",
    "#############################\n",
    "\n",
    "def visualize_attention(feat, title=\"\"):\n",
    "    \"\"\"Visualize the attention maps from CBAM\"\"\"\n",
    "    plt.figure(figsize=(10,5))\n",
    "    # Channel attention\n",
    "    avg_pool = torch.mean(feat, dim=1, keepdim=True)\n",
    "    max_pool, _ = torch.max(feat, dim=1, keepdim=True)\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(avg_pool[0].cpu().detach().numpy(), cmap='jet')\n",
    "    plt.title(f\"{title} - Channel Avg\")\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(max_pool[0].cpu().detach().numpy(), cmap='jet')\n",
    "    plt.title(f\"{title} - Channel Max\")\n",
    "    plt.show()\n",
    "\n",
    "################################\n",
    "rpn_model = EnhancedRPNWithROI(in_channels=512, mid_channels=256).to(device)\n",
    "\n",
    "vgg_model = torchvision.models.vgg16(pretrained=True).to(device)\n",
    "vgg_model.eval()\n",
    "for param in vgg_model.features.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "base_lr = 0.001\n",
    "\n",
    "backbone_params = list(vgg_model.features.parameters())\n",
    "\n",
    "new_params = list(rpn_model.parameters())\n",
    "\n",
    "req_features = [layer for layer in list(vgg_model.features)[:30]]\n",
    "\n",
    "\n",
    "# optimizer = torch.optim.Adam(rpn_model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': backbone_params, 'lr': base_lr * 0.1},\n",
    "    {'params': new_params, 'lr': base_lr}\n",
    "])\n",
    "\n",
    "###################################\n",
    "\n",
    "def train_epochs(req_features, rpn_model, optimizer, train_dl,\n",
    "                 epochs=20, rpn_lambda=10, iou_threshold=0.2, top_k=400):\n",
    "    \"\"\"\n",
    "    Integrated hierarchical sampling in place of random anchor subsampling.\n",
    "    \"\"\"\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # rpn_model.load_state_dict(torch.load(\"./cbma_final.pth\", map_location=device))\n",
    "    rpn_model.load_state_dict(torch.load(\"./1206.pth\", map_location=device))\n",
    "    rpn_model.train()\n",
    "    rpn_model.to(device)\n",
    "\n",
    "    epoch_train_recalls = []\n",
    "    epoch_train_errors = []\n",
    "    \n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        total_samples = 0\n",
    "        sum_loss = 0.0\n",
    "        sum_loss_cls = 0.0\n",
    "        sum_loss_loc = 0.0\n",
    "        batch_recalls = []\n",
    "\n",
    "        for batch in train_dl:\n",
    "            images = torch.stack([img.to(device) for img in batch[\"images\"]])\n",
    "            targets = [{\"boxes\": b.to(device), \"labels\": l.to(device)}\n",
    "                       for b, l in zip(batch[\"boxes\"], batch[\"labels\"])]\n",
    "\n",
    "            B = images.size(0)\n",
    "            total_samples += B\n",
    "\n",
    "            # ----- 1) Frozen backbone features -----\n",
    "            with torch.no_grad():\n",
    "                feat = images\n",
    "                for m in req_features:\n",
    "                    m.to(device)\n",
    "                    feat = m(feat)\n",
    "            X_FM, Y_FM = feat.shape[2], feat.shape[3]\n",
    "\n",
    "            # ----- 2) Generate all anchors (like bbox_generation but no labeling) -----\n",
    "            anchors = generate_anchor_grid_np(X_FM, Y_FM, ratios, anchor_scales)  \n",
    "            # shape = (total_anchors, 4) in NumPy\n",
    "            anchors = torch.from_numpy(anchors).float().to(device)\n",
    "\n",
    "            # Indices of anchors inside the image\n",
    "            H_IMG, W_IMG = images.shape[2], images.shape[3]\n",
    "            valid_idx = torch.where(\n",
    "                (anchors[:,0] >= 0) &            # x1 >= 0\n",
    "                (anchors[:,2] <= W_IMG) &        # x2 <= width\n",
    "                (anchors[:,1] >= 0) &            # y1 >= 0\n",
    "                (anchors[:,3] <= H_IMG)          # y2 <= height\n",
    "            )[0]\n",
    "\n",
    "            # Prepare big tensors for labels/locs across the batch\n",
    "            # We'll label only the anchors in valid_idx, but keep the shape the same length as 'anchors'\n",
    "            anchor_labels_all = -1 * torch.ones((B, anchors.size(0)), dtype=torch.int32, device=device)\n",
    "            anchor_locs_all   = torch.zeros((B, anchors.size(0), 4),  dtype=torch.float32, device=device)\n",
    "\n",
    "            # ----- 3) Per-image anchor labeling & hierarchical sampling -----\n",
    "            pos_iou_threshold = 0.7\n",
    "            neg_iou_threshold = 0.3\n",
    "            n_sample  = 256    # total anchors to keep\n",
    "            pos_ratio = 0.5\n",
    "\n",
    "            for i in range(B):\n",
    "                gt_boxes_i = targets[i][\"boxes\"]   # shape (M,4)\n",
    "                if gt_boxes_i.numel() == 0:\n",
    "                    # No ground truth => skip\n",
    "                    continue\n",
    "\n",
    "                # 3a) IoU labeling for the valid anchors\n",
    "                valid_anchors = anchors[valid_idx]\n",
    "                ious = compute_iou_torch(valid_anchors, gt_boxes_i)  # shape (N_valid, M)\n",
    "\n",
    "                max_ious, argmax_ious = ious.max(dim=1)   # best GT for each anchor\n",
    "                valid_labels = -1 * torch.ones_like(max_ious, dtype=torch.int32)\n",
    "\n",
    "                # Positive > pos_iou_threshold\n",
    "                valid_labels[max_ious >= pos_iou_threshold] = 1\n",
    "                # Negative < neg_iou_threshold\n",
    "                valid_labels[max_ious < neg_iou_threshold]  = 0\n",
    "\n",
    "                # Ensure each GT box has at least one anchor\n",
    "                gt_max_ious, gt_argmax = ious.max(dim=0)  # best anchor for each GT\n",
    "                for j in range(gt_boxes_i.size(0)):\n",
    "                    best_anchor_for_gt = (ious[:, j] == gt_max_ious[j]).nonzero(as_tuple=True)[0]\n",
    "                    valid_labels[best_anchor_for_gt] = 1\n",
    "\n",
    "                # pos_count = (valid_labels == 1).sum().item()\n",
    "                # print(f\"Found {pos_count} positives in image {i}\")\n",
    "\n",
    "                # 3b) Separate pos vs neg anchor indices\n",
    "                pos_inds = torch.where(valid_labels == 1)[0]\n",
    "                neg_inds = torch.where(valid_labels == 0)[0]\n",
    "\n",
    "                # 3c) Hierarchical sampling: sample some positives, some negatives\n",
    "                # We'll try to keep up to pos_ratio*n_sample as positives, remainder as negatives.\n",
    "                # So let's compute how many positives we want\n",
    "                max_pos = int(pos_ratio * n_sample)\n",
    "                n_pos = min(len(pos_inds), max_pos)\n",
    "                n_neg = n_sample - n_pos\n",
    "\n",
    "                # (i) sample from pos_inds by size\n",
    "                chosen_pos = hierarchical_sample_anchors(pos_inds, anchors[valid_idx], num_to_sample=n_pos)\n",
    "                # (ii) sample from neg_inds by size\n",
    "                chosen_neg = hierarchical_sample_anchors(neg_inds, anchors[valid_idx], num_to_sample=n_neg)\n",
    "\n",
    "                # anything not chosen => label = -1\n",
    "                chosen_mask = torch.zeros_like(valid_labels, dtype=torch.bool)\n",
    "                chosen_mask[chosen_pos] = True\n",
    "                chosen_mask[chosen_neg] = True\n",
    "\n",
    "                valid_labels[~chosen_mask] = -1\n",
    "\n",
    "                # 3d) Compute regression targets for the chosen positives\n",
    "                valid_locs = torch.zeros((valid_anchors.size(0), 4), dtype=torch.float32, device=device)\n",
    "                pos_chosen_mask = (valid_labels == 1)\n",
    "                if pos_chosen_mask.sum() > 0:\n",
    "                    pos_anchors = valid_anchors[pos_chosen_mask]\n",
    "                    assigned_gt = gt_boxes_i[argmax_ious[pos_chosen_mask]]  # matched GT\n",
    "\n",
    "                    # Convert anchor + GT to center/width/height\n",
    "                    anc_h = pos_anchors[:,2] - pos_anchors[:,0]\n",
    "                    anc_w = pos_anchors[:,3] - pos_anchors[:,1]\n",
    "                    anc_ctr_y = pos_anchors[:,0] + 0.5*anc_h\n",
    "                    anc_ctr_x = pos_anchors[:,1] + 0.5*anc_w\n",
    "\n",
    "                    gt_h = assigned_gt[:,2] - assigned_gt[:,0]\n",
    "                    gt_w = assigned_gt[:,3] - assigned_gt[:,1]\n",
    "                    gt_ctr_y = assigned_gt[:,0] + 0.5*gt_h\n",
    "                    gt_ctr_x = assigned_gt[:,1] + 0.5*gt_w\n",
    "\n",
    "                    dy = (gt_ctr_y - anc_ctr_y) / anc_h\n",
    "                    dx = (gt_ctr_x - anc_ctr_x) / anc_w\n",
    "                    dh = torch.log(gt_h / anc_h)\n",
    "                    dw = torch.log(gt_w / anc_w)\n",
    "\n",
    "                    valid_locs[pos_chosen_mask] = torch.stack([dy, dx, dh, dw], dim=1)\n",
    "\n",
    "                # 3e) Put back into the full-size arrays\n",
    "                anchor_labels_all[i, valid_idx] = valid_labels\n",
    "                anchor_locs_all[i, valid_idx]   = valid_locs\n",
    "\n",
    "            # Convert final label/loc arrays to float for loss\n",
    "            gt_scores = anchor_labels_all.to(torch.float32)  # (B, total_anchors)\n",
    "            gt_locs   = anchor_locs_all                      # (B, total_anchors, 4)\n",
    "\n",
    "            # ----- 4) Forward pass through RPN -----\n",
    "            pred_locs, pred_scores, objectness_score, pooled_feats = rpn_model(\n",
    "                feat, \n",
    "                anchors=anchors, \n",
    "                do_roi=True\n",
    "            )\n",
    "            # pred_locs: (B, all_anchors_in_featuremap, 4)\n",
    "            # pred_scores: (B, all_anchors_in_featuremap, 2)\n",
    "\n",
    "            # print(\"pred_scores shape:\", pred_scores.shape)  # expecting (B, A, 2)\n",
    "            # print(\"gt_scores shape:\", gt_scores.shape)      # expecting (B, A)\n",
    "\n",
    "            # print(\"After flatten:\")\n",
    "            # print(\"pred_scores.view(-1,2) ->\", pred_scores.view(-1,2).shape)\n",
    "            # print(\"gt_scores.view(-1)    ->\", gt_scores.view(-1).shape)\n",
    "\n",
    "            # Classification loss (ignore label = -1)\n",
    "            cls_loss = F.cross_entropy(\n",
    "                pred_scores.view(-1, 2),\n",
    "                gt_scores.view(-1).long(),\n",
    "                ignore_index=-1\n",
    "            )\n",
    "\n",
    "            # Smooth L1 for positives\n",
    "            pos_mask = (gt_scores == 1)\n",
    "            if pos_mask.sum() > 0:\n",
    "                pred_pos = pred_locs[pos_mask]\n",
    "                gt_pos   = gt_locs[pos_mask]\n",
    "                diff = torch.abs(gt_pos - pred_pos)\n",
    "                loc_loss = torch.where(diff < 1, 0.5 * diff**2, diff - 0.5).sum()\n",
    "                loc_loss = loc_loss / pos_mask.sum().float()\n",
    "            else:\n",
    "                loc_loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "            loss = cls_loss + rpn_lambda * loc_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            sum_loss     += loss.item()\n",
    "            sum_loss_cls += cls_loss.item()\n",
    "            sum_loss_loc += (rpn_lambda * loc_loss).item()\n",
    "\n",
    "            # ----- 5) Measure recall in this batch -----\n",
    "            with torch.no_grad():\n",
    "                batch_recall = 0.0\n",
    "                count = 0\n",
    "                for i in range(B):\n",
    "                    # Convert predicted anchor offsets back to box coords\n",
    "                    rois = pred_bbox_to_xywh(pred_locs[i], anchors)  # anchors shape (total_anchors, 4)\n",
    "\n",
    "                    # Top-k proposals\n",
    "                    k = min(top_k, objectness_score[i].shape[0])\n",
    "                    topk_inds = torch.topk(objectness_score[i], k=k).indices\n",
    "                    proposals = rois[topk_inds.cpu().numpy()]\n",
    "\n",
    "                    gt_boxes = batch[\"boxes\"][i].cpu().numpy()  # shape (M,4) in [y1,x1,y2,x2] or [x1,y1,x2,y2]?\n",
    "\n",
    "                    if len(gt_boxes) > 0:\n",
    "                        matched = 0\n",
    "                        for gt in gt_boxes:\n",
    "                            # If your GT is [y1,x1,y2,x2], do nothing\n",
    "                            # If your GT is [x1,y1,x2,y2], you need to reorder\n",
    "                            # Example if your code expects [y1,x1,y2,x2]:\n",
    "                            # gt_converted = np.array([gt[1], gt[0], gt[3], gt[2]])\n",
    "                            # but adapt to your dataset as needed\n",
    "                            gt_converted = gt  \n",
    "                            ious = compute_iou_vectorized(proposals, np.expand_dims(gt_converted, 0))\n",
    "                            best_iou = np.max(ious) if ious.size > 0 else 0.0\n",
    "                            if best_iou >= iou_threshold:\n",
    "                                matched += 1\n",
    "                        recall = matched / len(gt_boxes)\n",
    "                        batch_recall += recall\n",
    "                        count += 1\n",
    "\n",
    "                if count > 0:\n",
    "                    batch_recalls.append(batch_recall / count)\n",
    "\n",
    "        # End of epoch\n",
    "        epoch_recall = np.mean(batch_recalls) if batch_recalls else 0.0\n",
    "        epoch_train_recalls.append(epoch_recall)\n",
    "        epoch_train_errors.append(1 - epoch_recall)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Loss {sum_loss/total_samples:.3f} | \"\n",
    "              f\"Recall: {epoch_recall:.3f} | Error: {1-epoch_recall:.3f}\")\n",
    "\n",
    "        if (epoch+1) % 5 == 0:\n",
    "            torch.save(rpn_model.state_dict(), f\"./cecilia_{epoch+1}.pth\")\n",
    "\n",
    "    torch.save(rpn_model.state_dict(), \"./1206.pth\")\n",
    "    # ---- (Optional) Plot recall/error over epochs ----\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, epochs+1), epoch_train_recalls, 'b-o', label='Recall')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.title('Training Recall Over Epochs')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, epochs+1), epoch_train_errors, 'r-o', label='Error')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Error (1 - Recall)')\n",
    "    plt.title('Training Error Over Epochs')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # torch.save(rpn_model.state_dict(), \"./cbma_final_more_anchor.pth\")\n",
    "    return rpn_model\n",
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_dir = \"C:/360/trainA_origin_700/trainA_original_700\"\n",
    "pt_dir = 'C:/360/trainA_testing2'\n",
    "json_file_path =  \"C:/360/bdd100k_labels_images_train.json\"\n",
    "# Extract labels from JSON (adjust number as desired)\n",
    "all_labels = extract_first_n_labels(json_file_path, 20000)\n",
    "\n",
    "# Create the custom dataset using your method\n",
    "dataset = CustomDataset(image_dir, all_labels, pt_dir)\n",
    "\n",
    "# Split using random_split (70% train, 15% val, 15% test)\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "print(f\"Testing dataset size: {len(test_dataset)}\")\n",
    "\n",
    "# Create DataLoaders using your custom collate function\n",
    "batch_size = 8\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn, num_workers=0)\n",
    "val_loader   = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou_vectorized_yxyx(boxes1, boxes2):\n",
    "    \"\"\"\n",
    "    Compute IoU between boxes1 (N,4) and boxes2 (M,4) in y1,x1,y2,x2 format.\n",
    "    Returns IoU matrix of shape (N, M).\n",
    "    \"\"\"\n",
    "    # Convert to numpy if they're tensors\n",
    "    if isinstance(boxes1, torch.Tensor):\n",
    "        boxes1 = boxes1.cpu().numpy()\n",
    "    if isinstance(boxes2, torch.Tensor):\n",
    "        boxes2 = boxes2.cpu().numpy()\n",
    "\n",
    "    inter_y1 = np.maximum(boxes1[:, None, 0], boxes2[None, :, 0])\n",
    "    inter_x1 = np.maximum(boxes1[:, None, 1], boxes2[None, :, 1])\n",
    "    inter_y2 = np.minimum(boxes1[:, None, 2], boxes2[None, :, 2])\n",
    "    inter_x2 = np.minimum(boxes1[:, None, 3], boxes2[None, :, 3])\n",
    "\n",
    "    inter_h = np.maximum(inter_y2 - inter_y1, 0)\n",
    "    inter_w = np.maximum(inter_x2 - inter_x1, 0)\n",
    "    inter_area = inter_h * inter_w\n",
    "\n",
    "    area1 = (boxes1[:,2]-boxes1[:,0])*(boxes1[:,3]-boxes1[:,1])\n",
    "    area2 = (boxes2[:,2]-boxes2[:,0])*(boxes2[:,3]-boxes2[:,1])\n",
    "\n",
    "    union = area1[:,None] + area2[None,:] - inter_area\n",
    "    iou = inter_area / (union + 1e-6)\n",
    "    return iou\n",
    "\n",
    "\n",
    "def compute_recall_at_threshold(proposals, gt_boxes, iou_thresh=0.3):\n",
    "    \"\"\"\n",
    "    Compute recall at a fixed IoU threshold.\n",
    "\n",
    "    For each GT box, if the best IoU among proposals is >= iou_thresh, count it as a match.\n",
    "    Recall = (number of matched GT boxes) / (total GT boxes)\n",
    "    \"\"\"\n",
    "    if len(gt_boxes) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # Compute IoU between proposals and ground truth boxes\n",
    "    ious = compute_iou_vectorized_yxyx(proposals, gt_boxes)  # shape (N, M)\n",
    "    best_ious = ious.max(axis=0)  # best IoU for each GT\n",
    "    recall = np.sum(best_ious >= iou_thresh) / float(len(gt_boxes))\n",
    "    return recall\n",
    "\n",
    "\n",
    "\n",
    "def compute_mean_best_iou(proposals, gt_boxes):\n",
    "    \"\"\"\n",
    "    Compute the mean best IoU across all GT boxes.\n",
    "\n",
    "    For each GT box, we take the average of the first 5 highest IoU achieved by any proposal.\n",
    "    \"\"\"\n",
    "    if len(gt_boxes) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    ious = compute_iou_vectorized_yxyx(proposals, gt_boxes)  # shape (N, M)\n",
    "    best_ious = ious.max(axis=0)\n",
    "    return np.mean(best_ious)\n",
    "\n",
    "\n",
    "def compute_proposal_density(proposals, gt_boxes, iou_thresh=0.5):\n",
    "    \"\"\"\n",
    "    Compute the average number of qualifying proposals PER GT BOX.\n",
    "    Should return values >= 0 (typically >> 1 for good RPN performance).\n",
    "    \"\"\"\n",
    "    if len(gt_boxes) == 0 or len(proposals) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # Ensure numpy arrays\n",
    "    proposals = np.asarray(proposals, dtype=np.float32)\n",
    "    gt_boxes = np.asarray(gt_boxes, dtype=np.float32)\n",
    "\n",
    "    # Verify box format: (N,4) where columns are [y1,x1,y2,x2]\n",
    "    assert proposals.shape[1] == 4 and gt_boxes.shape[1] == 4\n",
    "\n",
    "    # Compute IoU matrix (N proposals x M GT boxes)\n",
    "    ious = compute_iou_vectorized_yxyx(proposals, gt_boxes)\n",
    "\n",
    "    # Count qualifying proposals PER GT BOX (sum across axis=0)\n",
    "    qual_counts = (ious >= iou_thresh).sum(axis=0)\n",
    "\n",
    "    # Return average count per GT box\n",
    "    return float(np.mean(qual_counts))\n",
    "\n",
    "def plot_training_metrics(train_losses, val_metrics, eval_every):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    # Plot training loss\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot validation metrics if available\n",
    "    if len(val_metrics) > 0:\n",
    "        # Get x-axis positions for validation points\n",
    "        val_epochs = [(i+1)*eval_every for i in range(len(val_metrics))]\n",
    "\n",
    "        # Plot Average Recall\n",
    "        plt.subplot(1, 3, 2)\n",
    "        ar_values = [np.mean(m['recall'])  for m in val_metrics]\n",
    "        plt.plot(val_epochs, ar_values, 'o-', label='Avg Recall')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Average Recall')\n",
    "        plt.title('Validation Recall')\n",
    "        plt.ylim(0, 1)\n",
    "        plt.legend()\n",
    "\n",
    "        # Plot Mean IoU\n",
    "        plt.subplot(1, 3, 3)\n",
    "        iou_values = [np.mean(m['mean_best_iou']) for m in val_metrics]\n",
    "        plt.plot(val_epochs, iou_values, 'o-', label='Mean IoU')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Mean Best IoU')\n",
    "        plt.title('Localization Quality')\n",
    "        plt.ylim(0, 1)\n",
    "        plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def decode_boxes(anchors, pred_locs):\n",
    "    \"\"\"\n",
    "    Decode regression offsets (pred_locs) to actual box coordinates based on anchors.\n",
    "    This function assumes that the offsets are in the form [dy, dx, dh, dw].\n",
    "\n",
    "    Args:\n",
    "        anchors (Tensor): Anchor boxes of shape (N, 4) in [y1, x1, y2, x2] format.\n",
    "        pred_locs (Tensor): Regression offsets of shape (N, 4).\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Decoded boxes of shape (N, 4) in [y1, x1, y2, x2] format.\n",
    "    \"\"\"\n",
    "    anchors = anchors.float()\n",
    "    # Compute widths, heights, and center coordinates of the anchors.\n",
    "    heights = anchors[:, 2] - anchors[:, 0]\n",
    "    widths = anchors[:, 3] - anchors[:, 1]\n",
    "    ctr_y = anchors[:, 0] + 0.5 * heights\n",
    "    ctr_x = anchors[:, 1] + 0.5 * widths\n",
    "\n",
    "    # Get the predicted offsets\n",
    "    dy = pred_locs[:, 0]\n",
    "    dx = pred_locs[:, 1]\n",
    "    dh = pred_locs[:, 2]\n",
    "    dw = pred_locs[:, 3]\n",
    "\n",
    "    # Apply the offsets to get the predicted center\n",
    "    pred_ctr_y = ctr_y + dy * heights\n",
    "    pred_ctr_x = ctr_x + dx * widths\n",
    "    # Compute predicted width and height\n",
    "    pred_h = heights * torch.exp(dh)\n",
    "    pred_w = widths * torch.exp(dw)\n",
    "\n",
    "    # Convert center coordinates back to box coordinates\n",
    "    y1 = pred_ctr_y - 0.5 * pred_h\n",
    "    x1 = pred_ctr_x - 0.5 * pred_w\n",
    "    y2 = pred_ctr_y + 0.5 * pred_h\n",
    "    x2 = pred_ctr_x + 0.5 * pred_w\n",
    "\n",
    "    pred_boxes = torch.stack([y1, x1, y2, x2], dim=1)\n",
    "    return pred_boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import box_iou\n",
    "\n",
    "def recursive_nms(boxes, scores, iou_threshold=0.5, recursion_limit=10):\n",
    "    \"\"\"\n",
    "    Custom NMS that recursively combines overlapping boxes by comparing all pairs.\n",
    "    \n",
    "    Args:\n",
    "        boxes: Tensor of shape [N, 4] (x1, y1, x2, y2 format)\n",
    "        scores: Tensor of shape [N] containing confidence scores\n",
    "        iou_threshold: IoU threshold for combining boxes\n",
    "        recursion_limit: Maximum number of recursive passes\n",
    "        \n",
    "    Returns:\n",
    "        combined_boxes: Tensor of combined boxes\n",
    "        keep_indices: Indices of kept boxes from original input\n",
    "    \"\"\"\n",
    "    if len(boxes) == 0:\n",
    "        return boxes, torch.empty(0, dtype=torch.long, device=boxes.device)\n",
    "    \n",
    "    # Convert to float32 if needed\n",
    "    boxes = boxes.float()\n",
    "    \n",
    "    # Initialize list to track which boxes to keep\n",
    "    keep = torch.ones(len(boxes), dtype=torch.bool, device=boxes.device)\n",
    "    \n",
    "    # Recursive combining\n",
    "    changed = True\n",
    "    recursion_count = 0\n",
    "    \n",
    "    while changed and recursion_count < recursion_limit:\n",
    "        changed = False\n",
    "        iou_matrix = box_iou(boxes, boxes)  # [N, N] matrix\n",
    "        \n",
    "        # Zero out diagonal (self-comparisons)\n",
    "        iou_matrix.fill_diagonal_(0)\n",
    "        \n",
    "        # Find all pairs that exceed IoU threshold\n",
    "        overlaps = iou_matrix > iou_threshold\n",
    "        \n",
    "        for i in range(len(boxes)):\n",
    "            if not keep[i]:\n",
    "                continue\n",
    "                \n",
    "            # Find all boxes that overlap with current box\n",
    "            overlapping_indices = torch.where(overlaps[i])[0]\n",
    "            \n",
    "            if len(overlapping_indices) > 0:\n",
    "                # Get the overlapping boxes and their scores\n",
    "                overlapping_boxes = boxes[overlapping_indices]\n",
    "                overlapping_scores = scores[overlapping_indices]\n",
    "                \n",
    "                # Combine with current box (weighted average by scores)\n",
    "                combined_box = combine_box_group(\n",
    "                    torch.cat([boxes[i].unsqueeze(0), overlapping_boxes]),\n",
    "                    torch.cat([scores[i].unsqueeze(0), overlapping_scores])\n",
    "                )\n",
    "                \n",
    "                # Replace current box with combined version\n",
    "                boxes[i] = combined_box\n",
    "                \n",
    "                # Mark overlapping boxes for removal\n",
    "                keep[overlapping_indices] = False\n",
    "                changed = True\n",
    "        \n",
    "        # Filter boxes after each pass\n",
    "        boxes = boxes[keep]\n",
    "        scores = scores[keep]\n",
    "        keep = torch.ones(len(boxes), dtype=torch.bool, device=boxes.device)\n",
    "        recursion_count += 1\n",
    "    \n",
    "    return boxes, torch.where(keep)[0]\n",
    "\n",
    "def combine_box_group(boxes, scores):\n",
    "    \"\"\"\n",
    "    Combine a group of boxes into one representative box using score-weighted average\n",
    "    \"\"\"\n",
    "    weights = scores / scores.sum()\n",
    "    combined_box = torch.sum(boxes * weights.view(-1, 1), dim=0)\n",
    "    return combined_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#####################\n",
    "\n",
    "small_train_dataset = torch.utils.data.Subset(train_dataset, list(range(3000)))\n",
    "small_train_loader = torch.utils.data.DataLoader(small_train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                                           collate_fn=custom_collate_fn, num_workers=0)\n",
    "\n",
    "small_val_dataset = torch.utils.data.Subset(val_dataset, list(range(300)))\n",
    "small_val_loader = torch.utils.data.DataLoader(small_val_dataset, batch_size=batch_size, shuffle=True,\n",
    "                                           collate_fn=custom_collate_fn, num_workers=0)\n",
    "\n",
    "###########################\n",
    "\n",
    "def generate_anchor_grid_np(X_FM, Y_FM, ratios, scales):\n",
    "    \"\"\"\n",
    "    Generate a (N,4) NumPy array of anchor boxes over an X_FM x Y_FM feature map.\n",
    "\n",
    "    The final shape is (X_FM * Y_FM * len(ratios)*len(scales), 4),\n",
    "    where each row is [y1, x1, y2, x2].\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    # X_FM => width of feature map (number of columns)\n",
    "    # Y_FM => height of feature map (number of rows)\n",
    "\n",
    "    total_positions = X_FM * Y_FM\n",
    "    num_anchor_per_pos = len(ratios) * len(scales)\n",
    "    total_anchors = total_positions * num_anchor_per_pos\n",
    "\n",
    "    # We assume your input image is (H=ISIZE[0], W=ISIZE[1])\n",
    "    # You may also do something like:\n",
    "    # sub_sampling_x = float(W_IMG) / X_FM\n",
    "    # sub_sampling_y = float(H_IMG) / Y_FM\n",
    "    # But in your original code, you used the global ISIZE.\n",
    "    # Adjust as needed if your shape is dynamic.\n",
    "\n",
    "    # If you’re using a fixed ISIZE = (height=720, width=1280), do:\n",
    "    H_IMG, W_IMG = ISIZE[0], ISIZE[1]\n",
    "\n",
    "    sub_sampling_x = W_IMG / float(X_FM)\n",
    "    sub_sampling_y = H_IMG / float(Y_FM)\n",
    "\n",
    "    # Create a grid of center positions\n",
    "    shift_x = np.arange(sub_sampling_x, (X_FM + 1) * sub_sampling_x, sub_sampling_x)\n",
    "    shift_y = np.arange(sub_sampling_y, (Y_FM + 1) * sub_sampling_y, sub_sampling_y)\n",
    "\n",
    "    shift_x, shift_y = np.meshgrid(shift_x, shift_y)  # shape (Y_FM, X_FM)\n",
    "    # Now each cell center is (cy, cx) = (shift_y[r,c] - sub_sampling_y/2, shift_x[r,c] - sub_sampling_x/2)\n",
    "    centers = np.stack([\n",
    "        shift_y.ravel() - sub_sampling_y / 2.0,\n",
    "        shift_x.ravel() - sub_sampling_x / 2.0\n",
    "    ], axis=1)  # shape (total_positions, 2)\n",
    "\n",
    "    anchors = []\n",
    "    for cy, cx in centers:\n",
    "        for ratio in ratios:\n",
    "            for scale in scales:\n",
    "                h = sub_sampling_y * scale * np.sqrt(ratio)\n",
    "                w = sub_sampling_x * scale * np.sqrt(1. / ratio)\n",
    "\n",
    "                y1 = cy - h * 0.5\n",
    "                x1 = cx - w * 0.5\n",
    "                y2 = cy + h * 0.5\n",
    "                x2 = cx + w * 0.5\n",
    "\n",
    "                anchors.append([x1, y1, x2, y2])\n",
    "\n",
    "    anchors = np.array(anchors, dtype=np.float32)  # shape (total_anchors, 4)\n",
    "\n",
    "\n",
    "    return anchors\n",
    "\n",
    "#########################\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def debug_check_image_and_boxes(dataloader, num_images=1):\n",
    "    \"\"\"\n",
    "    Pull 'num_images' samples from the dataloader, plot the image after resizing,\n",
    "    and overlay the GT boxes. Also print the box sizes relative to the image dimension.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Grab a single batch\n",
    "    batch = next(iter(dataloader))\n",
    "    \n",
    "    images = batch[\"images\"][:num_images]  # shape (num_images, C, H, W)\n",
    "    boxes_list = batch[\"boxes\"][:num_images]\n",
    "    indices = batch[\"indices\"][:num_images]\n",
    "\n",
    "    for i in range(len(images)):\n",
    "        img_tensor = images[i]\n",
    "        gt_boxes = boxes_list[i].cpu().numpy()  # shape (N, 4) in [x1,y1,x2,y2]\n",
    "\n",
    "        # 2) Print debug info\n",
    "        C, H, W = img_tensor.shape\n",
    "        print(f\"\\n--- Debug: Image index {indices[i]} ---\")\n",
    "        print(f\"  Tensor shape: (C={C}, H={H}, W={W}) => (should be 3,720,1280 if that is your resize?)\")\n",
    "        print(f\"  Num GT boxes: {len(gt_boxes)}\")\n",
    "\n",
    "        # 3) Plot the image\n",
    "        img_np = (img_tensor*255.0).clamp(0,255).byte().cpu().numpy()  # shape (C,H,W)\n",
    "        img_np = np.transpose(img_np, (1,2,0))  # (H,W,C)\n",
    "        plt.figure(figsize=(10,6))\n",
    "        plt.imshow(img_np)\n",
    "\n",
    "        # 4) Plot the GT boxes\n",
    "        for b_idx, box in enumerate(gt_boxes):\n",
    "            x1,y1,x2,y2 = box\n",
    "            # Print size info\n",
    "            box_w = x2 - x1\n",
    "            box_h = y2 - y1\n",
    "            print(f\"  Box {b_idx}: [x1={x1:.1f}, y1={y1:.1f}, x2={x2:.1f}, y2={y2:.1f}]\"\n",
    "                  f\" => w={box_w:.1f}, h={box_h:.1f}\")\n",
    "\n",
    "            # Add rectangle\n",
    "            rect = plt.Rectangle((x1, y1), box_w, box_h,\n",
    "                                 edgecolor='green', fill=False, lw=2)\n",
    "            plt.gca().add_patch(rect)\n",
    "\n",
    "        plt.title(f\"Image idx {indices[i]} - Resized: W={W},H={H}\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "debug_check_image_and_boxes(train_loader, num_images=2)\n",
    "\n",
    "######################\n",
    "import json\n",
    "def validate(rpn_model, data_loader, req_features,\n",
    "             n_images=7, top_k=40, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Validate the RPN on up to `n_images` from each batch.\n",
    "    1) Generates anchors with bbox_generation(...) \n",
    "    2) Runs the RPN to get predicted offsets & objectness\n",
    "    3) Decodes bounding boxes with pred_bbox_to_xywh(...)\n",
    "    4) Visualizes bounding boxes\n",
    "    5) Measures recall, error (1 - recall), and average IoU.\n",
    "\n",
    "    Args:\n",
    "        rpn_model: your trained RPN (EnhancedRPN) in eval mode\n",
    "        data_loader: DataLoader for your validation (or test) dataset\n",
    "        req_features: the list of frozen VGG layers (or backbone layers)\n",
    "        n_images: how many images to process from each batch\n",
    "        top_k: how many proposals to visualize (by objectness)\n",
    "        iou_threshold: IoU threshold used to decide if a GT is \"matched\"\n",
    "\n",
    "    Returns:\n",
    "        errors, recalls, avg_ious (lists)\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    rpn_model.eval().to(device)\n",
    "    save_json_path=\"C:/360/exported_anchors.json\"\n",
    "    anchor_results = {}\n",
    "    errors = []     # 1 - recall, per image\n",
    "    recalls = []    # recall, per image\n",
    "    avg_ious = []   # average best IoU, per image\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Loop over batches\n",
    "        count = 0\n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            count += 7\n",
    "            print(count)\n",
    "            # If you only want to visualize one batch, uncomment:\n",
    "            # if batch_idx >= 1:\n",
    "            #     break\n",
    "\n",
    "            # Collect up to n_images from this batch\n",
    "            images = batch[\"images\"][:n_images].to(device)\n",
    "            print(\"Batch keys:\", batch.keys()[\"img_ids\"])\n",
    "\n",
    "            targets = [\n",
    "                {\"boxes\": b.to(device), \"labels\": l.to(device)}\n",
    "                for (b, l) in zip(batch[\"boxes\"], batch[\"labels\"])\n",
    "            ][:n_images]\n",
    "            image_ids = batch[\"img_ids\"][:n_images]\n",
    "           \n",
    "\n",
    "            B = images.shape[0]\n",
    "            if B == 0:\n",
    "                continue\n",
    "\n",
    "            # Forward pass through the *frozen* backbone\n",
    "            feats = images.clone()\n",
    "            for m in req_features:\n",
    "                m.to(device)\n",
    "                feats = m(feats)\n",
    "\n",
    "            # The shape of the feature map\n",
    "            X_FM, Y_FM = feats.shape[2], feats.shape[3]\n",
    "\n",
    "            # Generate anchors using your original bbox_generation\n",
    "            # This returns: (anchor_locs, anchor_labels, anchors)\n",
    "            # but we only need \"anchors\" here for decoding\n",
    "            _, _, anchors = bbox_generation(\n",
    "                [img for img in images],  # pass as list\n",
    "                targets,\n",
    "                X_FM, Y_FM\n",
    "            )\n",
    "            # anchors is an np.array of shape (total_anchors, 4) in [y1, x1, y2, x2].\n",
    "            anchors_torch = torch.from_numpy(anchors).float().to(device)\n",
    "\n",
    "            # Run the RPN to get predicted offsets & classification\n",
    "            pred_locs, pred_scores, objectness_score, pooled_feats = rpn_model(\n",
    "                feats, \n",
    "                anchors_torch, \n",
    "                do_roi=True\n",
    "            )\n",
    "            # pred_locs.shape: (B, total_anchors, 4)\n",
    "            # pred_scores.shape: (B, total_anchors, 2)\n",
    "            # objectness_score.shape: (B, total_anchors)\n",
    "\n",
    "            # Process each image in this batch\n",
    "            for i in range(B):\n",
    "                # Decode from anchors + predicted offsets => actual boxes\n",
    "                rois = pred_bbox_to_xywh(pred_locs[i], anchors_torch)\n",
    "                # rois now shape (total_anchors, 4). Format is [x1, y1, x2, y2],\n",
    "                # if that's how your pred_bbox_to_xywh is implemented.\n",
    "\n",
    "                # Choose top-k by objectness\n",
    "                # if top_k is not None:\n",
    "                #     k = min(top_k, objectness_score[i].shape[0])\n",
    "                #     topk_inds = torch.topk(objectness_score[i], k=k).indices\n",
    "                #     proposals = rois[topk_inds.cpu().numpy()]\n",
    "                # else:\n",
    "                #     proposals = rois\n",
    "                # Choose top-k by objectness\n",
    "                if top_k is not None:\n",
    "                    k = min(top_k, objectness_score[i].shape[0])\n",
    "                    topk_inds = torch.topk(objectness_score[i], k=k).indices\n",
    "                    proposals = rois[topk_inds.cpu().numpy()]\n",
    "                    scores = objectness_score[i][topk_inds].cpu().numpy()\n",
    "                else:\n",
    "                    proposals = rois\n",
    "                    scores = objectness_score[i].cpu().numpy()\n",
    "\n",
    "                # Apply recursive NMS to reduce overlapping proposals\n",
    "                proposals_tensor = torch.from_numpy(proposals).float().to(device)\n",
    "                scores_tensor = torch.from_numpy(scores).float().to(device)\n",
    "                proposals, _ = recursive_nms(proposals_tensor, scores_tensor, iou_threshold=0.5, recursion_limit=top_k)\n",
    "                proposals = proposals.cpu().numpy()  # convert back to numpy for visualization\n",
    "\n",
    "\n",
    "                boxes_list = proposals.tolist()\n",
    "                anchor_results[str(image_ids[i])] = boxes_list\n",
    "\n",
    "                save_path = \"C:/360/output_roi_file\"\n",
    "                # Visualize the proposals\n",
    "                # images[i] => shape (3, H, W)\n",
    "                print(count+i)\n",
    "                show_corner_bbs(images[i], proposals, save_path, (count+i))\n",
    "\n",
    "                # Grab ground-truth boxes & visualize\n",
    "                gt_boxes = targets[i][\"boxes\"].cpu().numpy()  # shape (M,4)\n",
    "                if len(gt_boxes) > 0:\n",
    "                    \n",
    "                   \n",
    "                    show_ground_truth_bbs(images[i], gt_boxes, save_path, (count+i))\n",
    "\n",
    "                    # Compute recall\n",
    "                    matched = 0\n",
    "                    image_ious = []\n",
    "\n",
    "                    for gt in gt_boxes:\n",
    "                        # If your GT is [x1, y1, x2, y2] but your proposals are [x1, y1, x2, y2],\n",
    "                        # then you can pass them directly to compute_iou_vectorized.\n",
    "                        # If your GT is [y1, x1, y2, x2], reorder accordingly!\n",
    "                        gt_box = gt\n",
    "\n",
    "                        # Compute IoU with all proposals => shape (k, 1)\n",
    "                        ious = compute_iou_vectorized(proposals, np.expand_dims(gt_box, axis=0))\n",
    "                        best_iou = np.max(ious) if ious.size > 0 else 0.0\n",
    "                        image_ious.append(best_iou)\n",
    "                        if best_iou >= iou_threshold:\n",
    "                            matched += 1\n",
    "\n",
    "                    # Summarize for this image\n",
    "                    recall = matched / len(gt_boxes)\n",
    "                    error = 1 - recall\n",
    "                    avg_iou = np.mean(image_ious)\n",
    "\n",
    "                    recalls.append(recall)\n",
    "                    errors.append(error)\n",
    "                    avg_ious.append(avg_iou)\n",
    "\n",
    "                    print(f\"[Val] Image {i} metrics:\")\n",
    "                    print(f\"  - Recall: {recall:.3f}\")\n",
    "                    print(f\"  - Error : {error:.3f}\")\n",
    "                    print(f\"  - AvgIoU: {avg_iou:.3f}\")\n",
    "                else:\n",
    "                    print(f\"[Val] Image {i}: no ground-truth boxes available\")\n",
    "\n",
    "    # After we finish, plot the results\n",
    "    if errors:\n",
    "        plt.figure(figsize=(15, 5))\n",
    "\n",
    "        # Plot 1: Recall\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.plot(range(len(recalls)), recalls, 'b-o')\n",
    "        plt.ylim(0, 1.05)\n",
    "        plt.xlabel('Image Index')\n",
    "        plt.ylabel('Recall')\n",
    "        plt.title(f'Recall (IoU ≥ {iou_threshold})')\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Plot 2: Error\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(range(len(errors)), errors, 'r-o')\n",
    "        plt.ylim(0, 1.05)\n",
    "        plt.xlabel('Image Index')\n",
    "        plt.ylabel('Error (1 - Recall)')\n",
    "        plt.title('Error per Image')\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Plot 3: Average IoU\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.plot(range(len(avg_ious)), avg_ious, 'g-o')\n",
    "        plt.ylim(0, 1.05)\n",
    "        plt.xlabel('Image Index')\n",
    "        plt.ylabel('Avg Best IoU')\n",
    "        plt.title('Proposal Quality (Best IoU)')\n",
    "        plt.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"\\nValidation Summary:\")\n",
    "        print(f\"  Mean Recall:  {np.mean(recalls):.3f} ± {np.std(recalls):.3f}\")\n",
    "        print(f\"  Mean Error:   {np.mean(errors):.3f} ± {np.std(errors):.3f}\")\n",
    "        print(f\"  Mean AvgIoU:  {np.mean(avg_ious):.3f} ± {np.std(avg_ious):.3f}\")\n",
    "    else:\n",
    "        print(\"[Val] No ground-truth boxes found in the processed images.\")\n",
    "    \n",
    "    os.makedirs(os.path.dirname(save_json_path), exist_ok=True)\n",
    "    with open(save_json_path, \"w\") as f:\n",
    "        json.dump(anchor_results, f, indent=2)\n",
    "    \n",
    "    # Switch model back to train mode if you like\n",
    "    rpn_model.train().to(device)\n",
    "    return errors, recalls, avg_ious\n",
    "\n",
    "\n",
    "######################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_rpn = train_epochs(req_features, rpn_model, optimizer, small_train_loader, epochs=5, rpn_lambda=1)\n",
    "\n",
    "# Validate (visualize predictions) on both training and validation sets\n",
    "# print(\"Validation on training data:\")\n",
    "# validate(trained_rpn, small_train_loader, req_features)\n",
    "# print(\"Validation on validation data:\")\n",
    "# validate(trained_rpn, small_val_loader, req_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a directory to store results\n",
    "save_path = \"C:/360/output_roi\"\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "def show_corner_bbs(img, bbs, save_path=None, image_id=None, title=\"Predicted Boxes\"):\n",
    "    \"\"\"\n",
    "    Show and optionally save predicted bounding boxes on an image.\n",
    "\n",
    "    Args:\n",
    "        img: Tensor image (C, H, W) normalized to [0, 1]\n",
    "        bbs: List or array of bounding boxes in [x1, y1, x2, y2]\n",
    "        save_path: Directory path to save the image (optional)\n",
    "        image_id: Unique identifier for the saved image name (optional)\n",
    "        title: Title of the plot (optional)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        img_np = (img * 255.0).clamp(0, 255).byte().cpu().numpy()\n",
    "        img_np = np.transpose(img_np, (1, 2, 0))  # (H, W, C)\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.imshow(img_np.astype(np.uint8))\n",
    "        for bb in bbs:\n",
    "            plt.gca().add_patch(create_corner_rect(bb))\n",
    "        plt.title(title)\n",
    "        plt.axis('off')\n",
    "\n",
    "        if save_path and image_id is not None:\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "            filename = os.path.join(save_path, f\"pred_{image_id}.png\")\n",
    "            plt.savefig(filename, bbox_inches='tight', pad_inches=0)\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"[Error saving/showing predicted boxes]: {e}\")\n",
    "\n",
    "\n",
    "def show_ground_truth_bbs(img, bbs, save_path=None, image_id=None, title=\"Ground Truth Boxes\"):\n",
    "    \"\"\"\n",
    "    Show and optionally save ground-truth bounding boxes on an image.\n",
    "\n",
    "    Args:\n",
    "        img: Tensor image (C, H, W) normalized to [0, 1]\n",
    "        bbs: List or array of ground truth boxes in [x1, y1, x2, y2]\n",
    "        save_path: Directory path to save the image (optional)\n",
    "        image_id: Unique identifier for the saved image name (optional)\n",
    "        title: Title of the plot (optional)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        img_np = unnormalize_tensor(img)  # expected to be in [0, 255]\n",
    "        img_np = np.transpose(img_np, (1, 2, 0))  # (H, W, C)\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.imshow(img_np.astype(np.uint8))\n",
    "        for bb in bbs:\n",
    "            plt.gca().add_patch(create_ground_truth_rect(bb))\n",
    "        plt.title(title)\n",
    "        plt.axis('off')\n",
    "\n",
    "        if save_path and image_id is not None:\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "            filename = os.path.join(save_path, f\"gt_{image_id}.png\")\n",
    "            plt.savefig(filename, bbox_inches='tight', pad_inches=0)\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"[Error saving/showing ground truth boxes]: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Validation on training data:\")\n",
    "validate(trained_rpn, small_train_loader, req_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
